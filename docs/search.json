[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html#communicating-code-and-data",
    "href": "index.html#communicating-code-and-data",
    "title": "TH Simm Python Pages",
    "section": "Communicating code and data",
    "text": "Communicating code and data\n\nPresentation"
  },
  {
    "objectID": "index.html#links",
    "href": "index.html#links",
    "title": "TH Simm Python Pages",
    "section": "Links",
    "text": "Links\nWebsite built with Quarto visit for more info https://quarto.org/docs/websites."
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres.html#content",
    "href": "posts/Communicating_Code/CommCodePres.html#content",
    "title": "Overview: Communicating code and data",
    "section": "Content",
    "text": "Content\n\nNotebooks overview\nConverting Notebooks\nExample useage of notebooks"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres.html#jupyter-notebooks",
    "href": "posts/Communicating_Code/CommCodePres.html#jupyter-notebooks",
    "title": "Overview: Communicating code and data",
    "section": "Jupyter Notebooks",
    "text": "Jupyter Notebooks\nFrom TalkPython: Awesome Jupyter Libraries and Extensions\n\nJupyter is an amazing environment for exploring data and generating executable reports with Python. But there are many external tools, extensions, and libraries to make it so much better and make you more productive.\n\n\nA notebook consists of two parts\n\nmarkdown part where we can:\n\nwrite text, add images, links, html, LaTeX etc\n\ncode part which runs and displays output of code\n\n\nSome links:\n\nJupyter Book\nA curated list of awesome Jupyter projects\nCode Documentation QA of Code\nFastAI guide for better blogs"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres.html#example-of-a-notebook",
    "href": "posts/Communicating_Code/CommCodePres.html#example-of-a-notebook",
    "title": "Overview: Communicating code and data",
    "section": "Example of a notebook",
    "text": "Example of a notebook\nAn example notebook"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres.html#example-of-a-notebook-output",
    "href": "posts/Communicating_Code/CommCodePres.html#example-of-a-notebook-output",
    "title": "Overview: Communicating code and data",
    "section": "Example of a notebook: output",
    "text": "Example of a notebook: output\n\nimport matplotlib.pyplot as plt\nplt.plot(df2['date_of_sampling'])"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres.html#example-of-a-notebook-output-2",
    "href": "posts/Communicating_Code/CommCodePres.html#example-of-a-notebook-output-2",
    "title": "Overview: Communicating code and data",
    "section": "Example of a notebook: output 2",
    "text": "Example of a notebook: output 2\n\nimport altair as alt\nfrom vega_datasets import data\n\nmovies = alt.UrlData(\n    data.movies.url,\n    format=alt.DataFormat(parse={\"Release_Date\":\"date\"})\n)\nratings = ['G', 'NC-17', 'PG', 'PG-13', 'R']\ngenres = ['Action', 'Adventure', 'Black Comedy', 'Comedy',\n       'Concert/Performance', 'Documentary', 'Drama', 'Horror', 'Musical',\n       'Romantic Comedy', 'Thriller/Suspense', 'Western']\n\nbase = alt.Chart(movies, width=200, height=200).mark_point(filled=True).transform_calculate(\n    Rounded_IMDB_Rating = \"floor(datum.IMDB_Rating)\",\n    Hundred_Million_Production =  \"datum.Production_Budget > 100000000.0 ? 100 : 10\",\n    Release_Year = \"year(datum.Release_Date)\"\n).transform_filter(\n    alt.datum.IMDB_Rating > 0\n).transform_filter(\n    alt.FieldOneOfPredicate(field='MPAA_Rating', oneOf=ratings)\n).encode(\n    x=alt.X('Worldwide_Gross:Q', scale=alt.Scale(domain=(100000,10**9), clamp=True)),\n    y='IMDB_Rating:Q',\n    tooltip=\"Title:N\"\n)\n\n# A slider filter\nyear_slider = alt.binding_range(min=1969, max=2018, step=1)\nslider_selection = alt.selection_single(bind=year_slider, fields=['Release_Year'], name=\"Release Year_\")\n\n\nfilter_year = base.add_selection(\n    slider_selection\n).transform_filter(\n    slider_selection\n).properties(title=\"Slider Filtering\")\n\n# A dropdown filter\ngenre_dropdown = alt.binding_select(options=genres)\ngenre_select = alt.selection_single(fields=['Major_Genre'], bind=genre_dropdown, name=\"Genre\")\n\nfilter_genres = base.add_selection(\n    genre_select\n).transform_filter(\n    genre_select\n).properties(title=\"Dropdown Filtering\")\n\n#color changing marks\nrating_radio = alt.binding_radio(options=ratings)\n\nrating_select = alt.selection_single(fields=['MPAA_Rating'], bind=rating_radio, name=\"Rating\")\nrating_color_condition = alt.condition(rating_select,\n                      alt.Color('MPAA_Rating:N', legend=None),\n                      alt.value('lightgray'))\n\nhighlight_ratings = base.add_selection(\n    rating_select\n).encode(\n    color=rating_color_condition\n).properties(title=\"Radio Button Highlighting\")\n\n# Boolean selection for format changes\ninput_checkbox = alt.binding_checkbox()\ncheckbox_selection = alt.selection_single(bind=input_checkbox, name=\"Big Budget Films\")\n\nsize_checkbox_condition = alt.condition(checkbox_selection,\n                                        alt.SizeValue(25),\n                                        alt.Size('Hundred_Million_Production:Q')\n                                       )\n\nbudget_sizing = base.add_selection(\n    checkbox_selection\n).encode(\n    size=size_checkbox_condition\n).properties(title=\"Checkbox Formatting\")\n\n( filter_year | filter_genres) &  (highlight_ratings | budget_sizing  )"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres.html#communicating-when-code-is-a-large-element-of-what-is-being-presented",
    "href": "posts/Communicating_Code/CommCodePres.html#communicating-when-code-is-a-large-element-of-what-is-being-presented",
    "title": "Overview: Communicating code and data",
    "section": "Communicating when code is a large element of what is being presented",
    "text": "Communicating when code is a large element of what is being presented\n\nMicrosoft Word/ppt- type methods aren’t set-up well to include code\nProgramming files (e.g. .py) aren’t set-up well to share\nVideoing code with outputs is an option, but don’t translate to other formats (i.e. we may also need to do a written format of this)\nApps (e.g. streamlit) can be good.\n\nBut the code is hidden\n\nProgramming notebooks (e.g..ipynb) offer a good and easy to share code but with some limitations\n\nAn easier way is to convert the notebooks to html\n\ne.g. maybe someone doesn’t have python installed"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres.html#notebook-benefits",
    "href": "posts/Communicating_Code/CommCodePres.html#notebook-benefits",
    "title": "Overview: Communicating code and data",
    "section": "Notebook Benefits",
    "text": "Notebook Benefits\n\nNotebooks are intuitive\n\nYou have the code then the result of the code\nPlus can add details of how code works\nAnd it’s linear\n\nCan get things up and working quickly\nAid with communicating code\nEncourages Writing\n\nand writing things down aids thinking in the now and understanding what you did and why in the future\n\n\nCan use shell commands e.g. !pip install pandas\nCan use magic commands e.g. %%time to time a cell\n\nWith the ONS moving towards Python/R from Excel and a varied level of skills. The first of these is particularly important to aid communicating code"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres.html#what-i-have-used-to-convert-notebooks",
    "href": "posts/Communicating_Code/CommCodePres.html#what-i-have-used-to-convert-notebooks",
    "title": "Overview: Communicating code and data",
    "section": "What I have used to convert notebooks",
    "text": "What I have used to convert notebooks\n\nfastpages\n\nPreviously I converted notebooks to html via fastpages but this is now deprecated and they are recommending the use of quarto.\n\nquarto\n\nSo far I have found quarto really good and flexible (N.B. R works too)\nEasy to convert a notebook to multiple formats, including html, powerpoint, pdf, word doc\nBUT Quarto is not possible within ONS (as far as I can tell currently)\n\nnbconvert is another option I tried\n\nbut it doesn’t seem to have the functionality of fastpages or quarto.\n\nJupyter Books seems to be the best option within ONS\n\nMaybe not as good as quarto but it works!"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres.html#others",
    "href": "posts/Communicating_Code/CommCodePres.html#others",
    "title": "Overview: Communicating code and data",
    "section": "Others",
    "text": "Others\n\nI know some people use Sphinx,\n\nis recommended by QA\nFrom what I can tell sphinx on it’s own is not as easy to use as notebooks\nBut there is a jupyter extension nbsphinx\nJupyter Books uses Sphinx heavily under the hood\n\nnbdev\n\nI think is connected to quarto\n\nVoila\n\nVoilà turns Jupyter notebooks into standalone web applications.\nLooks good, bit like streamlit\nbut seems to interfere with other libraries and not checked whether works in ONS\nmercury seems similar\n\nAnything else people use and recommend?"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres.html#quarto-outputs",
    "href": "posts/Communicating_Code/CommCodePres.html#quarto-outputs",
    "title": "Overview: Communicating code and data",
    "section": "Quarto Outputs",
    "text": "Quarto Outputs\nWe can then create different files from this .ipynb Jupyter notebook using the following code:\n\nquarto render testPres.ipynb --to pptx\nquarto render testPres.ipynb --to pdf\nquarto render testPres.ipynb --to html\nquarto render testPres.ipynb --to revealjs\n\nor for Jupyter Books - jupyter-book build .\\PesticideDocs\\"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres.html#creating-a-webpage-from-this",
    "href": "posts/Communicating_Code/CommCodePres.html#creating-a-webpage-from-this",
    "title": "Overview: Communicating code and data",
    "section": "Creating a webpage from this",
    "text": "Creating a webpage from this\nTakes about 30 mins including installing chosen converter. (But can be done much quicker)\n\ncreate a Github repo for your website\nchoose the converter (e.g. Jupyter Books)\n\nAnd follow their instructions\n\ngo to settings -> Pages within the repo\n\nfew options to do\n\nOptional: add your own website url to it\n\nLink how to do this here"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres.html#example-documenting-code",
    "href": "posts/Communicating_Code/CommCodePres.html#example-documenting-code",
    "title": "Overview: Communicating code and data",
    "section": "Example: Documenting Code",
    "text": "Example: Documenting Code\n\nHere is my website for my research project on pesticides in UK food.\nThis is not the same as documentation for a package but there are parallels\n\nThis does a few things:\n\nDocuments the analysis steps I have taken including the code and outputs\n\nUseful for data transparency, useability of the code if needs modifiying/adapting, and why I did XYZ\n\nProvides a way to present the data\n\nThere is a streamlit app, but sometimes I like to be able to see the code"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres.html#example-discussing-code",
    "href": "posts/Communicating_Code/CommCodePres.html#example-discussing-code",
    "title": "Overview: Communicating code and data",
    "section": "Example: Discussing Code",
    "text": "Example: Discussing Code\n\nGP Tables example"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres.html#example-tool-to-aid-learning",
    "href": "posts/Communicating_Code/CommCodePres.html#example-tool-to-aid-learning",
    "title": "Overview: Communicating code and data",
    "section": "Example: Tool to aid learning",
    "text": "Example: Tool to aid learning\nA big area I have been using Jupyter Notebooks for is to aid learning\n\nIf you want to understand something it helps to write it down\nHaving the code next to it is a big advantage\nAnd if stored on github you can access it anywhere\n\nTensoflow cheat sheet"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres.html#example-debugging-code",
    "href": "posts/Communicating_Code/CommCodePres.html#example-debugging-code",
    "title": "Overview: Communicating code and data",
    "section": "Example: Debugging Code",
    "text": "Example: Debugging Code\n\nSince starting at ONS I have been working with understanding an existing project and latterly adding code to it\nThe project consists of multiple python files across several folders\n\nMy Python was good but lots of the functions and their useage weren’t immediately obvious to me\n\nbreak-points in VS Studio is really good to step through the code and work out what happens in the code.\n\nI had not used before with Python (but had lots with MATLAB), and it’s really useful\n\nBut it can be limited what you can do\n\ndifficult to probe code if want to write more than 1 line of code\nthe experience/knowledge exists as you go through it but no documentation to refer to later, e.g. function X does this when I give it Y etc\n\nBy copying and pasting code into Jupyter cells I could see and document how they worked (e.g. changing inputs)\n\nThis (copying and pasting) would get around code changes too (which would be an issue if modules were just imported)\nbecause this was all done in Jupyter notebook I can have a ipynb code file and a html file showing how the code works\nI could even save a pickle file of the variables at a particularly point to understand how the code would work from this point"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres.html#presenting-in-multiple-formats",
    "href": "posts/Communicating_Code/CommCodePres.html#presenting-in-multiple-formats",
    "title": "Overview: Communicating code and data",
    "section": "Presenting in multiple formats",
    "text": "Presenting in multiple formats\n\nJupyter notebooks can be used on their own or as html\nBut can also be used to create presentations, pdf/word documentation or even books\nThis presentation was done with Quarto using the revealjs format\n\nSo it is a presentation format but with a html file\n\nSome of these file types can be difficult within ONS framework to do\n\nI hit a wall when trying to go beyond html and docs with Jupyter books due to dependencies"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres.html#presenting-in-multiple-formats-video",
    "href": "posts/Communicating_Code/CommCodePres.html#presenting-in-multiple-formats-video",
    "title": "Overview: Communicating code and data",
    "section": "Presenting in multiple formats: video",
    "text": "Presenting in multiple formats: video\nVideo"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres.html#questions-comments",
    "href": "posts/Communicating_Code/CommCodePres.html#questions-comments",
    "title": "Overview: Communicating code and data",
    "section": "Questions/ Comments",
    "text": "Questions/ Comments\n\nThoughts on:\n\nusing notebooks\ndocumenting code\nencouraging communication of code across ONS areas and experiences\n\nCan we share html files? Or do we have to work within the current framework?\nAnything else?"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_html.html#what-i-have-used-to-convert-notebooks-to-html",
    "href": "posts/Communicating_Code/CommCodePres_html.html#what-i-have-used-to-convert-notebooks-to-html",
    "title": "Communicating code: Website",
    "section": "What I have used to convert notebooks to html",
    "text": "What I have used to convert notebooks to html\n\nfastpages\n\nPreviously I converted notebooks to html via fastpages but this is now deprecated and they are recommending the use of quarto.\n\nquarto\n\nSo far I have found quarto really good and flexible (N.B. R works too)\nEasy to convert a notebook to multiple formats, including html, powerpoint, pdf, word doc\nBUT Quarto is not possible if installing from non pip sources is an issue (as far as I can tell currently)\n\nnbconvert is another option I tried\n\nbut it doesn’t seem to have the functionality of fastpages or quarto.\n\nJupyter Books seems to be the best option within companies with installation issues\n\nMaybe not as good as quarto but it works!"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_html.html#others",
    "href": "posts/Communicating_Code/CommCodePres_html.html#others",
    "title": "Communicating code: Website",
    "section": "Others",
    "text": "Others\n\nI know some people use Sphinx,\n\nis recommended by QA\nFrom what I can tell sphinx on it’s own is not as easy to use as notebooks\nBut there is a jupyter extension nbsphinx\nJupyter Books uses Sphinx heavily under the hood\n\nnbdev\n\nI think is connected to quarto\n\nVoila\n\nVoilà turns Jupyter notebooks into standalone web applications.\nLooks good, bit like streamlit\nbut seems to interfere with other libraries\nmercury seems similar"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_html.html#creating-html-other-formats",
    "href": "posts/Communicating_Code/CommCodePres_html.html#creating-html-other-formats",
    "title": "Communicating code: Website",
    "section": "Creating html (& other formats)",
    "text": "Creating html (& other formats)\n\nQuarto\nInstallation is via a package i.e. .msi for Windows or .pkg for Mac. Which can cause issues.\nWorks with both ipynb and qmd files, which are both a mixture of markdown and executable code.\nThe only thing that needs to be done with the notebook is add a YAML block at the start of the notebook, like the following (raq not markdown was used):\n---\ntitle: \"Communicating code: Website\"\nsubtitle: \"Using the notebook format for a website\"\nauthor: \"Thomas H. Simm\"\nformat:\n  html:\n    toc: true\ntitle-slide-attributes:\n  data-background-size: contain\n  data-background-opacity: \"0.5\"\njupyter: python3\n---\nWe can create different files from this .ipynb Jupyter notebook using the following code:\n\nquarto render testPres.ipynb --to pptx\nquarto render testPres.ipynb --to pdf\nquarto render testPres.ipynb --to html\nquarto render testPres.ipynb --to revealjs\n\nFurther, formatting for projects (i.e. for website) can be done within the configuration file _quarto.yml\nproject:\n  type: website\n  output-dir: _site\n\nwebsite:\n  title: \"ThomasHSimm\"\n  favicon: /posts/Picture3.png\n  body-header: <img src=\"/posts/header2.png\" height=200>\n\n  navbar:\n    right:\n      - about.qmd\n      - icon: github\n        href: https://github.com/ThomasHSimm\n      - icon: mortarboard-fill\n        href: https://scholar.google.com/citations?hl=en&user=HdPDn1sAAAAJ\nformat:\n  html:\n    theme: \n      light: flatly\n      dark: darkly\n    css: styles.css"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_html.html#jupyter-books",
    "href": "posts/Communicating_Code/CommCodePres_html.html#jupyter-books",
    "title": "Communicating code: Website",
    "section": "Jupyter Books",
    "text": "Jupyter Books\nWe can create different files from this .ipynb Jupyter notebook using the following code:\n\njupyter-book build .\\PesticideDocs\\\njupyter-book build <path-to-book>\njupyter-book build <path-to-book> --builder pdfhtml\njupyter-book build <path-to-book> --builder singlehtml\n\nThe only difference in notebook is that it needs to have One header in a markdown cell for the table of contents, e.g. \n# Title of page\n\nConfiguration file\nA seperate files _config.yml is used to define how the html (or other) files will look\n# Book settings\n# Learn more at https://jupyterbook.org/customize/config.html\n\ntitle: Defra Pesticide Testing, Data Analysis\nauthor: Thomas Simm\nlogo: ONS-logo.png\nexclude_patterns: [_build, Thumbs.db, .DS_Store, \"**.ipynb_checkpoints\"]\n\n\n# Force re-execution of notebooks on each build.\n# See https://jupyterbook.org/content/execute.html\nexecute:\n  execute_notebooks: force\n\n# Define the name of the latex output file for PDF builds\nlatex:\n  latex_documents:\n    targetname: book.tex\n\n# Add a bibtex file so that we can create citations\nbibtex_bibfiles:\n  - references.bib\n\n# Information about where the book exists on the web\nrepository:\n  url: https://github.com/ThomasHSimm/Pesticide  # Online location of your book\n  path_to_book: docs  # Optional path to your book, relative to the repository root\n  branch: master  # Which branch of the repository should be used when creating links (optional)\n\n# Add GitHub buttons to your book\n# See https://jupyterbook.org/customize/config.html#add-a-link-to-your-repository\n# HTML-specific settings\nhtml:\n  favicon                   : \"_images/favicon.jpg\"  # A path to a favicon image\n  use_edit_page_button      : false  # Whether to add an \"edit this page\" button to pages. If `true`, repository information in repository: must be filled in\n  use_repository_button     : false  # Whether to add a link to your repository button\n  use_issues_button         : false  # Whether to add an \"open an issue\" button\n  use_multitoc_numbering    : true   # Continuous numbering across parts/chapters\n  extra_navbar              : Powered by <a href=\"https://jupyterbook.org\">Jupyter Book</a>\n                              <br>Home website <a href=\"https://thomashsimm.com/\">thomashsimm.com</a> # Will be displayed underneath the left navbar.\n  extra_footer              : \"\"  # Will be displayed underneath the footer.\n  google_analytics_id       : \"\"  # A GA id that can be used to track book views.\n  home_page_in_navbar       : true  # Whether to include your home page in the left Navigation Bar\n  baseurl                   : \"\"  # The base URL where your book will be hosted. Used for creating image previews and social links. e.g.: https://mypage.com/mybook/\n  comments:\n    hypothesis              : false\n    utterances              : false\n  announcement              : \"\" # A banner announcement at the top of the site.\n\nAnd in addition to the config file a table of contents file is required _toc.yml:\n# Table of contents\n# Learn more at https://jupyterbook.org/customize/toc.html\n\nformat: jb-book\nroot: intro\nchapters:\n- file: Pesticide_Plots\n- file: References\n- file: UK_areas\n- file: using_jupyter_books"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_html.html#creating-a-webpage-from-this",
    "href": "posts/Communicating_Code/CommCodePres_html.html#creating-a-webpage-from-this",
    "title": "Communicating code: Website",
    "section": "Creating a webpage from this",
    "text": "Creating a webpage from this\nTakes about 30 mins including installing the chosen converter. (But can be done much quicker)\n\ncreate a Github repo for your website\nchoose the converter (e.g. Jupyter Books)\n\nAnd follow their instructions\n\ngo to settings -> Pages within the repo\n\nfew options to do\n\nOptional: add your own website url to it\n\nLink how to do this here\nIn Quarto a command from your PC in the repo, publishes the website:\nquarto publish quarto-pub\nOr equivalently with Jupyter Books:\nghp-import -n -p -f _build/html"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_html.html#creating-directly-from-the-repo",
    "href": "posts/Communicating_Code/CommCodePres_html.html#creating-directly-from-the-repo",
    "title": "Communicating code: Website",
    "section": "Creating directly from the repo",
    "text": "Creating directly from the repo\nIf we instead want to convert notebook files directly from a repo to create a website then this can be done with Netlify.\nThis is useful if using Gitlab (i.e. not Github) or don’t want all the extra html files cluttering the repo.\n\nSteps:\nhttps://jupyterbook.org/en/stable/publish/netlify.html\n\nSign up and connect Github/Gitlab\nAdd a requirements.txt file and also toc.yml to directory\nOn netlify -> Add new site -> import from an existing repo\nInsert something like below\n\nN.B. the command:\npip install -r requirements.txt && jupyter-book build .\nand folder location \n\n\nExample:\n\nGitlab repo\nResulting website https://thomashsimm.netlify.app/intro.html\n\nAnd from the inner folder"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_notebooks.html#jupyter-notebooks",
    "href": "posts/Communicating_Code/CommCodePres_notebooks.html#jupyter-notebooks",
    "title": "Communicating code: Notebooks",
    "section": "Jupyter Notebooks",
    "text": "Jupyter Notebooks\nFrom TalkPython: Awesome Jupyter Libraries and Extensions\n\nJupyter is an amazing environment for exploring data and generating executable reports with Python. But there are many external tools, extensions, and libraries to make it so much better and make you more productive.\n\n\nA notebook consists of two parts\n\nmarkdown part where we can:\n\nwrite text, add images, links, html, LaTeX etc\n\ncode part which runs and displays output of code\n\n\nSome links:\n\nJupyter Book\nA curated list of awesome Jupyter projects\nCode Documentation QA of Code\nFastAI guide for better blogs"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_notebooks.html#example-of-a-notebook",
    "href": "posts/Communicating_Code/CommCodePres_notebooks.html#example-of-a-notebook",
    "title": "Communicating code: Notebooks",
    "section": "Example of a notebook",
    "text": "Example of a notebook\nAn example notebook"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_notebooks.html#markdown-in-a-notebook",
    "href": "posts/Communicating_Code/CommCodePres_notebooks.html#markdown-in-a-notebook",
    "title": "Communicating code: Notebooks",
    "section": "Markdown in a notebook",
    "text": "Markdown in a notebook\nSome useful commands:\n\n# Notebooks Markdown and Code and ## Markdown in a notebook\n![](ghtop_images/pest.png) looks like this\n\n\n\nAnd the same with a mp4 file ![](ghtop_images/revealjs.mp4)\n\nVideo\n\n> If we want text like this\n\n\nIf we want text like this\n\n\nOr if we want code use `a = b + c`\n\nor:\n```\na = b\na = a + c\n```\na = b + c\n\nHTML works too\n\n<img src=\"ghtop_images/pest.png\"></img>"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_notebooks.html#code-in-a-notebook",
    "href": "posts/Communicating_Code/CommCodePres_notebooks.html#code-in-a-notebook",
    "title": "Communicating code: Notebooks",
    "section": "Code in a notebook",
    "text": "Code in a notebook\nExample interactive format using altair:\n\nimport altair as alt\nfrom vega_datasets import data\n\nmovies = alt.UrlData(\n    data.movies.url,\n    format=alt.DataFormat(parse={\"Release_Date\":\"date\"})\n)\nratings = ['G', 'NC-17', 'PG', 'PG-13', 'R']\ngenres = ['Action', 'Adventure', 'Black Comedy', 'Comedy',\n       'Concert/Performance', 'Documentary', 'Drama', 'Horror', 'Musical',\n       'Romantic Comedy', 'Thriller/Suspense', 'Western']\n\nbase = alt.Chart(movies, width=200, height=200).mark_point(filled=True).transform_calculate(\n    Rounded_IMDB_Rating = \"floor(datum.IMDB_Rating)\",\n    Hundred_Million_Production =  \"datum.Production_Budget > 100000000.0 ? 100 : 10\",\n    Release_Year = \"year(datum.Release_Date)\"\n).transform_filter(\n    alt.datum.IMDB_Rating > 0\n).transform_filter(\n    alt.FieldOneOfPredicate(field='MPAA_Rating', oneOf=ratings)\n).encode(\n    x=alt.X('Worldwide_Gross:Q', scale=alt.Scale(domain=(100000,10**9), clamp=True)),\n    y='IMDB_Rating:Q',\n    tooltip=\"Title:N\"\n)\n\n# A slider filter\nyear_slider = alt.binding_range(min=1969, max=2018, step=1)\nslider_selection = alt.selection_single(bind=year_slider, fields=['Release_Year'], name=\"Release Year_\")\n\n\nfilter_year = base.add_selection(\n    slider_selection\n).transform_filter(\n    slider_selection\n).properties(title=\"Slider Filtering\")\n\n# A dropdown filter\ngenre_dropdown = alt.binding_select(options=genres)\ngenre_select = alt.selection_single(fields=['Major_Genre'], bind=genre_dropdown, name=\"Genre\")\n\nfilter_genres = base.add_selection(\n    genre_select\n).transform_filter(\n    genre_select\n).properties(title=\"Dropdown Filtering\")\n\n#color changing marks\nrating_radio = alt.binding_radio(options=ratings)\n\nrating_select = alt.selection_single(fields=['MPAA_Rating'], bind=rating_radio, name=\"Rating\")\nrating_color_condition = alt.condition(rating_select,\n                      alt.Color('MPAA_Rating:N', legend=None),\n                      alt.value('lightgray'))\n\nhighlight_ratings = base.add_selection(\n    rating_select\n).encode(\n    color=rating_color_condition\n).properties(title=\"Radio Button Highlighting\")\n\n# Boolean selection for format changes\ninput_checkbox = alt.binding_checkbox()\ncheckbox_selection = alt.selection_single(bind=input_checkbox, name=\"Big Budget Films\")\n\nsize_checkbox_condition = alt.condition(checkbox_selection,\n                                        alt.SizeValue(25),\n                                        alt.Size('Hundred_Million_Production:Q')\n                                       )\n\nbudget_sizing = base.add_selection(\n    checkbox_selection\n).encode(\n    size=size_checkbox_condition\n).properties(title=\"Checkbox Formatting\")\n\n( filter_year | filter_genres) &  (highlight_ratings | budget_sizing  )"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_notebooks.html#simpler-code-output",
    "href": "posts/Communicating_Code/CommCodePres_notebooks.html#simpler-code-output",
    "title": "Communicating code: Notebooks",
    "section": "Simpler code output",
    "text": "Simpler code output\nx = np.arange(0,np.pi,.01)\ny = np.sin(x)\nplt.plot(x,y)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nx = np.arange(0,np.pi,.01)\ny = np.sin(x)\nplt.plot(x,y)"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_notebooks.html#notebooks-my-view",
    "href": "posts/Communicating_Code/CommCodePres_notebooks.html#notebooks-my-view",
    "title": "Communicating code: Notebooks",
    "section": "Notebooks: my view",
    "text": "Notebooks: my view\nAlthough notebooks have their validish detractors I don’t like notebooks.- Joel Grus Youtube I think if you approach them in the right way they are a super powerful tool.\nThe negatives seem to be:\n\nencourage bad practice in code (a genuine problem)\nissues around order of what cell is run (easily got around with good practice)\nissues around lack of auto complete (I don’t see the issue, use in visual studio autocomplete is there)\nno grammar/spelling correction\nissues with using git and version control\n\nthere are ways around this though"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_notebooks.html#notebook-benefits",
    "href": "posts/Communicating_Code/CommCodePres_notebooks.html#notebook-benefits",
    "title": "Communicating code: Notebooks",
    "section": "Notebook Benefits",
    "text": "Notebook Benefits\n\nNotebooks are intuitive\n\nYou have the code then the result of the code\nPlus can add details of how code works\nAnd it’s linear\n\nCan get things up and working quickly\nAid with communicating code\nEncourages Writing\n\nand writing things down aids thinking in the now and understanding what you did and why in the future\n\nFastAI guide for better blogs\n\nCan use shell commands e.g. !pip install pandas\nCan use magic commands e.g. %%time to time a cell\nEasy to convert code to a pipeline\n\nWith many companies moving towards Python/R from Excel and a varied level of skills. The first of these is particularly important to aid communicating code"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_notebooks.html#communicating-when-code-is-a-large-element-of-what-is-being-presented",
    "href": "posts/Communicating_Code/CommCodePres_notebooks.html#communicating-when-code-is-a-large-element-of-what-is-being-presented",
    "title": "Communicating code: Notebooks",
    "section": "Communicating when code is a large element of what is being presented",
    "text": "Communicating when code is a large element of what is being presented\n\nMicrosoft Word/ppt- type methods aren’t set-up well to include code\nProgramming files (e.g. .py) aren’t set-up well to share\nVideoing code with outputs is an option, but don’t translate to other formats (i.e. we may also need to do a written format of this)\nApps (e.g. streamlit) can be good.\n\nBut the code is hidden\n\nProgramming notebooks (e.g..ipynb) offer a good and easy to share code but with some limitations\n\nAn easier way is to convert the notebooks to html\n\ne.g. maybe someone doesn’t have python installed"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_notebooks.html#example-documenting-code",
    "href": "posts/Communicating_Code/CommCodePres_notebooks.html#example-documenting-code",
    "title": "Communicating code: Notebooks",
    "section": "Example: Documenting Code",
    "text": "Example: Documenting Code\n\nHere is my website for my research project on pesticides in UK food.\nThis is not the same as documentation for a package but there are parallels\n\nThis does a few things:\n\nDocuments the analysis steps I have taken including the code and outputs\n\nUseful for data transparency, useability of the code if needs modifiying/adapting, and why I did XYZ\n\nProvides a way to present the data\n\nThere is a streamlit app, but sometimes I like to be able to see the code"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_notebooks.html#example-tool-to-aid-learning",
    "href": "posts/Communicating_Code/CommCodePres_notebooks.html#example-tool-to-aid-learning",
    "title": "Communicating code: Notebooks",
    "section": "Example: Tool to aid learning",
    "text": "Example: Tool to aid learning\nA big area I have been using Jupyter Notebooks for is to aid learning\n\nIf you want to understand something it helps to write it down\nHaving the code next to it is a big advantage\nAnd if stored on github you can access it anywhere\n\nTensoflow cheat sheet"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_notebooks.html#example-debugging-code",
    "href": "posts/Communicating_Code/CommCodePres_notebooks.html#example-debugging-code",
    "title": "Communicating code: Notebooks",
    "section": "Example: Debugging Code",
    "text": "Example: Debugging Code\n\nSince starting at ONS I have been working with understanding an existing project and latterly adding code to it\nThe project consists of multiple python files across several folders\n\nMy Python was good but lots of the functions and their useage weren’t immediately obvious to me\n\nbreak-points in VS Studio is really good to step through the code and work out what happens in the code.\n\nI had not used before with Python (but had lots with MATLAB), and it’s really useful\n\nBut it can be limited what you can do\n\ndifficult to probe code if want to write more than 1 line of code\nthe experience/knowledge exists as you go through it but no documentation to refer to later, e.g. function X does this when I give it Y etc\n\nBy copying and pasting code into Jupyter cells I could see and document how they worked (e.g. changing inputs)\n\nThis (copying and pasting) would get around code changes too (which would be an issue if modules were just imported)\nbecause this was all done in Jupyter notebook I can have a ipynb code file and a html file showing how the code works\nI could even save a pickle file of the variables at a particularly point to understand how the code would work from this point"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_pres.html#content",
    "href": "posts/Communicating_Code/CommCodePres_pres.html#content",
    "title": "Communicating code: Presentations",
    "section": "Content",
    "text": "Content\n\nQuarto\n\npowerpoint https://quarto.org/docs/presentations/\nhtml\n\nJupyter books\nStreamlit"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_pres.html#creating-the-template",
    "href": "posts/Communicating_Code/CommCodePres_pres.html#creating-the-template",
    "title": "Communicating code: Presentations",
    "section": "Creating the template",
    "text": "Creating the template\n(Office info correct for Office 365 Feb 2023, Version 2301 Build 16.0.16026.20002)\nIf your workplace has a custom template or you have one you always use, you can incorporate this into quarto.\nHowever, quarto is quite specific on the form this template takes, and requires the following elements - Title Slide - Title and Content - Section Header - Two Content - Comparison - Content with Caption - Blank\nBy selecting Layout from the Home tab in powerpoint the different layouts can be seen\n\nThey can then be modified by going to View tab - Slide Master.\nIf using your own template you will need to match the names of the slides given above. These can be found by hovering over the slides on the left or right clicking on one and selecting “Rename Layout”\n\nAlternatively, create a custom template using quarto and then modify this. The following command creates the template:\nquarto pandoc -o template.pptx --print-default-data-file reference.pptx\nThen go to View tab - Slide Master and modify each slide layout.\nNote if you are trying to match a template, some tips: - go to Design -> Slide Size and match this to your template - when View tab - Slide Master is selected go to first tab (see above it will be left indented) on one you are copying from and select all on this then paste to the new template - these will be background images and other things that want to be passed to all slides - Check other slides for images and font-styles etc to match to the new template"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_pres.html#load-the-template",
    "href": "posts/Communicating_Code/CommCodePres_pres.html#load-the-template",
    "title": "Communicating code: Presentations",
    "section": "Load the template",
    "text": "Load the template\nTo load the template the first cell in the notebook needs to be modified as follows to reference the template.pptx file.\nformat:\n  pptx:\n    reference-doc: template.pptx\n    slide-level: 2\nIn addition, we can also specify here the rule by which a new slide is defined. If slide-level: 2 is used a new slide is defined by “##’ and a new section header by ‘#’. So if we used ‘###’ this would be a heading within the slide.\nIf slide-level: 1 is used a new slide is defined by “#’ and ‘##’ this would be a heading within the slide (this is normally the default)."
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_pres.html#check-the-slides",
    "href": "posts/Communicating_Code/CommCodePres_pres.html#check-the-slides",
    "title": "Communicating code: Presentations",
    "section": "Check the slides",
    "text": "Check the slides\nI have found creation of slides to powerpoint more prone to strange results than if .doc/.pdf/.html are used.\nSo check the slides, see if interactive content or code has been included (probably not) and if the slide content goes outside the slide.\n\nIn the example above - There is overlap of text on a slide - Strange ouput of a slide - Code output not displayed"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_pres.html#adding-style-to-revealjs",
    "href": "posts/Communicating_Code/CommCodePres_pres.html#adding-style-to-revealjs",
    "title": "Communicating code: Presentations",
    "section": "Adding style to revealjs",
    "text": "Adding style to revealjs\nA simple way to add template like details to a revealjs file is to add a style.css sheet.\nIn the example below, the style sheet adds logo.png to the bottom right of each sheet\nThe file style.css looks like this:\n.reveal .slide-logo {\n  display: block;\n  position: fixed;\n  top: unset !important;\n  left: unset !important;\n  bottom: 50px;\n  right: 12px;\n  height: 100px !important;\n  width: 100x !important;\n  max-width: unset !important;\n  max-height: unset !important;\n}\nAnd the revealjs part at the top of the jupyter notebook looks like this\nrevealjs:\n    slide-number: true\n    height: 1080\n    width: 1920\n    logo: logo.png\n    css: style.css\nSo this would then look like the following, with the logo (logo.png) in the bottom right, and size and positioning given by the css file"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_pres.html#what-the-revealjs-file-looks-like",
    "href": "posts/Communicating_Code/CommCodePres_pres.html#what-the-revealjs-file-looks-like",
    "title": "Communicating code: Presentations",
    "section": "What the revealjs file looks like",
    "text": "What the revealjs file looks like\nVideo"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_pres.html#streamlit-functionality-overview",
    "href": "posts/Communicating_Code/CommCodePres_pres.html#streamlit-functionality-overview",
    "title": "Communicating code: Presentations",
    "section": "Streamlit Functionality: overview",
    "text": "Streamlit Functionality: overview\nStreamlit allows various functionality:\n\ntextbox\nimages/videos\ncharts/tables\nmenus/buttons\netc"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_pres.html#streamlit-functionality-streamlit_layout",
    "href": "posts/Communicating_Code/CommCodePres_pres.html#streamlit-functionality-streamlit_layout",
    "title": "Communicating code: Presentations",
    "section": "Streamlit Functionality: streamlit_layout",
    "text": "Streamlit Functionality: streamlit_layout\nBut unlike some apps (am thinking MATLAB GUIs) you can’t create the look and functionality separately. So if you want something in a certain position it can be tricky. HTML can be used with st.markdown to give more control but it isn’t recommended to use by streamlit.\nInstead, to create the layout as you would like they have the following features:"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_pres.html#streamlit-functionality-columns-and-sidebar",
    "href": "posts/Communicating_Code/CommCodePres_pres.html#streamlit-functionality-columns-and-sidebar",
    "title": "Communicating code: Presentations",
    "section": "Streamlit Functionality: columns and sidebar",
    "text": "Streamlit Functionality: columns and sidebar\nThe most useable are the first two: columns and sidebar\nColumns allows us to split the app vertically. The code is fairly simple:\nEither colL, colM, colR = st.columns(3) for 3 equal columns or to split columns with different sizes:\ncolL, _, colR = st.columns((10, 5, 20))\nwith colL:\n    st.write('On the left')\nwith colR:\n    st.write('On the right twice as big as left')\nst.sidebar just adds a sidebar to the app that can be hidden or shown.\nAnything in the sidebar is just prefixed by st.sidebar so:\nst.sidebar.write('I am in the sidebar')\nst.write('I am in the main app')\nst.sidebar.write('I am back in the sidebar')"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_pres.html#streamlit-functionality-html",
    "href": "posts/Communicating_Code/CommCodePres_pres.html#streamlit-functionality-html",
    "title": "Communicating code: Presentations",
    "section": "Streamlit Functionality: html",
    "text": "Streamlit Functionality: html\nIt is possible to add various additional personalisations using html. BUT it does come with security risks and so is [not recommended]](https://github.com/streamlit/streamlit/issues/152)\n\nBut it does allow much more control over the layout of the app that can be useful for a presentation: - Can add a background image - Can add background color to a textbox - Control over positioning of widgets - lots more\nHTML is implementated using st.markdown with unsafe_allow_html=True inside the former"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_pres.html#streamlit-functionality-html-examples",
    "href": "posts/Communicating_Code/CommCodePres_pres.html#streamlit-functionality-html-examples",
    "title": "Communicating code: Presentations",
    "section": "Streamlit Functionality: html examples",
    "text": "Streamlit Functionality: html examples\nadd background to a text box\ntext = \"Code Examples\"\n        st.markdown(f'<center><p style=font-family:\"Calibri\";background-color:#FFFFFF;color:#000000;font-size:42px;border-radius:10%><b>{text}</b></p></center>', unsafe_allow_html=True)\n\nOr to add a background image\nimport streamlit as st\nimport base64\n\n@st.cache(allow_output_mutation=True)\ndef get_base64_of_bin_file(bin_file):\n    with open(bin_file, 'rb') as f:\n        data = f.read()\n    return base64.b64encode(data).decode()\n\ndef set_png_as_page_bg(png_file):\n    bin_str = get_base64_of_bin_file(png_file) \n    page_bg_img = '''\n    <style>\n    .stApp {\n    background-image: url(\"data:image/png;base64,%s\");\n    background-size: contain;\n    background-repeat: no-repeat;\n    background-attachment: scroll; # doesn't work\n    }\n    </style>\n    ''' % bin_str\n    st.markdown(page_bg_img, unsafe_allow_html=True)\n    return"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_pres.html#streamlit-functionality-echo",
    "href": "posts/Communicating_Code/CommCodePres_pres.html#streamlit-functionality-echo",
    "title": "Communicating code: Presentations",
    "section": "Streamlit Functionality: echo",
    "text": "Streamlit Functionality: echo\n\nSometimes you want your Streamlit app to contain both your usual Streamlit graphic elements and the code that generated those elements. That’s where st.echo() comes in\n\nEasier to display this by an example:\n\nIn the example above the right of the image is given below (st.columns is used, where the input for the function is found from the left column).\n\nst.echo is used with the with statement.\neverything within the with is printed to the screen and executed\n\nwith st.echo():\n    # Everything inside this block will be both printed to the screen\n    # and executed.\n\n    def do_pd_replace(text, text_search, text_sub):\n        col_name = \"Start string\"\n        df = pd.DataFrame(data=[text], columns=[col_name])\n\n        df[\"Final String\"] = df[col_name].replace(\n            text_search, text_sub, regex=True\n        )\n\n        st.dataframe(df)\n        st.write(f\"text_search = '{text_search}' and text_sub = '{text_sub}'\")\n        st.write(f\"Input string = '{text}'\")\n        st.write(f\"Output string = '{df['Final String'].values[0]}'\")\n\n    do_pd_replace(text, text_search, text_sub)"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_pres.html#streamlit-functionality-pages",
    "href": "posts/Communicating_Code/CommCodePres_pres.html#streamlit-functionality-pages",
    "title": "Communicating code: Presentations",
    "section": "Streamlit Functionality: pages",
    "text": "Streamlit Functionality: pages\nBy simply creating a folder called pages and putting other streamlit .py files in the folder they can then be accessed in the sidebar.\n\nA main file needs to be outside the pages folder\nThe .py files in pages behave as if they were outside the folder (i.e. when loading files/functions)"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_pres.html#example-streamlit-presentation",
    "href": "posts/Communicating_Code/CommCodePres_pres.html#example-streamlit-presentation",
    "title": "Communicating code: Presentations",
    "section": "Example Streamlit Presentation",
    "text": "Example Streamlit Presentation\nVideo"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_tables.html#excel-files",
    "href": "posts/Communicating_Code/CommCodePres_tables.html#excel-files",
    "title": "Communicating code: Tables",
    "section": "Excel files",
    "text": "Excel files\n\nChatGPT: What are the problems of Excel and xlsx files?\n\n\nLimited scalability: Excel has a limit on the number of rows and columns it can handle effectively.\nInaccurate data: The manual nature of data entry in Excel can lead to errors and inaccuracies.\nFragility: Complex Excel spreadsheets can break easily if any formula or cell reference is altered.\nLack of security: Excel files are vulnerable to unauthorized access and malicious attacks.\nSlow performance: Large and complex Excel files can become slow to open and use, leading to decreased productivity.\nCompatibility issues: XLSX files may not be compatible with older versions of Excel, or with other software applications.\nLimited collaboration: Sharing Excel files and making changes to them can be difficult, especially when multiple people are involved.\n\nFor me it is the Slow performance alongside: (a) we are doing the data manipulation outside Excel anyway and (b) having to have another application open\n\nods with Excel\nAbout 10 s to open 3 ods files with Excel\n\n\n.ods converted to html in firefox browser\nAbout 6 s to open 3 converted ods files in a browser\n\n\n.ods converted to html firefox browser no new tabs\nAlmost instant when open converted ods files in same browser"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_tables.html#what-aspect-of-tables-i-am-considering",
    "href": "posts/Communicating_Code/CommCodePres_tables.html#what-aspect-of-tables-i-am-considering",
    "title": "Communicating code: Tables",
    "section": "What aspect of tables I am considering",
    "text": "What aspect of tables I am considering"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_tables.html#convert-xlsx-to-html",
    "href": "posts/Communicating_Code/CommCodePres_tables.html#convert-xlsx-to-html",
    "title": "Communicating code: Tables",
    "section": "Convert xlsx to html?",
    "text": "Convert xlsx to html?\n\nOpening xlsx files in Excel is slow\nConverting to html if we don’t want to edit could be an option\nIf we are moving to Python/R aren’t non-Excel options worth considering??\n\nConverting xlsx files to html\n\nSeems the most obvious course\nBut it doesn’t seem that easy with code\n\nbut easy within Excel"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_tables.html#what-does-chatgpt-say-to-convert-excel-file",
    "href": "posts/Communicating_Code/CommCodePres_tables.html#what-does-chatgpt-say-to-convert-excel-file",
    "title": "Communicating code: Tables",
    "section": "What does chatgpt say to convert excel file?",
    "text": "What does chatgpt say to convert excel file?\n\nConvert an excel file to html with python"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_tables.html#what-does-chatgpt-say-without-pandas",
    "href": "posts/Communicating_Code/CommCodePres_tables.html#what-does-chatgpt-say-without-pandas",
    "title": "Communicating code: Tables",
    "section": "What does chatgpt say without pandas?",
    "text": "What does chatgpt say without pandas?\n\nconvert excel file to html in python without pandas include the excel formatting such as column width\n\n\n\nimport pandas as pd\n\nimport os\nfrom pathlib import Path\nimport sys\n\nmodule_path = Path( os.getcwd() )\nmodule_path = module_path.parent.parent.parent.__str__() + '\\\\Pesticide'\n\ncwd = module_path\n\nfolder_path = os.path.join(cwd,'data')\n\nsys.path.insert(0, module_path)\n\ndf2 = pd.read_csv(os.path.join(folder_path,'combined_df.csv') ,index_col=0 )\n# change data type of columns\ndf2['date_of_sampling'] = pd.to_datetime(df2['date_of_sampling'])"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_tables.html#pandas",
    "href": "posts/Communicating_Code/CommCodePres_tables.html#pandas",
    "title": "Communicating code: Tables",
    "section": "pandas",
    "text": "pandas\n\nSince (in Python) we are mainly working with pandas. Let’s consider how pandas outputs can be modified.\npandas options\n\nSome code functionality\n# precision of all columns\npd.set_option(\"display.precision\", 2)\n# Or map as a string\ndf2['amount_pc_str'] = df2['amount_pc'].map(lambda x: '%.3f' % x)\n# some other options\npd.set_option('max_colwidth', 20)\npd.set_option('display.max_columns', None)\npd.set_option('display.expand_frame_repr', False)\npd.set_option('max_colwidth', 0)"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_tables.html#pandas-basic",
    "href": "posts/Communicating_Code/CommCodePres_tables.html#pandas-basic",
    "title": "Communicating code: Tables",
    "section": "pandas basic",
    "text": "pandas basic\n\ndf2\n\n\n\n\n\n  \n    \n      \n      sample_id\n      date_of_sampling\n      description\n      country_of_origin\n      retail_outlet\n      address\n      brand_name\n      packer_/_manufacturer_/_importer\n      product\n      address_postcode\n      packer_postcode\n      address_area\n      packer_area\n      chem_name\n      amount_detected\n      mrl\n      amount_pc\n    \n  \n  \n    \n      0\n      1958/2016\n      2016-08-08\n      Bramley Apples\n      UK\n      Asda\n      Creechbarrow Road, Taunton TA1 2AN\n      Asda\n      Asda Stores Ltd Leeds, UK LS11 5AD\n      Apple\n      TA1 2AN\n      LS11 5AD\n      Somerset\n      West Yorkshire\n      boscalid\n      0.03\n      2.0\n      0.015\n    \n    \n      1\n      1958/2016\n      2016-08-08\n      Bramley Apples\n      UK\n      Asda\n      Creechbarrow Road, Taunton TA1 2AN\n      Asda\n      Asda Stores Ltd Leeds, UK LS11 5AD\n      Apple\n      TA1 2AN\n      LS11 5AD\n      Somerset\n      West Yorkshire\n      pyraclostrobin\n      0.01\n      0.5\n      0.020\n    \n    \n      2\n      0230/2016\n      2016-08-08\n      Bramley Apples\n      UK\n      Co-op\n      Northgate, Louth LN11 0LT\n      Co-op\n      Co-operative Group Ltd Manchester M60 0AG\n      Apple\n      LN11 0LT\n      M60 0AG\n      Lincolnshire\n      Greater Manchester\n      boscalid\n      0.05\n      2.0\n      0.025\n    \n    \n      3\n      0230/2016\n      2016-08-08\n      Bramley Apples\n      UK\n      Co-op\n      Northgate, Louth LN11 0LT\n      Co-op\n      Co-operative Group Ltd Manchester M60 0AG\n      Apple\n      LN11 0LT\n      M60 0AG\n      Lincolnshire\n      Greater Manchester\n      flonicamid (sum)\n      0.02\n      0.2\n      0.100\n    \n    \n      4\n      0230/2016\n      2016-08-08\n      Bramley Apples\n      UK\n      Co-op\n      Northgate, Louth LN11 0LT\n      Co-op\n      Co-operative Group Ltd Manchester M60 0AG\n      Apple\n      LN11 0LT\n      M60 0AG\n      Lincolnshire\n      Greater Manchester\n      pyraclostrobin\n      0.03\n      0.5\n      0.060\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      35155\n      2858/2020 Organic\n      2020-10-20\n      Organic Sweet Potatoes\n      Spain\n      Tesco\n      300 Beverley Way, New Malden KT3 4PJ\n      Tesco\n      Tesco Stores Ltd Welwyn Garden City AL7 1GA\n      Sweet_Potatoes_Q4_(BNA)\n      KT3 4PJ\n      AL7 1GA\n      Greater London\n      Hertfordshire\n      0\n      0.00\n      0.0\n      0.000\n    \n    \n      35156\n      0562/2020 Organic\n      2020-10-05\n      Organic Duchy Sweet Potatoes\n      Egypt\n      Waitrose\n      Mill Lane, Swindon SN1 7BX\n      Waitrose\n      Waitrose Ltd Doncastle Road, Bracknell, Berksh...\n      Sweet_Potatoes_Q4_(BNA)\n      SN1 7BX\n      RG12 8YA\n      Wiltshire\n      Berkshire\n      0\n      0.00\n      0.0\n      0.000\n    \n    \n      35157\n      0563/2020\n      2020-10-05\n      Sweet Potatoes\n      USA\n      Waitrose\n      Mill Lane, Swindon SN1 7BX\n      Waitrose\n      Waitrose Ltd Doncastle Road, Bracknell, Berksh...\n      Sweet_Potatoes_Q4_(BNA)\n      SN1 7BX\n      RG12 8YA\n      Wiltshire\n      Berkshire\n      0\n      0.00\n      0.0\n      0.000\n    \n    \n      35158\n      2601/2020\n      2020-10-14\n      Sweet Potatoes\n      USA\n      Waitrose\n      Ossington Way, Newark NG24 1FF\n      Waitrose\n      Waitrose Ltd Doncastle Road, Bracknell, Berksh...\n      Sweet_Potatoes_Q4_(BNA)\n      NG24 1FF\n      RG12 8YA\n      Nottinghamshire\n      Berkshire\n      0\n      0.00\n      0.0\n      0.000\n    \n    \n      35159\n      2601/2020\n      2020-10-14\n      Sweet Potatoes\n      USA\n      Waitrose\n      Ossington Way, Newark NG24 1FF\n      Waitrose\n      Waitrose Ltd Doncastle Road, Bracknell, Berksh...\n      Sweet_Potatoes_Q4_(BNA)\n      NG24 1FF\n      RG12 8YA\n      Nottinghamshire\n      Berkshire\n      0\n      0.00\n      0.0\n      0.000\n    \n  \n\n35160 rows × 17 columns"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_tables.html#pandas-overview",
    "href": "posts/Communicating_Code/CommCodePres_tables.html#pandas-overview",
    "title": "Communicating code: Tables",
    "section": "pandas overview",
    "text": "pandas overview\n\nUsing pandas we can control various outputs\nBut these still need a format to display within\nAnd display functionality is not easy\n\nOr convert to a html file\ndf2.iloc[:500].to_html('df2_500.html')\nBut using a style sheet as shown in stack overflow by Parfait\n\ndf_out = df2.iloc[:500].copy()\n\npd.set_option('colheader_justify', 'center')   # FOR TABLE <th>\n\nhtml_string = '''\n<html>\n  <head><title>HTML Pandas Dataframe with CSS</title></head>\n  <link rel=\"stylesheet\" type=\"text/css\" href=\"df_style.css\"/>\n  <body>\n    {table}\n  </body>\n</html>.\n'''\n\n# OUTPUT AN HTML FILE\nwith open('df2_500.html', 'w') as f:\n    f.write(html_string.format(table=df_out.to_html(classes='mystyle')))"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_tables.html#section",
    "href": "posts/Communicating_Code/CommCodePres_tables.html#section",
    "title": "Communicating code: Tables",
    "section": "",
    "text": "https://www.python-excel.org/"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_tables.html#ipydatagrid",
    "href": "posts/Communicating_Code/CommCodePres_tables.html#ipydatagrid",
    "title": "Communicating code: Tables",
    "section": "ipydatagrid",
    "text": "ipydatagrid\nhttps://github.com/bloomberg/ipydatagrid\n\n\nfrom ipydatagrid import DataGrid, TextRenderer, VegaExpr\nimport ipydatagrid\ndatagrid = DataGrid(df2, selection_mode=\"cell\", editable=True,\n                   base_row_size=32, base_column_size=150)\n\ndatagrid = DataGrid(df2, base_row_size=30, base_column_size=150)\ndatagrid"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_tables.html#itables-code",
    "href": "posts/Communicating_Code/CommCodePres_tables.html#itables-code",
    "title": "Communicating code: Tables",
    "section": "itables code",
    "text": "itables code\nfrom itables import init_notebook_mode\n\nimport itables\ninit_notebook_mode(all_interactive=True)\n\nitables.show(df2)\n\nfrom itables import init_notebook_mode\n\nimport itables\ninit_notebook_mode(all_interactive=True)\n\nitables.show(df2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n      \n      sample_id\n      date_of_sampling\n      description\n      country_of_origin\n      retail_outlet\n      address\n      brand_name\n      packer_/_manufacturer_/_importer\n      product\n      address_postcode\n      packer_postcode\n      address_area\n      packer_area\n      chem_name\n      amount_detected\n      mrl\n      amount_pc\n    \n  Loading... (need help?)"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_tables.html#dash",
    "href": "posts/Communicating_Code/CommCodePres_tables.html#dash",
    "title": "Communicating code: Tables",
    "section": "Dash",
    "text": "Dash\nhttps://dash.plotly.com/datatable\n\nDownloaded 800,000 times per month, Dash is the original low-code framework for rapidly building data apps in Python, R, Julia, and F# (experimental).\n\nhttps://medium.com/plotly/introducing-jupyterdash-811f1f57c02e\n\nimport plotly.express as px\nfrom jupyter_dash import JupyterDash\nimport dash_core_components as dcc\nimport dash_html_components as html\nfrom dash.dependencies import Input, Output# Load Data\ndf = px.data.tips()# Build App\napp = JupyterDash(__name__)\napp.layout = html.Div([\n    html.H1(\"JupyterDash Demo\"),\n    dcc.Graph(id='graph'),\n    html.Label([\n        \"colorscale\",\n        dcc.Dropdown(\n            id='colorscale-dropdown', clearable=False,\n            value='plasma', options=[\n                {'label': c, 'value': c}\n                for c in px.colors.named_colorscales()\n            ])\n    ]),\n])# Define callback to update graph\n@app.callback(\n    Output('graph', 'figure'),\n    [Input(\"colorscale-dropdown\", \"value\")]\n)\ndef update_figure(colorscale):\n    return px.scatter(\n        df, x=\"total_bill\", y=\"tip\", color=\"size\",\n        color_continuous_scale=colorscale,\n        render_mode=\"webgl\", title=\"Tips\"\n    )# Run app and display result inline in the notebook\napp.run_server(mode='inline')\n\nC:\\Users\\44781\\AppData\\Local\\Temp\\ipykernel_15260\\3294666565.py:3: UserWarning: \nThe dash_core_components package is deprecated. Please replace\n`import dash_core_components as dcc` with `from dash import dcc`\n  import dash_core_components as dcc\nC:\\Users\\44781\\AppData\\Local\\Temp\\ipykernel_15260\\3294666565.py:4: UserWarning: \nThe dash_html_components package is deprecated. Please replace\n`import dash_html_components as html` with `from dash import html`\n  import dash_html_components as html"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_tables.html#streamlit",
    "href": "posts/Communicating_Code/CommCodePres_tables.html#streamlit",
    "title": "Communicating code: Tables",
    "section": "Streamlit",
    "text": "Streamlit\n\nA faster way to build and share data apps\n\n\nDash can be run within a notebook but is principally an app.\nStreamlit is a similar app.\nBut much easier to code.\n\nimport pandas as pd\nimport streamlit as st\nall_dfs = pd.read_csv(\"./data/combined_df.csv\")\nst.dataframe(all_dfs.head())"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_tables.html#and-more",
    "href": "posts/Communicating_Code/CommCodePres_tables.html#and-more",
    "title": "Communicating code: Tables",
    "section": "And more",
    "text": "And more\n\nDataTables\n\nDataTables is a plug-in for the jQuery Javascript library. It is a highly flexible tool, built upon the foundations of progressive enhancement, that adds all of these advanced features to any HTML table.\n\n\n\nJupyter widgets\nIf you are looking for Jupyter widgets, have a look at (taken from https://mwouts.github.io/itables/references.html) - QGrid by Quantopian - IPyaggrid by Louis Raison and Olivier Borderies - IPySheet by QuantStack."
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#content",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#content",
    "title": "Presentation: Communicating code and data",
    "section": "Content",
    "text": "Content\n\nNotebooks\n\nWhat are they?\nExamples\nPros and cons\n\nApps\n\nVoila\nStreamlit\n\nWebsites and HTML\n\nConverting notebooks to HTML and websites\n\nPresentations\n\nUsing notebooks for presentations\n\nTabular Data\n\nComments on Excel\nThoughts on code alternatives"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#jupyter-notebooks",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#jupyter-notebooks",
    "title": "Presentation: Communicating code and data",
    "section": "Jupyter Notebooks",
    "text": "Jupyter Notebooks\nFrom TalkPython: Awesome Jupyter Libraries and Extensions\n\nJupyter is an amazing environment for exploring data and generating executable reports with Python. But there are many external tools, extensions, and libraries to make it so much better and make you more productive.\n\n\nA notebook consists of two parts\n\nmarkdown part where we can:\n\nwrite text, add images, links, html, LaTeX etc\n\ncode part which runs and displays output of code\n\n\nSome links:\n\nJupyter Book\nA curated list of awesome Jupyter projects\nCode Documentation QA of Code\nFastAI guide for better blogs"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#example-of-a-notebook",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#example-of-a-notebook",
    "title": "Presentation: Communicating code and data",
    "section": "Example of a notebook",
    "text": "Example of a notebook\nAn example notebook"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#markdown-in-a-notebook-1",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#markdown-in-a-notebook-1",
    "title": "Presentation: Communicating code and data",
    "section": "Markdown in a notebook 1",
    "text": "Markdown in a notebook 1\nSome useful commands:\n\n# Notebooks General and ## Markdown in a notebook 1\n![](ghtop_images/pest.png) looks like this"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#markdown-in-a-notebook-2",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#markdown-in-a-notebook-2",
    "title": "Presentation: Communicating code and data",
    "section": "Markdown in a notebook 2",
    "text": "Markdown in a notebook 2\n\nAnd the same with a mp4 file ![](ghtop_images/revealjs.mp4)\nOr a youtube video \"\""
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#markdown-in-a-notebook-3",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#markdown-in-a-notebook-3",
    "title": "Presentation: Communicating code and data",
    "section": "Markdown in a notebook 3",
    "text": "Markdown in a notebook 3\n\n> If we want text like this\n\n\nIf we want text like this\n\n\nOr if we want code use `a = b + c`\n\nor:\n```\na = b\na = a + c\n```\n\na = b\n\na = a + c"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#markdown-in-a-notebook-4",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#markdown-in-a-notebook-4",
    "title": "Presentation: Communicating code and data",
    "section": "Markdown in a notebook 4",
    "text": "Markdown in a notebook 4\n\nHTML works too\n\n<img src=\"ghtop_images/pest.png\"></img>"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#code-in-a-notebook",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#code-in-a-notebook",
    "title": "Presentation: Communicating code and data",
    "section": "Code in a notebook",
    "text": "Code in a notebook\nExample interactive format using altair:"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#simpler-code-output",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#simpler-code-output",
    "title": "Presentation: Communicating code and data",
    "section": "Simpler code output",
    "text": "Simpler code output\nx = np.arange(0,np.pi,.01)\ny = np.sin(x)\nplt.plot(x,y)"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#other-code-stuff",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#other-code-stuff",
    "title": "Presentation: Communicating code and data",
    "section": "Other code stuff",
    "text": "Other code stuff\n\nCan use shell commands e.g. !pip install pandas\nCan use magic commands e.g. %%time to time a cell\n\n%%time\ny=0\nfor x in range(0,100):\n    y+=x\nprint(f\"y is {y}\")\n\n\ny is 4950\nCPU times: total: 0 ns\nWall time: 0 ns"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#but-not-everyone-loves-notebooks",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#but-not-everyone-loves-notebooks",
    "title": "Presentation: Communicating code and data",
    "section": "But Not everyone loves notebooks :(",
    "text": "But Not everyone loves notebooks :(\nNotebooks have their validish detractors I don’t like notebooks.- Joel Grus Youtube"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#notebooks-opinion",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#notebooks-opinion",
    "title": "Presentation: Communicating code and data",
    "section": "Notebooks Opinion",
    "text": "Notebooks Opinion\nAlthough notebooks have their validish detractors I don’t like notebooks.- Joel Grus Youtube I think if you approach them in the right way they are a super powerful tool.\nThe negatives seem to be:\n\nencourage bad practice in code (a genuine problem)\nissues around order of what cell is run (easily got around with good practice)\nissues around lack of auto complete (I don’t see the issue, use in visual studio autocomplete is there)\nno grammar/spelling correction\nissues with using git and version control\n\nthere are ways around this though\n\nanything else?"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#notebook-benefits",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#notebook-benefits",
    "title": "Presentation: Communicating code and data",
    "section": "Notebook Benefits",
    "text": "Notebook Benefits\n\nNotebooks are intuitive\n\nYou have the code then the result of the code\nCan add text or images\nAnd it’s linear\n\nCan get things up and working quickly\nAid with communicating code\nEncourages Writing\n\nand writing things down aids thinking in the now and understanding what you did and why in the future\n\nFastAI guide for better blogs\n\nEasy to convert code to a pipeline\nTreat it as a notebook\n\nin the same way an artist would have a sketchbook to help make final piece\n\nWith many companies moving towards Python/R from Excel and a varied level of skills.\n\n“Aid with communicating code” is particularly important"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#example-documenting-code",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#example-documenting-code",
    "title": "Presentation: Communicating code and data",
    "section": "Example: Documenting Code",
    "text": "Example: Documenting Code\n\nHere is my website for my research project on pesticides in UK food\nThis is not the same as documentation for a package but there are parallels\n\nThis does a few things:\n\nDocuments the analysis steps I have taken including the code and outputs\n\nUseful for data transparency, useability of the code if needs modifiying/adapting, and why I did XYZ\n\nProvides a way to present the data\n\nThere is a streamlit app, but sometimes I like to be able to see the code"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#example-tool-to-aid-learning",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#example-tool-to-aid-learning",
    "title": "Presentation: Communicating code and data",
    "section": "Example: Tool to aid learning",
    "text": "Example: Tool to aid learning\nA big area I have been using Jupyter Notebooks for is to aid learning\n\nIf you want to understand something it helps to write it down\nHaving the code next to it is a big advantage\nAnd if stored on github you can access it anywhere\n\nTensoflow cheat sheet"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#example-debugging-code",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#example-debugging-code",
    "title": "Presentation: Communicating code and data",
    "section": "Example: Debugging Code",
    "text": "Example: Debugging Code\n\nSince starting at ONS I have been working with understanding an existing project and latterly adding code to it\nThe project consists of multiple python files across several folders\n\nMy Python was good but lots of the functions and their useage weren’t immediately obvious to me\n\nbreak-points in VS Studio is really good to step through the code and work out what happens in the code.\n\nI had not used before with Python (but had lots with MATLAB), and it’s really useful\n\nBut it can be limited what you can do\n\ndifficult to probe code if want to write more than 1 line of code\nthe experience/knowledge exists as you go through it but no documentation to refer to later, e.g. function X does this when I give it Y etc"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#example-debugging-code-2",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#example-debugging-code-2",
    "title": "Presentation: Communicating code and data",
    "section": "Example: Debugging Code 2",
    "text": "Example: Debugging Code 2\n\nBy copying and pasting code into Jupyter cells I could see and document how they worked (e.g. changing inputs)\n\nThis (copying and pasting) would get around code changes too (which would be an issue if modules were just imported)\nbecause this was all done in Jupyter notebook I can have a ipynb code file and a html file showing how the code works\nI could even save a pickle file of the variables at a particularly point to understand how the code would work from this point"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#apps-overview",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#apps-overview",
    "title": "Presentation: Communicating code and data",
    "section": "Apps Overview",
    "text": "Apps Overview\nThere are many packages that can be used to convert python code to an app\nUsing Notebooks directly\n\nVoila\nmercury\n\nApps without notebooks\n\nPySimpleGUI\n\nSimple and useful but not the best for displaying data\n\nDash\n\nLooks really good, but also super complicated\n\nStreamlit\n\nEasy and looks good"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#voila",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#voila",
    "title": "Presentation: Communicating code and data",
    "section": "Voila",
    "text": "Voila\n\nVoila is relatively simple to use\nrun with something like voila .\\Excel_Voila.ipynb\nconverts notebook to an app\ncan use things like ipywidgets for interactivity\nthe reason I’m interested in it’s use is streamlit doesn’t seem to give flexibility to modify table output I’d like"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#streamlit-overview",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#streamlit-overview",
    "title": "Presentation: Communicating code and data",
    "section": "Streamlit Overview",
    "text": "Streamlit Overview\n\nStreamlit is an open-source Python library that makes it easy to create and share beautiful, custom web apps for machine learning and data science. In just a few minutes you can build and deploy powerful data apps. So let’s get started!\n\nPrincipally used to create apps, but some of the functionality works well for code/data presentations"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#streamlit-functionality-overview",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#streamlit-functionality-overview",
    "title": "Presentation: Communicating code and data",
    "section": "Streamlit Functionality: overview",
    "text": "Streamlit Functionality: overview\nStreamlit allows various functionality:\n\ntextbox\nimages/videos\ncharts/tables\nmenus/buttons\netc"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#streamlit-functionality-streamlit_layout",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#streamlit-functionality-streamlit_layout",
    "title": "Presentation: Communicating code and data",
    "section": "Streamlit Functionality: streamlit_layout",
    "text": "Streamlit Functionality: streamlit_layout\nBut unlike some apps (am thinking MATLAB GUIs) you can’t create the look and functionality separately. So if you want something in a certain position it can be tricky. HTML can be used with st.markdown to give more control but it isn’t recommended to use by streamlit.\nInstead, to create the layout as you would like they have the following features:"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#streamlit-functionality-columns-and-sidebar",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#streamlit-functionality-columns-and-sidebar",
    "title": "Presentation: Communicating code and data",
    "section": "Streamlit Functionality: columns and sidebar",
    "text": "Streamlit Functionality: columns and sidebar\nThe most useable are the first two: columns and sidebar\nColumns allows us to split the app vertically. The code is fairly simple:\nEither colL, colM, colR = st.columns(3) for 3 equal columns or to split columns with different sizes:\ncolL, _, colR = st.columns((10, 5, 20))\nwith colL:\n    st.write('On the left')\nwith colR:\n    st.write('On the right twice as big as left')\nst.sidebar just adds a sidebar to the app that can be hidden or shown.\nAnything in the sidebar is just prefixed by st.sidebar so:\nst.sidebar.write('I am in the sidebar')\nst.write('I am in the main app')\nst.sidebar.write('I am back in the sidebar')"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#streamlit-functionality-html",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#streamlit-functionality-html",
    "title": "Presentation: Communicating code and data",
    "section": "Streamlit Functionality: html",
    "text": "Streamlit Functionality: html\nIt is possible to add various additional personalisations using html. - BUT it does come with security risks and so is [not recommended]](https://github.com/streamlit/streamlit/issues/152)\n\nAllows much more control over the layout of the app that can be useful for a presentation: - Can add a background image - Can add background color to a textbox - Control over positioning of widgets - lots more\nHTML is implementated using st.markdown with unsafe_allow_html=True inside the former"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#streamlit-functionality-html-examples",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#streamlit-functionality-html-examples",
    "title": "Presentation: Communicating code and data",
    "section": "Streamlit Functionality: html examples",
    "text": "Streamlit Functionality: html examples\nadd background to a text box\ntext = \"Code Examples\"\n        st.markdown(f'<center><p style=font-family:\"Calibri\";background-color:#FFFFFF;color:#000000;font-size:42px;border-radius:10%><b>{text}</b></p></center>', unsafe_allow_html=True)\n\nOr to add a background image\nimport streamlit as st\nimport base64\n\n@st.cache(allow_output_mutation=True)\ndef get_base64_of_bin_file(bin_file):\n    with open(bin_file, 'rb') as f:\n        data = f.read()\n    return base64.b64encode(data).decode()\n\ndef set_png_as_page_bg(png_file):\n    bin_str = get_base64_of_bin_file(png_file) \n    page_bg_img = '''\n    <style>\n    .stApp {\n    background-image: url(\"data:image/png;base64,%s\");\n    background-size: contain;\n    background-repeat: no-repeat;\n    background-attachment: scroll; # doesn't work\n    }\n    </style>\n    ''' % bin_str\n    st.markdown(page_bg_img, unsafe_allow_html=True)\n    return"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#streamlit-functionality-echo",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#streamlit-functionality-echo",
    "title": "Presentation: Communicating code and data",
    "section": "Streamlit Functionality: echo",
    "text": "Streamlit Functionality: echo\n\nSometimes you want your Streamlit app to contain both your usual Streamlit graphic elements and the code that generated those elements. That’s where st.echo() comes in\n\nEasier to display this by an example:\n\nIn the example above the right of the image is given below (st.columns is used, where the input for the function is found from the left column).\n\nst.echo is used with the with statement.\neverything within the with is printed to the screen and executed\n\nwith st.echo():\n    # Everything inside this block will be both printed to the screen\n    # and executed.\n\n    def do_pd_replace(text, text_search, text_sub):\n        col_name = \"Start string\"\n        df = pd.DataFrame(data=[text], columns=[col_name])\n\n        df[\"Final String\"] = df[col_name].replace(\n            text_search, text_sub, regex=True\n        )\n\n        st.dataframe(df)\n        st.write(f\"text_search = '{text_search}' and text_sub = '{text_sub}'\")\n        st.write(f\"Input string = '{text}'\")\n        st.write(f\"Output string = '{df['Final String'].values[0]}'\")\n\n    do_pd_replace(text, text_search, text_sub)"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#streamlit-functionality-pages",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#streamlit-functionality-pages",
    "title": "Presentation: Communicating code and data",
    "section": "Streamlit Functionality: pages",
    "text": "Streamlit Functionality: pages\nBy simply creating a folder called pages and putting other streamlit .py files in the folder they can then be accessed in the sidebar.\n\nA main file needs to be outside the pages folder\nThe .py files in pages behave as if they were outside the folder (i.e. when loading files/functions)"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#example-streamlit-presentation",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#example-streamlit-presentation",
    "title": "Presentation: Communicating code and data",
    "section": "Example Streamlit Presentation",
    "text": "Example Streamlit Presentation\nVideo"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#what-i-have-used-to-convert-notebooks-to-html",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#what-i-have-used-to-convert-notebooks-to-html",
    "title": "Presentation: Communicating code and data",
    "section": "What I have used to convert notebooks to html",
    "text": "What I have used to convert notebooks to html\n\nfastpages\n\nI have used fastpages, but this is now deprecated and they are recommending the use of quarto\n\nquarto\n\nSo far I have found quarto really good and flexible (N.B. R works too)\nEasy to convert a notebook to multiple formats, including html, powerpoint, pdf, word doc\nBUT Quarto is not possible if installing from non pip sources is an issue (as far as I can tell currently)\n\nnbconvert is another option I tried\n\nbut it doesn’t seem to have the functionality of fastpages or quarto\n\nJupyter Books seems to be the best option within companies with installation issues\n\nMaybe not as good as quarto but it works!"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#others",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#others",
    "title": "Presentation: Communicating code and data",
    "section": "Others",
    "text": "Others\n\nI know some people use Sphinx,\n\nis recommended by QA\nFrom what I can tell sphinx on it’s own is not as easy to use as notebooks\nBut there is a jupyter extension nbsphinx\nJupyter Books uses Sphinx heavily under the hood\n\nnbdev\n\nI think is connected to quarto\n\nVoila\n\nVoilà turns Jupyter notebooks into standalone web applications.\nLooks good, bit like streamlit\nbut seems to interfere with other libraries\nmercury seems similar"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#creating-html-other-formats",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#creating-html-other-formats",
    "title": "Presentation: Communicating code and data",
    "section": "Creating html (& other formats)",
    "text": "Creating html (& other formats)\nQuarto\nInstallation is via a package i.e. .msi for Windows or .pkg for Mac. Which can cause issues.\nWorks with both ipynb and qmd files, which are both a mixture of markdown and executable code.\nThe only thing that needs to be done with the notebook is add a YAML block at the start of the notebook, like the following (raq not markdown was used):\n---\ntitle: \"Communicating code: Website\"\nsubtitle: \"Using the notebook format for a website\"\nauthor: \"Thomas H. Simm\"\nformat:\n  html:\n    toc: true\ntitle-slide-attributes:\n  data-background-size: contain\n  data-background-opacity: \"0.5\"\njupyter: python3\n---\nWe can create different files from this .ipynb Jupyter notebook using the following code:\n\nquarto render testPres.ipynb --to pptx\nquarto render testPres.ipynb --to pdf\nquarto render testPres.ipynb --to html\nquarto render testPres.ipynb --to revealjs"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#quarto-1",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#quarto-1",
    "title": "Presentation: Communicating code and data",
    "section": "Quarto",
    "text": "Quarto\nFurther, formatting for projects (i.e. for website) can be done within the configuration file _quarto.yml\nproject:\n  type: website\n  output-dir: _site\n\nwebsite:\n  title: \"ThomasHSimm\"\n  favicon: /posts/Picture3.png\n  body-header: <img src=\"/posts/header2.png\" height=200>\n\n  navbar:\n    right:\n      - about.qmd\n      - icon: github\n        href: https://github.com/ThomasHSimm\n      - icon: mortarboard-fill\n        href: https://scholar.google.com/citations?hl=en&user=HdPDn1sAAAAJ\nformat:\n  html:\n    theme: \n      light: flatly\n      dark: darkly\n    css: styles.css"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#jupyter-books",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#jupyter-books",
    "title": "Presentation: Communicating code and data",
    "section": "Jupyter Books",
    "text": "Jupyter Books\nWe can create different files from this .ipynb Jupyter notebook using the following code:\n\njupyter-book build .\\PesticideDocs\\\njupyter-book build <path-to-book>\njupyter-book build <path-to-book> --builder pdfhtml\njupyter-book build <path-to-book> --builder singlehtml\n\nThe only difference in notebook is that it needs to have One header in a markdown cell for the table of contents, e.g. \n# Title of page"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#configuration-file",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#configuration-file",
    "title": "Presentation: Communicating code and data",
    "section": "Configuration file",
    "text": "Configuration file\nA seperate files _config.yml is used to define how the html (or other) files will look\n# Book settings\n# Learn more at https://jupyterbook.org/customize/config.html\n\ntitle: Defra Pesticide Testing, Data Analysis\nauthor: Thomas Simm\nlogo: ONS-logo.png\nexclude_patterns: [_build, Thumbs.db, .DS_Store, \"**.ipynb_checkpoints\"]\n\n\n# Force re-execution of notebooks on each build.\n# See https://jupyterbook.org/content/execute.html\nexecute:\n  execute_notebooks: force\n\n# Define the name of the latex output file for PDF builds\nlatex:\n  latex_documents:\n    targetname: book.tex\n\n# Add a bibtex file so that we can create citations\nbibtex_bibfiles:\n  - references.bib\n\n# Information about where the book exists on the web\nrepository:\n  url: https://github.com/ThomasHSimm/Pesticide  # Online location of your book\n  path_to_book: docs  # Optional path to your book, relative to the repository root\n  branch: master  # Which branch of the repository should be used when creating links (optional)\n\n# Add GitHub buttons to your book\n# See https://jupyterbook.org/customize/config.html#add-a-link-to-your-repository\n# HTML-specific settings\nhtml:\n  favicon                   : \"_images/favicon.jpg\"  # A path to a favicon image\n  use_edit_page_button      : false  # Whether to add an \"edit this page\" button to pages. If `true`, repository information in repository: must be filled in\n  use_repository_button     : false  # Whether to add a link to your repository button\n  use_issues_button         : false  # Whether to add an \"open an issue\" button\n  use_multitoc_numbering    : true   # Continuous numbering across parts/chapters\n  extra_navbar              : Powered by <a href=\"https://jupyterbook.org\">Jupyter Book</a>\n                              <br>Home website <a href=\"https://thomashsimm.com/\">thomashsimm.com</a> # Will be displayed underneath the left navbar.\n  extra_footer              : \"\"  # Will be displayed underneath the footer.\n  google_analytics_id       : \"\"  # A GA id that can be used to track book views.\n  home_page_in_navbar       : true  # Whether to include your home page in the left Navigation Bar\n  baseurl                   : \"\"  # The base URL where your book will be hosted. Used for creating image previews and social links. e.g.: https://mypage.com/mybook/\n  comments:\n    hypothesis              : false\n    utterances              : false\n  announcement              : \"\" # A banner announcement at the top of the site."
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#table-of-content",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#table-of-content",
    "title": "Presentation: Communicating code and data",
    "section": "Table of content",
    "text": "Table of content\nAnd in addition to the config file a table of contents file is required _toc.yml:\n# Table of contents\n# Learn more at https://jupyterbook.org/customize/toc.html\n\nformat: jb-book\nroot: intro\nchapters:\n- file: Pesticide_Plots\n- file: References\n- file: UK_areas\n- file: using_jupyter_books"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#creating-a-webpage-from-this",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#creating-a-webpage-from-this",
    "title": "Presentation: Communicating code and data",
    "section": "Creating a webpage from this",
    "text": "Creating a webpage from this\nTakes about 30 mins including installing the chosen converter. (But can be done much quicker)\n\ncreate a Github repo for your website\nchoose the converter (e.g. Jupyter Books)\n\nAnd follow their instructions\n\ngo to settings -> Pages within the repo\n\nfew options to do\n\nOptional: add your own website url to it\n\nLink how to do this here\nIn Quarto a command from your PC in the repo, publishes the website:\nquarto publish quarto-pub\nOr equivalently with Jupyter Books:\nghp-import -n -p -f _build/html"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#creating-directly-from-the-repo",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#creating-directly-from-the-repo",
    "title": "Presentation: Communicating code and data",
    "section": "Creating directly from the repo",
    "text": "Creating directly from the repo\nIf we instead want to convert notebook files directly from a repo to create a website then this can be done with Netlify.\nThis is useful if using Gitlab (i.e. not Github) or don’t want all the extra html files cluttering the repo.\nSteps:\nhttps://jupyterbook.org/en/stable/publish/netlify.html\n\nSign up and connect Github/Gitlab\nAdd a requirements.txt file and also toc.yml to directory\nOn netlify -> Add new site -> import from an existing repo\nInsert something like below\n\nN.B. the command:\npip install -r requirements.txt && jupyter-book build .\nand folder location"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#on-netlify",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#on-netlify",
    "title": "Presentation: Communicating code and data",
    "section": "On netlify",
    "text": "On netlify\n\nExample:\n\nGitlab repo\nResulting website https://thomashsimm.netlify.app/intro.html"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#quarto-presentations",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#quarto-presentations",
    "title": "Presentation: Communicating code and data",
    "section": "Quarto Presentations",
    "text": "Quarto Presentations\nQuarto supports a variety of formats for creating presentations, including:\n\nrevealjs — reveal.js (HTML)\npptx — PowerPoint (MS Office)\nbeamer — Beamer (LaTeX/PDF)\n\nI’ll consider the first two"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#quarto-powerpoint-overview",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#quarto-powerpoint-overview",
    "title": "Presentation: Communicating code and data",
    "section": "Quarto PowerPoint overview",
    "text": "Quarto PowerPoint overview\nThe steps to make a PowerPoint presentation from a notebook:\n\nCreate the inbuilt template.pptx file\nAdjust it to match your own template\nAt the top of the notebook insert format for pptx including the template file\nChoose how you will define a new page\nYou will probably need to manually check the slides and adjust as required\n\nespecially for interactive content and code"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#creating-the-template",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#creating-the-template",
    "title": "Presentation: Communicating code and data",
    "section": "Creating the template",
    "text": "Creating the template\n(Office info correct for Office 365 Feb 2023, Version 2301 Build 16.0.16026.20002)\nIf your workplace has a custom template or you have one you always use, you can incorporate this into quarto.\nHowever, quarto is quite specific on the form this template takes, and requires the following elements\n\nTitle Slide\nTitle and Content\nSection Header\nTwo Content\nComparison\nContent with Caption\nBlank"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#creating-a-ppt-template",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#creating-a-ppt-template",
    "title": "Presentation: Communicating code and data",
    "section": "Creating a ppt template",
    "text": "Creating a ppt template\nBy selecting Layout from the Home tab in powerpoint the different layouts can be seen\n\nThey can then be modified by going to View tab - Slide Master."
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#creating-a-ppt-template-2",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#creating-a-ppt-template-2",
    "title": "Presentation: Communicating code and data",
    "section": "Creating a ppt template 2",
    "text": "Creating a ppt template 2\nIf using your own template you will need to match the names of the slides given above. These can be found by hovering over the slides on the left or right clicking on one and selecting “Rename Layout”"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#creating-a-ppt-template-3",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#creating-a-ppt-template-3",
    "title": "Presentation: Communicating code and data",
    "section": "Creating a ppt template 3",
    "text": "Creating a ppt template 3\nAlternatively, create a custom template using quarto and then modify this. The following command creates the template:\nquarto pandoc -o template.pptx --print-default-data-file reference.pptx\nThen go to View tab - Slide Master and modify each slide layout.\nNote if you are trying to match a template, some tips: - go to Design -> Slide Size and match this to your template - when View tab - Slide Master is selected go to first tab (see above it will be left indented) on one you are copying from and select all on this then paste to the new template - these will be background images and other things that want to be passed to all slides - Check other slides for images and font-styles etc to match to the new template"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#load-the-template",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#load-the-template",
    "title": "Presentation: Communicating code and data",
    "section": "Load the template",
    "text": "Load the template\nTo load the template the first cell in the notebook needs to be modified as follows to reference the template.pptx file.\nformat:\n  pptx:\n    reference-doc: template.pptx\n    slide-level: 2\nIn addition, we can also specify here the rule by which a new slide is defined. If slide-level: 2 is used a new slide is defined by “##’ and a new section header by ‘#’. So if we used ‘###’ this would be a heading within the slide.\nIf slide-level: 1 is used a new slide is defined by “#’ and ‘##’ this would be a heading within the slide (this is normally the default)."
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#check-the-slides",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#check-the-slides",
    "title": "Presentation: Communicating code and data",
    "section": "Check the slides",
    "text": "Check the slides\nI have found creation of slides to powerpoint more prone to strange results than if .doc/.pdf/.html are used.\nSo check the slides, see if interactive content or code has been included (probably not) and if the slide content goes outside the slide.\n\nIn the example above - There is overlap of text on a slide - Strange ouput of a slide - Code output not displayed"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#adding-style-to-revealjs",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#adding-style-to-revealjs",
    "title": "Presentation: Communicating code and data",
    "section": "Adding style to revealjs",
    "text": "Adding style to revealjs\nA simple way to add template like details to a revealjs file is to add a style.css sheet.\nIn the example below, the style sheet adds logo.png to the bottom right of each sheet\nThe file style.css looks like this:\n.reveal .slide-logo {\n  display: block;\n  position: fixed;\n  top: unset !important;\n  left: unset !important;\n  bottom: 50px;\n  right: 12px;\n  height: 100px !important;\n  width: 100x !important;\n  max-width: unset !important;\n  max-height: unset !important;\n}"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#adding-style-to-revealjs-1",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#adding-style-to-revealjs-1",
    "title": "Presentation: Communicating code and data",
    "section": "Adding style to revealjs",
    "text": "Adding style to revealjs\nAnd the revealjs part at the top of the jupyter notebook looks like this\nrevealjs:\n    slide-number: true\n    height: 1080\n    width: 1920\n    logo: logo.png\n    css: style.css\nSo this would then look like the following, with the logo (logo.png) in the bottom right, and size and positioning given by the css file"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#what-the-revealjs-file-looks-like",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#what-the-revealjs-file-looks-like",
    "title": "Presentation: Communicating code and data",
    "section": "What the revealjs file looks like",
    "text": "What the revealjs file looks like\nVideo"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#excel-files",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#excel-files",
    "title": "Presentation: Communicating code and data",
    "section": "Excel files",
    "text": "Excel files\n\nChatGPT: What are the problems of Excel and xlsx files?\n\n\nLimited scalability: Excel has a limit on the number of rows and columns it can handle effectively.\nInaccurate data: The manual nature of data entry in Excel can lead to errors and inaccuracies.\nFragility: Complex Excel spreadsheets can break easily if any formula or cell reference is altered.\nLack of security: Excel files are vulnerable to unauthorized access and malicious attacks.\nSlow performance: Large and complex Excel files can become slow to open and use, leading to decreased productivity.\nCompatibility issues: XLSX files may not be compatible with older versions of Excel, or with other software applications.\nLimited collaboration: Sharing Excel files and making changes to them can be difficult, especially when multiple people are involved.\n\nFor me it is the Slow performance alongside: (a) we are doing the data manipulation outside Excel anyway and (b) having to have another application open"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#what-aspect-of-tables-i-am-considering",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#what-aspect-of-tables-i-am-considering",
    "title": "Presentation: Communicating code and data",
    "section": "What aspect of tables I am considering",
    "text": "What aspect of tables I am considering"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#loading-data",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#loading-data",
    "title": "Presentation: Communicating code and data",
    "section": "Loading data",
    "text": "Loading data\n.ods with Excel\nAbout 10 s to open 3 ods files with Excel\n.ods converted to html in firefox browser\nAbout 6 s to open 3 converted ods files in a browser\n.ods converted to html firefox browser no new tabs\nAlmost instant when open converted ods files in same browser"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#convert-xlsx-to-html",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#convert-xlsx-to-html",
    "title": "Presentation: Communicating code and data",
    "section": "Convert xlsx to html?",
    "text": "Convert xlsx to html?\n\nOpening xlsx files in Excel is slow\nConverting to html if we don’t want to edit could be an option\nIf we are moving to Python/R aren’t non-Excel options worth considering??\n\nConverting xlsx files to html\n\nSeems the most obvious course\nBut it doesn’t seem that easy with code\n\nbut easy within Excel"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#pandas",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#pandas",
    "title": "Presentation: Communicating code and data",
    "section": "pandas",
    "text": "pandas\n\nSince (in Python) we are mainly working with pandas. Let’s consider how pandas outputs can be modified.\npandas options\n\nSome code functionality\n# precision of all columns\npd.set_option(\"display.precision\", 2)\n# Or map as a string\ndf2['amount_pc_str'] = df2['amount_pc'].map(lambda x: '%.3f' % x)\n# some other options\npd.set_option('max_colwidth', 20)\npd.set_option('display.max_columns', None)\npd.set_option('display.expand_frame_repr', False)\npd.set_option('max_colwidth', 0)"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#pandas-basic",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#pandas-basic",
    "title": "Presentation: Communicating code and data",
    "section": "pandas basic",
    "text": "pandas basic\n\n\n\n\n\n\n  \n    \n      \n      sample_id\n      date_of_sampling\n      description\n      country_of_origin\n      retail_outlet\n      address\n      brand_name\n      packer_/_manufacturer_/_importer\n      product\n      address_postcode\n      packer_postcode\n      address_area\n      packer_area\n      chem_name\n      amount_detected\n      mrl\n      amount_pc\n    \n  \n  \n    \n      0\n      1958/2016\n      2016-08-08\n      Bramley Apples\n      UK\n      Asda\n      Creechbarrow Road, Taunton TA1 2AN\n      Asda\n      Asda Stores Ltd Leeds, UK LS11 5AD\n      Apple\n      TA1 2AN\n      LS11 5AD\n      Somerset\n      West Yorkshire\n      boscalid\n      0.03\n      2.0\n      0.015\n    \n    \n      1\n      1958/2016\n      2016-08-08\n      Bramley Apples\n      UK\n      Asda\n      Creechbarrow Road, Taunton TA1 2AN\n      Asda\n      Asda Stores Ltd Leeds, UK LS11 5AD\n      Apple\n      TA1 2AN\n      LS11 5AD\n      Somerset\n      West Yorkshire\n      pyraclostrobin\n      0.01\n      0.5\n      0.020\n    \n    \n      2\n      0230/2016\n      2016-08-08\n      Bramley Apples\n      UK\n      Co-op\n      Northgate, Louth LN11 0LT\n      Co-op\n      Co-operative Group Ltd Manchester M60 0AG\n      Apple\n      LN11 0LT\n      M60 0AG\n      Lincolnshire\n      Greater Manchester\n      boscalid\n      0.05\n      2.0\n      0.025\n    \n    \n      3\n      0230/2016\n      2016-08-08\n      Bramley Apples\n      UK\n      Co-op\n      Northgate, Louth LN11 0LT\n      Co-op\n      Co-operative Group Ltd Manchester M60 0AG\n      Apple\n      LN11 0LT\n      M60 0AG\n      Lincolnshire\n      Greater Manchester\n      flonicamid (sum)\n      0.02\n      0.2\n      0.100\n    \n    \n      4\n      0230/2016\n      2016-08-08\n      Bramley Apples\n      UK\n      Co-op\n      Northgate, Louth LN11 0LT\n      Co-op\n      Co-operative Group Ltd Manchester M60 0AG\n      Apple\n      LN11 0LT\n      M60 0AG\n      Lincolnshire\n      Greater Manchester\n      pyraclostrobin\n      0.03\n      0.5\n      0.060\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      35155\n      2858/2020 Organic\n      2020-10-20\n      Organic Sweet Potatoes\n      Spain\n      Tesco\n      300 Beverley Way, New Malden KT3 4PJ\n      Tesco\n      Tesco Stores Ltd Welwyn Garden City AL7 1GA\n      Sweet_Potatoes_Q4_(BNA)\n      KT3 4PJ\n      AL7 1GA\n      Greater London\n      Hertfordshire\n      0\n      0.00\n      0.0\n      0.000\n    \n    \n      35156\n      0562/2020 Organic\n      2020-10-05\n      Organic Duchy Sweet Potatoes\n      Egypt\n      Waitrose\n      Mill Lane, Swindon SN1 7BX\n      Waitrose\n      Waitrose Ltd Doncastle Road, Bracknell, Berksh...\n      Sweet_Potatoes_Q4_(BNA)\n      SN1 7BX\n      RG12 8YA\n      Wiltshire\n      Berkshire\n      0\n      0.00\n      0.0\n      0.000\n    \n    \n      35157\n      0563/2020\n      2020-10-05\n      Sweet Potatoes\n      USA\n      Waitrose\n      Mill Lane, Swindon SN1 7BX\n      Waitrose\n      Waitrose Ltd Doncastle Road, Bracknell, Berksh...\n      Sweet_Potatoes_Q4_(BNA)\n      SN1 7BX\n      RG12 8YA\n      Wiltshire\n      Berkshire\n      0\n      0.00\n      0.0\n      0.000\n    \n    \n      35158\n      2601/2020\n      2020-10-14\n      Sweet Potatoes\n      USA\n      Waitrose\n      Ossington Way, Newark NG24 1FF\n      Waitrose\n      Waitrose Ltd Doncastle Road, Bracknell, Berksh...\n      Sweet_Potatoes_Q4_(BNA)\n      NG24 1FF\n      RG12 8YA\n      Nottinghamshire\n      Berkshire\n      0\n      0.00\n      0.0\n      0.000\n    \n    \n      35159\n      2601/2020\n      2020-10-14\n      Sweet Potatoes\n      USA\n      Waitrose\n      Ossington Way, Newark NG24 1FF\n      Waitrose\n      Waitrose Ltd Doncastle Road, Bracknell, Berksh...\n      Sweet_Potatoes_Q4_(BNA)\n      NG24 1FF\n      RG12 8YA\n      Nottinghamshire\n      Berkshire\n      0\n      0.00\n      0.0\n      0.000\n    \n  \n\n35160 rows × 17 columns"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#pandas-overview",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#pandas-overview",
    "title": "Presentation: Communicating code and data",
    "section": "pandas overview",
    "text": "pandas overview\n\nUsing pandas we can control various outputs\nBut these still need a format to display within\nAnd display functionality is not easy\n\nOr convert to a html file\ndf2.iloc[:500].to_html('df2_500.html')\nBut using a style sheet as shown in stack overflow by Parfait"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#ipydatagrid",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#ipydatagrid",
    "title": "Presentation: Communicating code and data",
    "section": "ipydatagrid",
    "text": "ipydatagrid\nhttps://github.com/bloomberg/ipydatagrid"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#itables-code",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#itables-code",
    "title": "Presentation: Communicating code and data",
    "section": "itables code",
    "text": "itables code\nfrom itables import init_notebook_mode\n\nimport itables\ninit_notebook_mode(all_interactive=True)\n\nitables.show(df2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n      \n      sample_id\n      date_of_sampling\n      description\n      country_of_origin\n      retail_outlet\n      address\n      brand_name\n      packer_/_manufacturer_/_importer\n      product\n      address_postcode\n      packer_postcode\n      address_area\n      packer_area\n      chem_name\n      amount_detected\n      mrl\n      amount_pc\n    \n  Loading... (need help?)"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#dash",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#dash",
    "title": "Presentation: Communicating code and data",
    "section": "Dash",
    "text": "Dash\nhttps://dash.plotly.com/datatable\n\nDownloaded 800,000 times per month, Dash is the original low-code framework for rapidly building data apps in Python, R, Julia, and F# (experimental).\n\nhttps://medium.com/plotly/introducing-jupyterdash-811f1f57c02e\n\n\nOSError: Address 'http://127.0.0.1:8050' already in use.\n    Try passing a different port to run_server."
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#streamlit",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#streamlit",
    "title": "Presentation: Communicating code and data",
    "section": "Streamlit",
    "text": "Streamlit\n\nA faster way to build and share data apps\n\n\nDash can be run within a notebook but is principally an app.\nStreamlit is a similar app.\nBut much easier to code.\n\nimport pandas as pd\nimport streamlit as st\nall_dfs = pd.read_csv(\"./data/combined_df.csv\")\nst.dataframe(all_dfs.head())\n# Or\nst.tables(all_dfs)"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#and-more",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#and-more",
    "title": "Presentation: Communicating code and data",
    "section": "And more",
    "text": "And more\nDataTables\n\nDataTables is a plug-in for the jQuery Javascript library. It is a highly flexible tool, built upon the foundations of progressive enhancement, that adds all of these advanced features to any HTML table.\n\nJupyter widgets\nIf you are looking for Jupyter widgets, have a look at (taken from https://mwouts.github.io/itables/references.html) - QGrid by Quantopian - IPyaggrid by Louis Raison and Olivier Borderies - IPySheet by QuantStack."
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#more-details-on-itables",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#more-details-on-itables",
    "title": "Presentation: Communicating code and data",
    "section": "More details on itables",
    "text": "More details on itables\nFrom my brief review I found itables the best package\n\nIt works\nIt gives lots of control of table output to be consistent with good-practice\n\nColumn width\nNumber formatting\nColumn alignment\n\nAlongside\n\nSearch\nColumn ordering\nHow many rows are shown\nScrolling options\netc"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#itables-code1",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#itables-code1",
    "title": "Presentation: Communicating code and data",
    "section": "itables code1",
    "text": "itables code1\ndef _indi_columnDefs(cols, format,col_width):\n    if 'num_format' in format:\n        if format['num_format'] == \"#,##0\":\n            format_str = \"',', '.', 0, ''\"\n        elif format['num_format'] ==  '0.0':\n            format_str = \"',', '.', 1, ''\"\n        else: \n            format_str = \"',', '.', 3, ''\"\n    else:\n        format_str = \"',', '.', 3, ''\"\n    columnDefs= {\n            \"targets\": cols,\n            \"className\":f\"dt-{format['align']}\",\n            \"render\": JavascriptCode(f\"$.fn.dataTable.render.number({format_str})\"),\n            \"width\": f\"{col_width}px\",\n        }\n    return columnDefs"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#itables-code2",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#itables-code2",
    "title": "Presentation: Communicating code and data",
    "section": "itables code2",
    "text": "itables code2\nstyle = {\n    \"general\": {\n        \"table_output\" :{\n            \"pct_cols\" : {\n                \"regex_defined\": [\"%\" , \"[Pp]ercentage\"],\n                \"format\": {\"num_format\": \"0.0\", \"align\": \"right\"}\n            },\n            \"total_cols\" : {\n                    \"regex_defined\": [\"[Tt]otal\" ],\n                    \"format\": { \"num_format\": \"0.0\", \"align\": \"center\"}\n            },\n            \"code_cols\" : {\n                \"regex_defined\": [\"[Cc]ode\"],\n                \"format\": { \"align\": \"left\"}\n            },\n            \"_else\" : {\n                \"regex_defined\": [],\n                \"format\": {\"num_format\": \"#,##0\", \"align\": \"right\"}\n            }\n        \n        },\n        \"col_width\": 130.0\n    }\n}"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#itables-code3",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#itables-code3",
    "title": "Presentation: Communicating code and data",
    "section": "itables code3",
    "text": "itables code3\n\ndef _create_style_col(style: dict, df: pd.DataFrame):\n    style_col = {}\n    colsAll=[]\n    for keys in style['general']['table_output'].keys():\n        if keys !='_else':\n            # print(\">>\",keys)\n            cols=[]\n            for i,column in enumerate(df.columns):\n                # print(column)\n                if [True for reg in style['general']['table_output'][keys]['regex_defined'] if re.search(reg,column)]:\n                    # print(i,column)\n                    cols.append(i)\n            # print(cols)\n            colsAll = colsAll + cols\n            style_col[keys]=cols\n\n    style_col['_else'] = [ i for i,column in enumerate(df.columns) if i not in colsAll]\n\n    return style_col"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#itables-code4",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#itables-code4",
    "title": "Presentation: Communicating code and data",
    "section": "itables code4",
    "text": "itables code4\ndef create_columnDefs(style: dict, df: pd.DataFrame):\n    \n    style_col = _create_style_col(style, df)\n    columnDefs = []\n    for keys in style['general']['table_output'].keys():\n        columnDefs.append( _indi_columnDefs(style_col[keys],\n                                            style['general']['table_output'][keys]['format'],\n                                            style['general']['col_width']) )\n    return columnDefs"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#itables-code5",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#itables-code5",
    "title": "Presentation: Communicating code and data",
    "section": "itables code5",
    "text": "itables code5\n\ncolumnDefs= create_columnDefs(style, data)\nshow(\n    data,\n    # style=\"table-layout:auto;width:80%;float:left\",\n    classes=\"display\",\n    \n    # specify how many rows\n    lengthMenu = [25,100,-1],\n    # or to scroll through data\n    scrollX=True,\n    # scrollY=\"800px\", \n    scrollCollapse=True, \n    # paging=False,\n\n    style=f\"width:{style['general']['col_width']*10}px\",\n    autoWidth=False,\n\n    # add footer\n    # footer=True,\n    \n    columnDefs=columnDefs,\n    tags =f'<caption style=\"caption-side: Bottom\">File: {files[0]} and Tab: {ansa.value}</caption>'\n)"
  },
  {
    "objectID": "posts/GolfSwing/2021-12-01-GolfPos1FastAI.html#introduction",
    "href": "posts/GolfSwing/2021-12-01-GolfPos1FastAI.html#introduction",
    "title": "Body position recognition using FastAI",
    "section": "Introduction",
    "text": "Introduction\nThis is a first step in a project to analyse golf swings.\nIn this first step I try to identify different parts of the body and golf equipment during the golf swing. This step is of limited success for overall analysis but the steps used are useful for the lessons learnt.\nThis work uses deep learning to identify locations (vectors) on images and fitting by regression.\nIn this step I will use a dataSet found at  Git Hub GolfSwing and the paper of the work https://arxiv.org/abs/1903.06528.\nWhat this dataset/paper does is split the golf swing into a number of sequences based on the position of the body and golf club, e.g. start, golf club parallel to ground, striking the ball etc. We will call these the golf positions. These positions are shown below.\n\n\n\nvia GIPHY\n\n\nThe dataset includes a series of videos that have been characterised based on the different swing sequences.\n\nSteps in this page\n\nDownload the video dataset and the details of the frames of the different positions\nCreate images at the different positions from the videos\nClassify points on the images and a file for each image of these\nUpload data to GitHub and download on notebook for analysis\nUse deep learning to identify the positions on the images"
  },
  {
    "objectID": "posts/GolfSwing/2021-12-01-GolfPos1FastAI.html#use-the-video-analysis-dataset-to-create-images-of-golf-swings",
    "href": "posts/GolfSwing/2021-12-01-GolfPos1FastAI.html#use-the-video-analysis-dataset-to-create-images-of-golf-swings",
    "title": "Body position recognition using FastAI",
    "section": "Use the video analysis dataset to create images of golf swings",
    "text": "Use the video analysis dataset to create images of golf swings\nFirst I cloned the directory (https://github.com/wmcnally/golfdb) onto my local PC. I then need to identify which videos to use- I want the ones behind the golfer and preferably of lower frame rate.\nBelow are the names of the videos I selected\n\nimport numpy as np\nimport os\n\nuseVids=[1,3,5,7,13,24,43,46,48,71,77,81,83,89,93,242,681,1060]\nnp.shape(useVids)\n\n(18,)\n\n\nI now want to find the frames in each video that represent the selected positions.\nThese exist in a ‘.pkl’ file. So we open the file and then select the videos (rows) we want to use.\n\nimport pandas as pd\nimport pickle\n\nfile_path= cda + \"\\data\\\\golfDB.pkl\"\n\n\ndata = pickle.load(open(file_path,\"rb\"))\naa=[]\ni=0\nfor ii in useVids:\n    if i==0:\n        aa=data[ii==data.id]\n        \n    else:\n        aa=aa.append(data[ii==data.id])\n       \n    i=i+1\naa.reset_index(drop=True,inplace=True)\naa.tail()\n\n\nIn the DataFrame (aa) the details we want are just the ‘events’ so we know what frames to save as images from the videos\nFirst we create a function that takes a video location and details of the frames (or the selected golf positions) and then creates a new folder containing images of those frames.\nThis uses the library cv2 and a secondary check to normalise the positions if it is different from that given (this was useful in earlier versions but later ones the frame number matched that given by the aa dataFrame).\nThe function works by finding a frame rate then stepping through the video by adding the time per frame after each step. If the frame is at a position given by the input (from aa) it is saved as an image.\n\ndef createImages(fila,pos):\n    ''' \n    Given a video file location (fila) it will save as images to a folder\n    Given positions in video (pos) these images from the video are saved\n    pos is created based on positions of swings\n    '''\n    import cv2\n    import numpy as np\n    import os\n    \n    # create a video capture object\n    cap = cv2.VideoCapture(fila)\n    \n    # get details of the video clip\n    duration = cap.get(cv2.CAP_PROP_POS_MSEC)\n    \n    frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    duration_seconds = frame_count / fps\n    print('duration is ',duration,'. frame_count is ',frame_count,'. fps is ',fps,'. duration sec is',duration_seconds)\n    \n    #alter pos based on frame count\n    posb4=pos\n    pos=(pos/(np.max(pos)/frame_count))\n    pos=np.array([int(nn) for nn in pos])\n    pos=pos[1:-2]#ignore first value and last two\n    \n    \n    # create a folder if it doesn't exist\n    folder = fila.split('\\\\')[-1].split('.')[0]\n    folder = '_images'+folder\n    print(folder)\n    try:\n        os.mkdir(folder)\n    except:\n        pass\n\n    \n    vidcap = cap\n    \n    # this function creates an image from part of a video and \n    # saves as a JPG file\n    def getFrame(sec,go):\n        vidcap.set(cv2.CAP_PROP_POS_MSEC,sec)\n        hasFrames,image = vidcap.read()\n        if hasFrames and go:\n            cv2.imwrite(os.path.join(folder,\"frame{:d}.jpg\".format(count)), image)     # save frame as JPG file\n        return hasFrames\n    \n    # goes through the video clip and steps through based on frame rate\n    sec = 0\n    frameRate = 1000/fps \n    count=1\n    go=0\n    success = True\n    while success:\n        count = count + 1\n        sec = sec + frameRate\n        #only saves images if at positions in pos\n        if count in pos:\n            go=1\n        else:\n            go=0\n        success = getFrame(sec,go)\n\n    print(\"{} images are extacted in {}.\".format(count,folder))\n\nAnd below I call the script for the videos I selected\n\nimport cv2\nfila = cda + '\\\\data\\\\videos_160\\\\'\nfor ii,aai in enumerate(aa.id):\n    fold = fila + str(aai)+'.mp4'\n    pos=aa.iloc[ii,7]\n    pos=pos-pos[0]\n    if ii>1:\n        cII(fold,pos)\n        cap = createImages.VideoCapture(fold)\n\nSo now we have a series of folders for each video with images given by the selected positions"
  },
  {
    "objectID": "posts/GolfSwing/2021-12-01-GolfPos1FastAI.html#manually-classify-points-on-the-images",
    "href": "posts/GolfSwing/2021-12-01-GolfPos1FastAI.html#manually-classify-points-on-the-images",
    "title": "Body position recognition using FastAI",
    "section": "Manually classify points on the images",
    "text": "Manually classify points on the images\nTo be able to perform analysis on the images they first need to be labelled.\nTo do this I decided to take the manual approach and classify the images myself. I decided to choose the following regions in each image: - The ball - The end of the golf club (clubhead) - The back wrist - the back elbow - the top of the head\nThis is done using the follwing function\n\ndef imDo(im):\n    \n    fig=plt.figure(figsize=(20, 15))\n    plt.imshow(im)\n\n    def tellme(s):\n        print(s)\n        plt.title(s, fontsize=16)\n\n    tellme('You will define golf swing, click to begin')\n\n    plt.waitforbuttonpress()\n\n    while True:\n        pts = []\n        while len(pts) < 5:\n            tellme('Select golf ball-golf club- wrist- elbow- head with mouse')\n            pts = np.asarray(plt.ginput(5, timeout=-1))\n            if len(pts) < 5:\n                tellme('Too few points, starting over')\n                time.sleep(1)  # Wait a second\n        \n        ph = plt.plot(pts[:, 0], pts[:, 1], marker='x',markersize=20,markeredgewidth=3)\n\n        tellme('Happy? Key click for yes, mouse click for no')\n\n        if plt.waitforbuttonpress():\n            break\n    plt.close(fig)\n    return pts\n\nBefore we can call this function we want to make sure the image appears as a new window\nAlso some imports\n\nimport fastbook\n\nfrom fastbook import *\nfrom fastai.vision.all import *\nimport matplotlib\n\ncda = os.getcwd()\n\nmatplotlib.use('TKAgg')\n\nNow for each image file created, the script below runs imDo which plots the image then asks the user to select 5 points on the image for classification.\nthese points are then save as txt file with the same name as the image file to be used later in modeling\n\n\nfoldOuta=cda+'//_dataSel//'\nlsa = os.listdir(foldOuta)\nlsa\nptsALL=[]\nfor ii,folds in enumerate(lsa):\n    if ii>0:\n        print(folds)\n        img_files = get_image_files(foldOuta+folds)\n        for fils in img_files:\n            im = PILImage.create(fils)\n            pts=imDo(im)\n            ptsALL.append(pts)\n            fnom=str(fils).split('\\\\')[-1].split('.')[0]\n            \n            np.savetxt(foldOuta+folds+'\\\\'+fnom+'.txt',pts)"
  },
  {
    "objectID": "posts/GolfSwing/2021-12-01-GolfPos1FastAI.html#upload-data-for-use-in-modeling",
    "href": "posts/GolfSwing/2021-12-01-GolfPos1FastAI.html#upload-data-for-use-in-modeling",
    "title": "Body position recognition using FastAI",
    "section": "Upload data for use in modeling",
    "text": "Upload data for use in modeling\nFastai has a function called untar_data that prepares images in a .tgz folder ready to use for analysis.\nA tgz file can be made by a Python script, but all the ones I tried produced an error, so instead I used\nTo create a tar file see https://opensource.com/article/17/7/how-unzip-targz-file\nOpen up a terminal go to the folder that contains the folder wanting to compress and then tar with the command line\ntar –create –verbose –file GC.tgz GolfComb\nI have then uploaded it to GitHub. Go to the file on Github open it and right click on ‘view raw’ and select copy link."
  },
  {
    "objectID": "posts/GolfSwing/2021-12-01-GolfPos1FastAI.html#model-the-data",
    "href": "posts/GolfSwing/2021-12-01-GolfPos1FastAI.html#model-the-data",
    "title": "Body position recognition using FastAI",
    "section": "Model the data",
    "text": "Model the data\nThe rest needs to be done with a GPU. I have done this with https://colab.research.google.com/ (free time is limited but details not published) and the code tab for a notebook on https://www.kaggle.com/ (36 h per month for free)\nFirst import the fastai stuff\n\n!pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\nfrom fastbook import *\n\n\nfrom fastai.vision.all import *\n\nimport os\nimport re\nimport numpy\n\nuntar the data and set the path\n\nurl='https://github.com/ThomasHSimm/GolfSwingTSimm/blob/main/_dataSel/GC.tgz?raw=true'\n\npath = untar_data(url)\n\n\nPath.BASE_PATH = path\n\nHave a look at the data\n\n(path/'Test').ls()\n\n\nA function to classify the points on the image\nLoads the text file for each image and returns a TensorPoint object of points on the image\n\ndef get_pointa_img(fileo):\n     \n    txtfile = str(fileo)[0:-4] + '.txt'\n    # print(txtfile)\n    pts=np.loadtxt(txtfile)\n    pp=pts[-1,:]\n    # print(pp)\n    return TensorPoint(pp)\n\nCreate a DataBlock\nThe DataBlock is the foundation of the model. It needs to know - the location of the images, - the label for the images (points on images in this case) - separation of data into test and validation sets (done automatically if not specified) - the type of data used blocks=(ImageBlock, PointBlock) - any resizing of images - any transforms (Data Augmentation)\n\nitem_tfms = [Resize(448, method='squish')]\nbiwi = DataBlock(\n    blocks=(ImageBlock, PointBlock),\n    get_items=get_image_files,\n    item_tfms=item_tfms,\n    get_y=get_pointa_img,\n    batch_tfms=[*aug_transforms(size=224, min_scale=0.75),\n                               Normalize.from_stats(*imagenet_stats)])\n\nNow create a DataLoaders object which has the path of the data and the batch size (here 30)\nBatch size is important to specify to avoid memory issues\n\ndls = biwi.dataloaders(path,30)\n\nNow create the learner\nPass it the dataLoaders, we’re doing transfer learning from resnet50 (imageNet trained model), what metrics we’ll use for loss, and the range in y values we want\n\nlearn = cnn_learner(dls, resnet50, metrics=partial(accuracy_multi, thresh=0.02),y_range=(-1,1))\n\nHave a look at the data. Can see the transforms\n\ndls.show_batch(max_n=8, figsize=(12,12))\n\n\n\n\nFind the best learning rate\n\n\nlearn.lr_find()\n\n\nTo fit the model we have a few options:\n\nlearn.fit(10,lr=4e-3)\nlearn.fit_one_cycle()\nlearn.fine_tune(10, base_lr=1e-3, freeze_epochs=7)\nlearn.fine_tune(15, lr)\n\nFastAI adds an extra 2 layers on the end of neural network these can then be fitted using fine_tune. It is recommended to do a few fits frozen before unfreezing. This is normally the best option for transfer learning.\nBut the other ones can be used. In general fit can be more unstable and lead to bigger losses, but can be useful if fine_tune is not bringing losses down.\nhttps://forums.fast.ai/t/fine-tune-vs-fit-one-cycle/66029/6\nfit_one_cycle = New Model\nfine_tuning = with Transfer Learning?\n\nI’d say yes but with a very strong but, only because it’s easy to fall into a trap that way. fine_tuning is geared towards transfer learning specifically, but you can also just do fit_one_cycle as well! (Or flat_cos).\n\nFor beginners it’s a great starting fit function (and advanced too), but also don’t forget that you can then build on what that function is doing. For instance, I wonder how modifying/adapting that function for Ranger/flat_cos would need to change!\n\nlearn.fine_tune(10, base_lr=1e-3, freeze_epochs=7)\n\n\n\nlearn.lr_find()\n\n\nSome more fitting, reducing the learning rate after steps\n\nlearn.fit(20,lr=1e-4)\n\n\nSome more fitting\nMixing fit with fine_tune and reducing learning rate seems to work best for reducing loss\nLoss here is:\ntrain_loss  valid_loss\n\n0.054042    0.008305"
  },
  {
    "objectID": "posts/GolfSwing/2021-12-01-GolfPos1FastAI.html#results",
    "href": "posts/GolfSwing/2021-12-01-GolfPos1FastAI.html#results",
    "title": "Body position recognition using FastAI",
    "section": "Results",
    "text": "Results\nLook at the results, pretty good for ~10 mins of 81 images of learning although doesn’t always get the top of the head.\n\nlearn.show_results()\n\n\n\n#save the model\nlearn.export(fname='headTry1.pkl')\n\nHowever, when this is generalised to other points, such as hands and clubhead, that are less static the results are poor.\nPresumably a combination of the low resolution of the images making it difficult to identify features and the lack of images.\n\nIncreasing the res of the images/videos improves the classification considerably.\nBut still not quite there, probably needs more labelling\ntrain_loss     valid_loss\n0.030079       0.031188"
  },
  {
    "objectID": "posts/GolfSwing/2022-02-26-GolfSwingPart2.html#overview",
    "href": "posts/GolfSwing/2022-02-26-GolfSwingPart2.html#overview",
    "title": "Golf Swing Part II- Separating Swing Positions",
    "section": "Overview",
    "text": "Overview\nIn a previous part Part 1 a neural network model was used to find positions on the body during a golf swing. This work used images taken from videos of golf swing (analysed using the code below by the authors listed) because it is often easier to work with images rather than videos.\nBut to get images of the golf swing to analyse it can be useful to get them at different parts of the golf swing. This is what this part does.\nTaken from https://github.com/wmcnally/golfdb and shown in the paper here https://arxiv.org/abs/1903.06528\n[Ref Paper] McNally, William, et al. \"Golfdb: A video database for golf swing sequencing.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. 2019\nThe code separates the golf swing into a number of different segments based on body and golf club positions.\nThis code can be run on kaggle here https://www.kaggle.com/thomassimm/golfdb-lessimports\nThe input is an mp3 file of a golf swing\nThe ouput is a series of images at different parts of the golf swing"
  },
  {
    "objectID": "posts/GolfSwing/2022-02-26-GolfSwingPart2.html#the-code",
    "href": "posts/GolfSwing/2022-02-26-GolfSwingPart2.html#the-code",
    "title": "Golf Swing Part II- Separating Swing Positions",
    "section": "The Code",
    "text": "The Code\n\nSpecify the file to use\nAdd downloaded directory (not always necsessary) and specify the video file.\n\n!cp -r ../input/golfdb3/* ./\n\nstra='../input/golfdb3/test_video.mp4'\nstra='../input/golfdb2/golfdb/data/videos_160/1017.mp4'\nstra\n\n\n\nImports, classes and defs\nSome imports. Neural nets using Torch\n\nimport scipy.io\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n# from eval import ToTensor, Normalize\n# from model import EventDetector\nimport numpy as np\nimport torch.nn.functional as F\nimport cv2\nfrom torch.autograd import Variable\n\nThe following classes and definitions are taken from the files in the GitHub directory\n\nclass SampleVideo(Dataset):\n    def __init__(self, path, input_size=160, transform=None):\n        self.path = path\n        self.input_size = input_size\n        self.transform = transform\n\n    def __len__(self):\n        return 1\n\n    def __getitem__(self, idx):\n        cap = cv2.VideoCapture(self.path)\n        frame_size = [cap.get(cv2.CAP_PROP_FRAME_HEIGHT), cap.get(cv2.CAP_PROP_FRAME_WIDTH)]\n        ratio = self.input_size / max(frame_size)\n        new_size = tuple([int(x * ratio) for x in frame_size])\n        delta_w = self.input_size - new_size[1]\n        delta_h = self.input_size - new_size[0]\n        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n        left, right = delta_w // 2, delta_w - (delta_w // 2)\n\n        # preprocess and return frames\n        images = []\n        for pos in range(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))):\n            _, img = cap.read()\n            resized = cv2.resize(img, (new_size[1], new_size[0]))\n            b_img = cv2.copyMakeBorder(resized, top, bottom, left, right, cv2.BORDER_CONSTANT,\n                                       value=[0.406 * 255, 0.456 * 255, 0.485 * 255])  # ImageNet means (BGR)\n\n            b_img_rgb = cv2.cvtColor(b_img, cv2.COLOR_BGR2RGB)\n            images.append(b_img_rgb)\n        cap.release()\n        labels = np.zeros(len(images)) # only for compatibility with transforms\n        sample = {'images': np.asarray(images), 'labels': np.asarray(labels)}\n        if self.transform:\n            sample = self.transform(sample)\n        return sample\n\n\nclass ToTensor(object):\n    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n    def __call__(self, sample):\n        images, labels = sample['images'], sample['labels']\n        images = images.transpose((0, 3, 1, 2))\n        return {'images': torch.from_numpy(images).float().div(255.),\n                'labels': torch.from_numpy(labels).long()}\n\n\nclass Normalize(object):\n    def __init__(self, mean, std):\n        self.mean = torch.tensor(mean, dtype=torch.float32)\n        self.std = torch.tensor(std, dtype=torch.float32)\n\n    def __call__(self, sample):\n        images, labels = sample['images'], sample['labels']\n        images.sub_(self.mean[None, :, None, None]).div_(self.std[None, :, None, None])\n        return {'images': images, 'labels': labels}\n\n\nimport torch.nn as nn\nimport math\n\n\"\"\"\nhttps://github.com/tonylins/pytorch-mobilenet-v2\n\"\"\"\n\ndef conv_bn(inp, oup, stride):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\ndef conv_1x1_bn(inp, oup):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio):\n        super(InvertedResidual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n\n        hidden_dim = round(inp * expand_ratio)\n        self.use_res_connect = self.stride == 1 and inp == oup\n\n        if expand_ratio == 1:\n            self.conv = nn.Sequential(\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n        else:\n            self.conv = nn.Sequential(\n                # pw\n                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\n\nclass MobileNetV2(nn.Module):\n    def __init__(self, n_class=1000, input_size=224, width_mult=1.):\n        super(MobileNetV2, self).__init__()\n        block = InvertedResidual\n        min_depth = 16\n        input_channel = 32\n        last_channel = 1280\n        interverted_residual_setting = [\n            # t, c, n, s\n            [1, 16, 1, 1],\n            [6, 24, 2, 2],\n            [6, 32, 3, 2],\n            [6, 64, 4, 2],\n            [6, 96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n\n        # building first layer\n        assert input_size % 32 == 0\n        input_channel = int(input_channel * width_mult) if width_mult >= 1.0 else input_channel\n        self.last_channel = int(last_channel * width_mult) if width_mult > 1.0 else last_channel\n        self.features = [conv_bn(3, input_channel, 2)]\n        # building inverted residual blocks\n        for t, c, n, s in interverted_residual_setting:\n            output_channel = max(int(c * width_mult), min_depth)\n            for i in range(n):\n                if i == 0:\n                    self.features.append(block(input_channel, output_channel, s, expand_ratio=t))\n                else:\n                    self.features.append(block(input_channel, output_channel, 1, expand_ratio=t))\n                input_channel = output_channel\n        # building last several layers\n        self.features.append(conv_1x1_bn(input_channel, self.last_channel))\n        # make it nn.Sequential\n        self.features = nn.Sequential(*self.features)\n\n        # building classifier\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(self.last_channel, n_class),\n        )\n\n        self._initialize_weights()\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.mean(3).mean(2)\n        x = self.classifier(x)\n        return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                n = m.weight.size(1)\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n\n\nimport torch.nn as nn\nclass EventDetector(nn.Module):\n    def __init__(self, pretrain, width_mult, lstm_layers, lstm_hidden, bidirectional=True, dropout=True):\n        super(EventDetector, self).__init__()\n        self.width_mult = width_mult\n        self.lstm_layers = lstm_layers\n        self.lstm_hidden = lstm_hidden\n        self.bidirectional = bidirectional\n        self.dropout = dropout\n\n        net = MobileNetV2(width_mult=width_mult)\n        state_dict_mobilenet = torch.load('mobilenet_v2.pth.tar')\n        if pretrain:\n            net.load_state_dict(state_dict_mobilenet)\n\n        self.cnn = nn.Sequential(*list(net.children())[0][:19])\n        self.rnn = nn.LSTM(int(1280*width_mult if width_mult > 1.0 else 1280),\n                           self.lstm_hidden, self.lstm_layers,\n                           batch_first=True, bidirectional=bidirectional)\n        if self.bidirectional:\n            self.lin = nn.Linear(2*self.lstm_hidden, 9)\n        else:\n            self.lin = nn.Linear(self.lstm_hidden, 9)\n        if self.dropout:\n            self.drop = nn.Dropout(0.5)\n\n    def init_hidden(self, batch_size):\n        if self.bidirectional:\n            return (Variable(torch.zeros(2*self.lstm_layers, batch_size, self.lstm_hidden).cuda(), requires_grad=True),\n                    Variable(torch.zeros(2*self.lstm_layers, batch_size, self.lstm_hidden).cuda(), requires_grad=True))\n        else:\n            return (Variable(torch.zeros(self.lstm_layers, batch_size, self.lstm_hidden).cuda(), requires_grad=True),\n                    Variable(torch.zeros(self.lstm_layers, batch_size, self.lstm_hidden).cuda(), requires_grad=True))\n\n    def forward(self, x, lengths=None):\n        batch_size, timesteps, C, H, W = x.size()\n        self.hidden = self.init_hidden(batch_size)\n\n        # CNN forward\n        c_in = x.view(batch_size * timesteps, C, H, W)\n        c_out = self.cnn(c_in)\n        c_out = c_out.mean(3).mean(2)\n        if self.dropout:\n            c_out = self.drop(c_out)\n\n        # LSTM forward\n        r_in = c_out.view(batch_size, timesteps, -1)\n        r_out, states = self.rnn(r_in, self.hidden)\n        out = self.lin(r_out)\n        out = out.view(batch_size*timesteps,9)\n\n        return out\n\n\n\nRun the code\n\nseq_length=64\n\nds = SampleVideo(stra, transform=transforms.Compose([ToTensor(),\n                                Normalize([0.485, 0.456, 0.406],\n                                          [0.229, 0.224, 0.225])]))\n\ndl = DataLoader(ds, batch_size=1, shuffle=False, drop_last=False)\n\nmodel = EventDetector(pretrain=True,\n                      width_mult=1.,\n                      lstm_layers=1,\n                      lstm_hidden=256,\n                      bidirectional=True,\n                      dropout=False)\ntry:\n    save_dict = torch.load('models/swingnet_1800.pth.tar')\nexcept:\n    print(\"Model weights not found. Download model weights and place in 'models' folder. See README for instructions\")\n    \n    \ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\nmodel.load_state_dict(save_dict['model_state_dict'])\nmodel.to(device)\nmodel.eval()\nprint(\"Loaded model weights\")\n\nprint('Testing...')\nfor sample in dl:\n    images = sample['images']\n    # full samples do not fit into GPU memory so evaluate sample in 'seq_length' batches\n    batch = 0\n    while batch * seq_length < images.shape[1]:\n        if (batch + 1) * seq_length > images.shape[1]:\n            image_batch = images[:, batch * seq_length:, :, :, :]\n        else:\n            image_batch = images[:, batch * seq_length:(batch + 1) * seq_length, :, :, :]\n        logits = model(image_batch.cuda())\n        if batch == 0:\n            probs = F.softmax(logits.data, dim=1).cpu().numpy()\n        else:\n            probs = np.append(probs, F.softmax(logits.data, dim=1).cpu().numpy(), 0)\n        batch += 1\n\n        \nevents = np.argmax(probs, axis=0)[:-1]\nprint('Predicted event frames: {}'.format(events))\n\n\nconfidence = []\nfor i, e in enumerate(events):\n    confidence.append(probs[e, i])\nprint('Confidence: {}'.format([np.round(c, 3) for c in confidence]))\n\nOutput:\nUsing device: cuda\nLoaded model weights\nTesting…\nPredicted event frames: [ 82 121 137 166 189 203 213 245]\nConfidence: [0.215, 0.376, 0.79, 0.767, 0.827, 0.968, 0.935, 0.247]"
  },
  {
    "objectID": "posts/GolfSwing/2022-02-26-GolfSwingPart2.html#plot-the-results",
    "href": "posts/GolfSwing/2022-02-26-GolfSwingPart2.html#plot-the-results",
    "title": "Golf Swing Part II- Separating Swing Positions",
    "section": "Plot the results",
    "text": "Plot the results\n\nimport os\n##delte images\nlsa=os.listdir()\nfimg=[ ll for ll in lsa if ll.split('.')[-1]=='jpg']\n# print(fimg)\nimgs=[os.remove(ff) for ff in fimg]\n\nfimg=[ ll for ll in lsa if ll.split('.')[-1]=='jpg']\n\n\ndef createImages(fila,pos,nomS):\n    ''' \n    Given a video file location (fila) it will save as images to a folder\n    Given positions in video (pos) these images from the video are saved\n    pos is created based on positions of swings\n    '''\n    import cv2\n    cap = cv2.VideoCapture(fila)\n    eventNom=[0,1,2,3,4,5,6,7]\n    for i, e in enumerate(events):\n        cap.set(cv2.CAP_PROP_POS_FRAMES, e)\n        _, img = cap.read()\n        cv2.imwrite(os.path.join(os.getcwd(),'_'+ nomS+'_'+\"frame{:d}.jpg\".format(eventNom[i])), img)     # save frame as JPG file\n    \n    \nfila=stra\npos=events\ncreateImages(fila,pos,'10')\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nlsa=os.listdir()\nfimg=[ ll for ll in lsa if ll.split('.')[-1]=='jpg']\nfimg.sort()\n\nimgs=[mpimg.imread(ff) for ff in fimg]\n\n\ncap = cv2.VideoCapture(stra)\n\n\n# plt.subplot(4,2,1)\nf, axs = plt.subplots(4,2,figsize=(15,15))\nfor i, e in enumerate(events):\n    cap.set(cv2.CAP_PROP_POS_FRAMES, e)\n    _, img = cap.read()\n    plt.subplot(4,2,i+1)\n    plt.imshow(img)\n    plt.title(e)"
  },
  {
    "objectID": "posts/GolfSwing/2022-02-27-GolfSwingPart3.html#overview",
    "href": "posts/GolfSwing/2022-02-27-GolfSwingPart3.html#overview",
    "title": "Golf Swing Part III- Using pre-trained models- FiftyOne and Streamlit App",
    "section": "Overview",
    "text": "Overview\nIn a previous part Part 1 a neural network model was used to find positions on the body during a golf swing. The model was not particularly succesful, perhaps due to the lack of data (specific to the golf swing) that was used to train the model on.\nThis problem can be got around by using a model that has been pre-trained on human gestures. Several pre-trained models can be found here Pre-trained models. I tried a few and found the chose the model keypoint-rcnn-resnet50-fpn-coco-torch worked well with this data. Link to model and Paper of model.\nThe input to the model is taken from Part 2 which separated a golf video into a series of images of the swing.\nIn this page I use the model with both fiftyOne and as a streamlit app.\n\nyoutube: https://youtu.be/Q0BB0huWb6s https://youtu.be/Q0BB0huWb6s"
  },
  {
    "objectID": "posts/GolfSwing/2022-02-27-GolfSwingPart3.html#code",
    "href": "posts/GolfSwing/2022-02-27-GolfSwingPart3.html#code",
    "title": "Golf Swing Part III- Using pre-trained models- FiftyOne and Streamlit App",
    "section": "Code",
    "text": "Code\nThe code can be run on google colab here COCO50_1 (works best on google chrome)"
  },
  {
    "objectID": "posts/GolfSwing/2022-02-27-GolfSwingPart3.html#installs-and-imports",
    "href": "posts/GolfSwing/2022-02-27-GolfSwingPart3.html#installs-and-imports",
    "title": "Golf Swing Part III- Using pre-trained models- FiftyOne and Streamlit App",
    "section": "Installs and imports",
    "text": "Installs and imports\n\n!pip uninstall opencv_python_headless\n\n!pip install opencv-python-headless==4.5.4.60\n\n!pip install fiftyone\n\nimport fiftyone as fo"
  },
  {
    "objectID": "posts/GolfSwing/2022-02-27-GolfSwingPart3.html#upoad-some-images-to-the-workspace",
    "href": "posts/GolfSwing/2022-02-27-GolfSwingPart3.html#upoad-some-images-to-the-workspace",
    "title": "Golf Swing Part III- Using pre-trained models- FiftyOne and Streamlit App",
    "section": "Upoad some images to the workspace",
    "text": "Upoad some images to the workspace\nUntar and create a dataset object from them\nAnd look at them\nThe image files can be found here GC2.tgz\n\nimport tarfile\nmy_tar = tarfile.open('/content/GC2.tgz')\nmy_tar.extractall('/content/my_folder') # specify which folder to extract to\nmy_tar.close()\n\n\nimport fiftyone as fo\nimport fiftyone.zoo as foz\n\nimport fiftyone as fo\n\nname = \"my_folder\"\ndataset_dir = \"/content\"\n\n# Create the dataset\ndataset = fo.Dataset.from_dir(\n    dataset_dir=dataset_dir,\n    dataset_type=fo.types.ImageDirectory,\n    name=name,\n)\n\nsession = fo.launch_app(dataset)\n\n\nThis screen is interative and allows us to look at the images"
  },
  {
    "objectID": "posts/GolfSwing/2022-02-27-GolfSwingPart3.html#load-the-trained-model",
    "href": "posts/GolfSwing/2022-02-27-GolfSwingPart3.html#load-the-trained-model",
    "title": "Golf Swing Part III- Using pre-trained models- FiftyOne and Streamlit App",
    "section": "Load the trained model",
    "text": "Load the trained model\nApply the model to the dataset\nView the results\n\nmodel = foz.load_zoo_model(\"keypoint-rcnn-resnet50-fpn-coco-torch\")\n\n# label_types=[\"classification\", \"classifications\", \"detections\", \"instances\", \"segmentations\", \"keypoints\", \"polylines\", \"polygons\", \"scalar\"],\n\ndataset.apply_model(model, label_field=\"predictions\",label_types='predictions_keypoints')\n\nsession = fo.launch_app(dataset)\n\n\n\nyoutube: https://youtu.be/dkxtOBWD7Vw"
  },
  {
    "objectID": "posts/GolfSwing/2022-02-27-GolfSwingPart3.html#extract-data-from-the-model",
    "href": "posts/GolfSwing/2022-02-27-GolfSwingPart3.html#extract-data-from-the-model",
    "title": "Golf Swing Part III- Using pre-trained models- FiftyOne and Streamlit App",
    "section": "Extract data from the model",
    "text": "Extract data from the model\nWe might want to use the data from the model outside of fiftyOne.\nIn the following I extract the data so that it can be plotted.\n\ndef plotPredOne(i):\n  import numpy as np\n\n  import matplotlib.pyplot as plt\n  import matplotlib.image as mpimg\n\n  img = mpimg.imread(i['filepath'])\n\n  #need to take account of more than one person in image\n  points1 = np.array(i['predictions_keypoints']['keypoints'][0]['points'])\n  adjPts = np.shape(img)[0]\n  box1 = np.array(i['predictions_detections']['detections'][0]['bounding_box']) \n  box1=box1*adjPts\n  # Bboxes are in [top-left-x, top-left-y, width, height] format\n  box2=np.array([ \n      [box1[0], box1[1]],\n      [box1[0] +box1[2] ,box1[1] ],\n      [box1[0] +box1[2] ,box1[1] +box1[3]] ,\n      [box1[0]  ,box1[1] +box1[3]],\n      [box1[0], box1[1]]\n      ])\n \n  plt.figure()\n  plt.imshow(img)\n\n  plt.plot(points1[:,0]*adjPts,points1[:,1]*adjPts, '+k',markersize=10,linewidth=3)\n  plt.plot(box2[:,0],box2[:,1], '--og',markersize=10,linewidth=3)\n\n    #back of body\n  v=[4,6,12,14,16]\n  plt.plot(points1[v,0]*adjPts,points1[v,1]*adjPts, '-k<',markersize=10,linewidth=2)\n\n  #front of body\n  v=[0,5,11,13,15]\n  plt.plot(points1[v,0]*adjPts,points1[v,1]*adjPts, '-w>',markersize=10,linewidth=2)\n\n  vects = np.array([[ 5,6],#shoulders also 4?\n         [11,12], #hips\n         [13,14], #knees\n         [15,16],#heels\n         [7,8],#elbows\n         [9,10],#hands\n         ]) \n  mak='gcyrmb'\n  for iv,v in enumerate(vects):\n    plt.plot(points1[v,0]*adjPts,points1[v,1]*adjPts, '-'+mak[iv],markersize=10,linewidth=3)\n\n\nfor iii,i in enumerate(dataset):\n  \n  plotPredOne(i)"
  },
  {
    "objectID": "posts/GolfSwing/2022-02-27-GolfSwingPart3.html#convert-into-a-streamlit-app",
    "href": "posts/GolfSwing/2022-02-27-GolfSwingPart3.html#convert-into-a-streamlit-app",
    "title": "Golf Swing Part III- Using pre-trained models- FiftyOne and Streamlit App",
    "section": "Convert into a Streamlit App",
    "text": "Convert into a Streamlit App\nTo convert to a streamlit app I will use the PyTorch module rather than the fiftyOne.\nI will also keep it simple by loading only 3 images- start of swing, top of backswing and at impact- and modeling these at the start of the load part of the app.\nThe app will then just plot the images as shown above.\n\nImports and give the app a title\n\nimport streamlit as st\nimport torch\nimport torchvision\nfrom torchvision import transforms\nfrom PIL import Image\nimport tarfile\nimport os\nfrom torchvision.io import read_image\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nst.title('Golf Swing')\n\n\n\nLoading the data & applying the model\nCreate a function to load data and model the data\nload_data(choi)\nImages are loaded from Gc2.tgz\nmy_tar = tarfile.open(cda2+'/GC2.tgz')\nThe particular model to use is loaded\nmodel = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=True)\nmodel.eval()\nImages are loaded and converted to a tensor\nnumber_img = Image.open(cda2+'/images/'+image_filename)\nconvert_tensor = transforms.ToTensor()\nAnd predictions are made\npredictions=model(imgTens)\nIn the main body the function is called\ndata_load_state = st.text('Loading data...')\npredictions,imgLocAll,cda2=load_data(1)\ndata_load_state.text(\"Loaded data (using st.cache)\")\n\n# So only have to do this when app launches\n@st.cache()\n\n# the function 'choi' is the video file to use\ndef load_data(choi):\n\n    # the images are in the GC2.tgz file- this needs to be untarred first\n    cda = os.getcwd()\n    cda2=cda\n    my_tar = tarfile.open(cda2+'/GC2.tgz')\n    my_tar.extractall(cda2) # specify which folder to extract to\n    my_tar.close()\n\n    # Create a variable of the image names and which video they are part of\n    imgAll=[]\n    vidAll=[]\n    i=0\n    last1=' '\n    for xx in os.listdir(cda2+'/images/'):\n        if xx[-1]=='g':\n            imgAll = np.append(imgAll, xx)\n            if xx.split('_')[1]!=last1:\n                i=i+1\n            vidAll=np.append(vidAll,i)\n            last1=xx.split('_')[1]\n\n    vidAllUnq=np.unique(vidAll)\n    \n    # Load the model to be used\n    model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=True)\n    model.eval()\n    \n    # Select the images to be used\n    imgs = imgAll[vidAll==vidAllUnq[choi]]\n\n    # make sure in correct order\n    aa=[int(xx.split('_')[-1].split('e')[1].split('.')[0]) for xx in imgs]\n    ind=sorted(range(len(aa)), key=lambda k: aa[k])\n    imgs=imgs[ind]\n\n    # create tensor of images to be used- here 3 (images) X width X height\n    imgTens=[]\n    imgLocAll=[]\n    \n    # Just use the start, top and impact of swing\n    iiUse=[0,3,5]\n    for ii,image_filename in enumerate(imgs):\n    #             print(cda2+'images/'+image_filename)\n        if ii in iiUse:\n            number_img = Image.open(cda2+'/images/'+image_filename)\n            convert_tensor = transforms.ToTensor()\n            number_img=convert_tensor(number_img)\n            imgTens.append(number_img)\n            imgLocAll.append(image_filename)\n\n    # Make the predictions\n    predictions=model(imgTens)\n    \n    return predictions,imgLocAll,cda2\n\n# Outside the function, the load function is called\ndata_load_state = st.text('Loading data...')\npredictions,imgLocAll,cda2=load_data(1)\ndata_load_state.text(\"Loaded data (using st.cache)\")\n\n\n\nStreamlit user interface\nUser selects the images from this box:\nchoice=imgLocAll\nimgSEL = st.sidebar.selectbox(     'Select how to search',      choice)\nDisplay to user what swing it is:\nSwingPos=['Start','Back','Through']\nSwingPos[numSEL]\nAnd at the end of the file the figure is displayed in streamlit with the following command:\nst.pyplot(fig)\n\n\nThe plot part\nExtract the data from the model about different parts of the body:\npoints1=np.array([x.detach().numpy()[0:2] for x in predictions[numSEL]['keypoints'][0]])\nThe plot lines plot different parts of the body, the following plot the back of the body\nv=[4,6,12,14,16]\nplt.plot(points1[v,0]*adjPts,points1[v,1]*adjPts, '-w<',markersize=10,linewidth=2)\n\n# load the images so can be plotted\nimg = mpimg.imread(cda2+'/images/'+imgSEL)\n\n# the image selected\nnumSEL=[oo for oo,x in enumerate(choice) if x==imgSEL][0]\n\n# get data from model as a numpy array - here want keypoints other info is also available\npoints1=np.array([x.detach().numpy()[0:2] for x in predictions[numSEL]['keypoints'][0]])\n\n# create a plot\nfig=plt.figure(figsize=(7,7))\nplt.imshow(img)\n\n# Plot across back and front of body\nadjPts=1\n#back of body\nv=[4,6,12,14,16]\nplt.plot(points1[v,0]*adjPts,points1[v,1]*adjPts, '-w<',markersize=10,linewidth=2)\n\n#front of body\nv=[0,5,11,13,15]\nplt.plot(points1[v,0]*adjPts,points1[v,1]*adjPts, '-k>',markersize=10,linewidth=2)\n\n# Plot over lines on body\nvects = np.array([[ 5,6],#shoulders also 4?\n     [11,12], #hips\n     [13,14], #knees\n     [15,16],#heels\n     [7,8],#elbows\n     [9,10],#hands\n     ]) \nmak='gcyrmb'\nfor iv,v in enumerate(vects):\n    plt.plot(points1[v,0]*adjPts,points1[v,1]*adjPts, '-'+mak[iv],markersize=10,linewidth=3)\n\nLEG=['Back','Front','Shoulders','Hips','Knees','Heels','Elbows','Hands']\nplt.legend(LEG)\nfor x in points1:\n    plt.plot(x[0],x[1],'+b')\n\n\n\nRequirements.txt\nFinally streamlit needs a requirements text in the GitHub repository\n\ntorch\ntorchvision\nPillow\nmatplotlib\nnumpy"
  },
  {
    "objectID": "posts/GolfSwing/2022-02-27-GolfSwingPart3.html#the-streamlit-app",
    "href": "posts/GolfSwing/2022-02-27-GolfSwingPart3.html#the-streamlit-app",
    "title": "Golf Swing Part III- Using pre-trained models- FiftyOne and Streamlit App",
    "section": "The Streamlit App",
    "text": "The Streamlit App\nStreamlit App\nGitHub page\n\nyoutube: https://youtu.be/Q0BB0huWb6s"
  },
  {
    "objectID": "posts/GolfSwing/GolfSwingPart4.html#modifications-to-code-for-separating-swing-video",
    "href": "posts/GolfSwing/GolfSwingPart4.html#modifications-to-code-for-separating-swing-video",
    "title": "ThomasHSimm",
    "section": "Modifications to code for separating swing video",
    "text": "Modifications to code for separating swing video\n\n- Imported files are not saved to a location\nThis means can’t pass ‘path’ when creating the Dataset. So that line is removed from the __init__\nIn theory the video could then be loaded with cap = cv2.VideoCapture(self) but this doesn’t work as openCV requires a file. So a get around for this is create a temp file https://discuss.streamlit.io/t/how-to-access-uploaded-video-in-streamlit-by-open-cv/5831/4\nf = st.file_uploader(\"Upload file\")\ntfile = tempfile.NamedTemporaryFile(delete=False)\ntfile.write(f.read())\nvf = cv.VideoCapture(tfile.name)\nFor the same reason the image files are not saved as a file\n\n\n- Video load issue\nIf the video is loaded a second time ret, img = cv2.VideoCapture there are problems receieving the video i.e. ret=False.\nA get around used was to copy the imported file\nuploaded_filesCOPY = copy.copy( uploaded_files )\n\n\n- Using on a CPU instead of a GPU\nThe code needed modifying slightly to allow it to work using a CPU. Although it does have the following line, a few more changes were needed\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nAdding a description in the loads\ntorch.load('mobilenet_v2.pth.tar',map_location=torch.device('cpu'))\nRemoving the cdu() part at end of e.g. variables\nVariable(torch.zeros(2*self.lstm_layers, batch_size, self.lstm_hidden).cuda()\n# to     \nVariable(torch.zeros(2*self.lstm_layers, batch_size, self.lstm_hidden)"
  },
  {
    "objectID": "posts/olympics/2022-07-20-OlympicsSQL-HeightWeightAge.html#background",
    "href": "posts/olympics/2022-07-20-OlympicsSQL-HeightWeightAge.html#background",
    "title": "Olympics data with SQL and pandas- height weight and age",
    "section": "Background",
    "text": "Background\nI did some initial plots on the changes in the characteristics of athletes given in the data, height, weight and age, of athletes attending the Olympics by year (see below).\nFrom these plots I was really intrigued as to what may be the cause of these changes.\nMainly what was happening between 1960 and 1980 were there seemed to be changes in each of the parameters?\nMy initial thought was this could be related to some combination of - a switch from amateurs to professionals - the Cold War between USA and USSR - an after effect of WWII\n\n\n\n\nOlympic Background\nThroughout much of the Olympic’s history there has been tension around professionals and amateur athletes. The games were intended for amateur athletes, and those who played sport professionally were banned or even had their medals stripped.\nThe reasoning behind amateurism was based on how sport was seen by the aristrocracy and greatly influenced Pierre de Coubertin, who is thought of as the father of the Olympic games:\n\"There was also a prevailing concept of fairness, in which practising or training was considered tantamount to cheating.[2] Those who practised a sport professionally were considered to have an unfair advantage over those who practised it merely as a hobby.[2]\"\nThe Soviet Union, who competed from 1952-1988, entered teams of athletes who were all nominally students, soldiers, or working in a profession, but all of whom were in reality paid by the state to train on a full-time basis.[3] The situation greatly disadvantaged American and Western European athletes, and was a major factor in the decline of American medal hauls in the 1970s and 1980s. However, workarounds in Western countries also allowed individuals to focus full-time on sport while passing the amateur rules.[4]\nThis abuse of amateur rules by the Eastern Bloc nations prompted the IOC to shift away from pure amateurism.The rules were steadily relaxed from 1972, amounting only to technicalities and lip service, until being completely abandoned in the 1990s\n\nWikipedia Olympic Games And Amateurism\nEassom 1994, pp. 120–123\n“The Role of Sports in The Soviet Union – Guided History”. blogs.bu.edu.\nDegrees of Difficulty: How Women’s Gymnastics Rose to Prominence and Fell from Grace”, by Georgia Cervin"
  },
  {
    "objectID": "posts/olympics/2022-07-20-OlympicsSQL-HeightWeightAge.html#load-data-and-libraries",
    "href": "posts/olympics/2022-07-20-OlympicsSQL-HeightWeightAge.html#load-data-and-libraries",
    "title": "Olympics data with SQL and pandas- height weight and age",
    "section": "Load data and libraries",
    "text": "Load data and libraries\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pandasql import sqldf\nimport copy\nimport numpy as np\nimport scipy.stats\n\n\ndf_F_S =pd.read_csv('athlete_F_S')\ndf_F_W=pd.read_csv('athlete_F_W')\ndf_M_S=pd.read_csv('athlete_M_S')\ndf_M_W=pd.read_csv('athlete_M_W')\n\ndf_all_athletes= pd.read_csv('all_athletes')\ndf_country= pd.read_csv('country')\ndf_event= pd.read_csv('event')\n# df_games= pd.read_csv('games')\n# df_population= pd.read_csv('population')\n\n# df_country = df_country.groupby('NOC').max()\n# df_country.head(10)"
  },
  {
    "objectID": "posts/olympics/2022-07-20-OlympicsSQL-HeightWeightAge.html#what-are-the-best-weight-height-age",
    "href": "posts/olympics/2022-07-20-OlympicsSQL-HeightWeightAge.html#what-are-the-best-weight-height-age",
    "title": "Olympics data with SQL and pandas- height weight and age",
    "section": "What are the best Weight, Height, Age?",
    "text": "What are the best Weight, Height, Age?\nObviously this will depend on event. But if we average across events what are - the best of values of these? - and how do these change with time?\nTo get this figure the methodology is fairly simple, in the SQL query: - We take the average of weight, height, age across years and medal type - Because we want a simple binary answer (medal or not) we create a variable called medal which is 1 if they got a medal and 0 otherwise - We then group on this and take an average - The two function below are just so we can plot for avg_weight, avg_height and avg_age without repeating the same steps\nhere Male summer athletes are used but the result for female summer athletes show the same trend\n\nmedalQ=sqldf('\\\n    SELECT                                 \\\n        Year,                              \\\n        medal,                             \\\n        AVG(avg_weight)    AS avg_weight,  \\\n        AVG(avg_height)    AS avg_height,  \\\n        AVG(avg_age)       AS avg_age      \\\n    FROM                                   \\\n         (SELECT                           \\\n         Year,                             \\\n         MAX(Medal_Gold,Medal_Silver,Medal_Bronze)\\\n                           AS medal,       \\\n         avg(Weight)       AS avg_weight,  \\\n         AVG(Height)       AS avg_height,  \\\n         AVG(age)          AS avg_age      \\\n         from df_M_S                       \\\n         group by                          \\\n             Year,                         \\\n             Medal_Gold,Medal_Silver,Medal_Bronze           \\\n         order by Year asc) A              \\\n     GROUP BY Year, medal;',locals())  \nmedalQ.head()\n\n\n\n\n\n  \n    \n      \n      Year\n      medal\n      avg_weight\n      avg_height\n      avg_age\n    \n  \n  \n    \n      0\n      1896\n      0\n      70.444444\n      169.916667\n      23.896552\n    \n    \n      1\n      1896\n      1\n      71.551282\n      175.217949\n      23.211671\n    \n    \n      2\n      1900\n      0\n      76.971429\n      175.054545\n      29.428571\n    \n    \n      3\n      1900\n      1\n      72.711355\n      178.202932\n      28.454139\n    \n    \n      4\n      1904\n      0\n      71.742424\n      175.131579\n      26.752080\n    \n  \n\n\n\n\n\ndef modname(string):\n    string=''.join([string[0].upper(),string[1:].lower()])\n    string=string.replace('_',' ')\n    return string\n    \ndef plotMedal(plotchoi,medalQ):\n    plt.subplots(figsize=(8,5))\n    plt.plot(medalQ[medalQ.medal==1].Year,medalQ[medalQ.medal==1][plotchoi],'g*-')\n\n    plt.plot(medalQ[medalQ.medal==0].Year,medalQ[medalQ.medal==0][plotchoi],'rv--')\n\n    plt.legend(['Medal','No medal'])\n    plt.grid(True)\n    plt.xlabel('Year')\n    plt.ylabel(modname(plotchoi))\n\n\nplotchoi='avg_height'\nplotMedal(plotchoi,medalQ)\n\nplotchoi='avg_weight'\nplotMedal(plotchoi,medalQ)\n\nplotchoi='avg_age'\nplotMedal(plotchoi,medalQ)\nplt.ylim([18,40])\n\n(18.0, 40.0)\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer: What is the best weight, height and age?\n\nAge, height and weight of athletes change with year\nAfter the initial years (> ~1930)\n\nAthletes who get more medals have greater height and weight\nWhereas, the age is indistinguishable"
  },
  {
    "objectID": "posts/olympics/2022-07-20-OlympicsSQL-HeightWeightAge.html#the-change-in-athletes-based-on-weight-height-and-age",
    "href": "posts/olympics/2022-07-20-OlympicsSQL-HeightWeightAge.html#the-change-in-athletes-based-on-weight-height-and-age",
    "title": "Olympics data with SQL and pandas- height weight and age",
    "section": "The change in athletes based on weight, height and age",
    "text": "The change in athletes based on weight, height and age\nHere the methodology used to produce the figures in the background section is presented.\nThe method is fairly simple, we just use each athlete table and then GROUP BY year and take the averages\n\ndf_F=\\\n     sqldf('SELECT                              \\\n              Year,                             \\\n               avg(Height) AS avg_height,       \\\n               avg(Weight) AS avg_weight,       \\\n               avg(Age)    AS avg_age           \\\n            FROM                                \\\n                df_F_S AS d                     \\\n            GROUP BY                            \\\n               Year                             \\\n            ORDER BY                            \\\n                Year asc;',locals())\ndf_M=\\\n     sqldf('SELECT                              \\\n              Year,                             \\\n               avg(Height) AS avg_height,       \\\n               avg(Weight) AS avg_weight,       \\\n               avg(Age)    AS avg_age           \\\n            FROM                                \\\n                df_M_S AS d                     \\\n            GROUP BY                            \\\n               Year                             \\\n            ORDER BY                            \\\n                Year asc;',locals())\n               \ndf_Fw=\\\n     sqldf('SELECT                              \\\n              Year,                             \\\n               avg(Height) AS avg_height,       \\\n               avg(Weight) AS avg_weight,       \\\n               avg(Age)    AS avg_age           \\\n            FROM                                \\\n                df_F_W AS d                     \\\n            GROUP BY                            \\\n               Year                             \\\n            ORDER BY                            \\\n                Year asc;',locals())\ndf_Mw=\\\n     sqldf('SELECT                              \\\n              Year,                             \\\n               avg(Height) AS avg_height,       \\\n               avg(Weight) AS avg_weight,       \\\n               avg(Age)    AS avg_age           \\\n            FROM                                \\\n                df_M_W AS d                     \\\n            GROUP BY                            \\\n               Year                             \\\n            ORDER BY                            \\\n                Year asc;',locals())\n               \ndf_F.head()                    \n\n\n\n\n\n  \n    \n      \n      Year\n      avg_height\n      avg_weight\n      avg_age\n    \n  \n  \n    \n      0\n      1900\n      NaN\n      NaN\n      29.791667\n    \n    \n      1\n      1904\n      NaN\n      NaN\n      50.230769\n    \n    \n      2\n      1906\n      NaN\n      NaN\n      23.500000\n    \n    \n      3\n      1908\n      NaN\n      NaN\n      33.897436\n    \n    \n      4\n      1912\n      NaN\n      NaN\n      22.379310\n    \n  \n\n\n\n\n\ndef yrplot(df_F,df_M,df_Fw,df_Mw,whatplot= 'avg_weight'): \n    cola=['r>','b<','mo','cs']\n\n    plt.subplots(figsize=(6,4))\n    plt.plot(df_F.Year,df_F[whatplot],cola[0],markersize=10)\n\n    plt.plot(df_M.Year,df_M[whatplot],cola[1],markersize=10)\n\n    plt.plot(df_Fw.Year,df_Fw[whatplot],cola[2])\n    plt.plot(df_Mw.Year,df_Mw[whatplot],cola[3])\n\n    def doPlot(df_F,avgNo,whatplot,col,lw):\n        bb = df_F.Year.rolling(avgNo).mean()\n        cc = df_F[whatplot]\n        cc = cc.rolling(avgNo).mean()\n        plt.plot(bb,cc,col,linewidth=lw)\n\n    doPlot(df_F,3,whatplot,'r-',4)\n    doPlot(df_M,3,whatplot,'b-',4)\n\n    doPlot(df_Fw,3,whatplot,'m--',2)\n    doPlot(df_Mw,3,whatplot,'c--',2)\n\n    plt.legend(['Female Summer','Male Summer','Female Winter','Male Winter'])\n    plt.ylabel(modname(whatplot),fontsize=14)\n    plt.xlabel('Year',fontsize=14)\n    plt.xlim([1890, 2020])\n    plt.grid(True)\n\n\nyrplot(df_F,df_M,df_Fw,df_Mw,whatplot= 'avg_weight')\nyrplot(df_F,df_M,df_Fw,df_Mw,whatplot= 'avg_height')\nyrplot(df_F,df_M,df_Fw,df_Mw,whatplot= 'avg_age')\nplt.ylim([18,35]);\n\n\n\n\n\n\n\n\n\n\n\nAre the changes due to changes in athletes or changes in the Olympics?\n\nJoin athlete table df_M_S with event table df_event\n\nUse this to get a list of events and the year they occur\n\nGroup this table\n\nTo get which event meet criteria of a minimum date, maximum date and having occured so many times\n\n\n(select\nEvent_id\n……\nAND max(year)>1990) usea\n\nGroup the table of events with the athlete table df_M_S\nTake the average over the events and year\nTake this average just over year\n\nThis stops changes due to changes in number of athletes in a particular event\n\n\nThere are 15-20 events included in the results below. Although, a relatively low figure this still represents a lot of athletes. Furthermore, when we split these events into 2 the same trends we find in all the data are seen in the two splits. The absolute values can differ but the min./max. values seem to fairly consistent. But obviously more exploration would be beneficial if this is led by theories or experts in the areas. Without this we could spend forever looking for trends.\n\n# --find events that have occured more than a set amount within a range of dates\n# -- i.e. events that can focus on to see results of changes with time\n\ndef do_same_event(df_M_S,df_F_S,df_event,athlete_df_name='df_M_S',counta='17',yr_start='1900',yr_end='1990'):\n    tempa= sqldf('                               \\\n    SELECT                                   \\\n        Year,                                \\\n        avg(wgt)            AS avg_weight,   \\\n        avg(hgt)            AS avg_height,   \\\n        avg(aga)            AS avg_age       \\\n    FROM                                     \\\n        (SELECT                              \\\n        Year,                                \\\n        AVG(weight)         AS wgt,          \\\n        AVG(height)         AS hgt,          \\\n        AVG(age)            AS aga           \\\n        FROM                                 \\\n        (SELECT                              \\\n          Event_id                           \\\n        FROM                                 \\\n            (SELECT                          \\\n            E.Event_id,                      \\\n            Year,                            \\\n            count(*)        AS counta        \\\n            FROM                             \\\n                {0}         AS A             \\\n            LEFT JOIN                        \\\n                df_event    AS E             \\\n            ON                               \\\n                E.event_id = A.event_id      \\\n            GROUP BY                         \\\n                E.Event_id,                  \\\n                Year                         \\\n            ORDER BY year asc) AA            \\\n        GROUP BY AA.Event_id                 \\\n        HAVING COUNT(*) >{1}                 \\\n        AND MIN(year)<{2}                    \\\n        AND MAX(year)>{3}                    \\\n        ORDER BY event_id asc                \\\n        LIMIT 80) usea                       \\\n        LEFT JOIN                            \\\n            {0}              AS a            \\\n        ON                                   \\\n            usea.event_id = a.event_id       \\\n        GROUP BY                             \\\n            year,                            \\\n            usea.event_id                    \\\n        ORDER BY year asc) two               \\\n        GROUP BY year;'.format(athlete_df_name,counta,yr_start,yr_end),locals())\n    return tempa\n\n\ntempaM = do_same_event(df_M_S,df_F_S,df_event,'df_M_S')\ntempaF = do_same_event(df_M_S,df_F_S,df_event,'df_F_S',counta='12',yr_start='1945',yr_end='1990')\n\ntempaF.head()\n\n\n\n\n\n  \n    \n      \n      Year\n      avg_weight\n      avg_height\n      avg_age\n    \n  \n  \n    \n      0\n      1900\n      NaN\n      NaN\n      25.250000\n    \n    \n      1\n      1906\n      NaN\n      NaN\n      23.500000\n    \n    \n      2\n      1908\n      NaN\n      NaN\n      31.200000\n    \n    \n      3\n      1912\n      NaN\n      NaN\n      21.759259\n    \n    \n      4\n      1920\n      NaN\n      160.145833\n      21.737132\n    \n  \n\n\n\n\n\ndef modname(string):\n    string=''.join([string[0].upper(),string[1:].lower()])\n    string=string.replace('_',' ')\n    return string\n\ndef yrplot1(df_F,df_M,whatplot): \n    cola=['r>','b<','mo','cs']\n\n    fig,ax=plt.subplots(figsize=(8,5))\n    ax.plot(df_F.Year,df_F[whatplot],cola[0],markersize=10)\n    ax2=ax.twinx()\n    ax2.plot(df_M.Year,df_M[whatplot],cola[1],markersize=10)\n\n#     plt.plot(df_Fw.Year,df_Fw[whatplot],cola[2])\n#     plt.plot(df_Mw.Year,df_Mw[whatplot],cola[3])\n\n\n    def doPlot(df_F,avgNo,whatplot,col,lw,xx):\n        bb = df_F.Year.rolling(avgNo).mean()\n        cc = df_F[whatplot]\n        cc = cc.rolling(avgNo).mean()\n        xx.plot(bb,cc,col,linewidth=lw)\n\n    doPlot(df_F,3,whatplot,'r-',4,ax)\n    doPlot(df_M,3,whatplot,'b-',4,ax2)\n\n#     doPlot(df_Fw,3,whatplot,'m--',2)\n#     doPlot(df_Mw,3,whatplot,'c--',2)\n\n    fig.legend(['Female Summer','-','Male Summer','-'],loc='upper center')#,'Female Winter','Male Winter'])\n    ax.set_ylabel('Female ' + modname(whatplot),fontsize=14)\n    ax2.set_ylabel('Male ' + modname(whatplot),fontsize=14)\n    ax.set_xlabel('Year',fontsize=14)\n    plt.xlim([1890, 2020])\n    ax2.grid(True)\n    \n\n\nyrplot1(tempaF,tempaM,whatplot= 'avg_weight')\nyrplot1(tempaF,tempaM,whatplot= 'avg_height')\nyrplot1(tempaF,tempaM,whatplot= 'avg_age')\n# no limit"
  },
  {
    "objectID": "posts/olympics/2022-07-20-OlympicsSQL-HeightWeightAge.html#the-cold-war",
    "href": "posts/olympics/2022-07-20-OlympicsSQL-HeightWeightAge.html#the-cold-war",
    "title": "Olympics data with SQL and pandas- height weight and age",
    "section": "The Cold War",
    "text": "The Cold War\n\ndef yrplot(df__,whatplot= 'avg_weight'): \n    \n    countries=['EST', 'EUN' ,'ROW', 'USA' ,'WES']\n    \n#     df__.NOCSMALL.unique()\n#     countries=np.sort(countries)\n    print(countries)\n    cola=['>','o','+','*','<']\n    colur=[[1,0.6,.6],[1,0,0],[.5,.5,.5],[0,0,1],[.6,.6,1]]\n#     ['EST' 'EUN' 'ROW' 'USA' 'WES']\n#     'EST','USA','WES','ROW','EUN'\n\n    fig,ax1=plt.subplots(figsize=(8,5))\n    \n    for i,country in enumerate(countries):\n        if country!='ROW':\n            ax1.plot(df__[df__.NOCSMALL==country].Year,\\\n                 df__[df__.NOCSMALL==country][whatplot],\\\n                 marker=cola[i],linestyle='None',color=colur[i]\\\n                 ,markersize=10)\n\n\n    def doPlot(df_F,avgNo,whatplot,country,col,lw,ax1):\n        bb = df_F[df__.NOCSMALL==country].Year.rolling(avgNo).mean()\n        cc = df_F[df__.NOCSMALL==country][whatplot]\n        cc = cc.rolling(avgNo).mean()\n        ax1.plot(bb,cc,linewidth=lw,color=col)\n        return ax1\n\n    for i,country in enumerate(countries):\n        if country!='ROW':\n            ax1=doPlot(df__,avgNo=3,whatplot=whatplot,country=country,col=colur[i],lw=3,ax1=ax1)\n    \n    lega = ['East Europe','Russia','USA','West Europe']\n    plt.legend(lega)\n    plt.grid(True)\n    plt.ylabel(modname(whatplot))\n    \n    return ax1\n\n\ndef get_df_USA_USSR(df_M_S,df_F_S,nameDF):\n    USA_USSR=sqldf(\\\n           'SELECT                            \\\n              Year,                           \\\n              NOCSMALL,                       \\\n              AVG(avg_height)  AS avg_height, \\\n              AVG(avg_weight)  AS avg_weight, \\\n              AVG(avg_age)     AS avg_age,    \\\n              SUM(number_of_athletes) AS number_of_athletes         \\\n           FROM                                    \\\n               (SELECT                             \\\n                   Year,                           \\\n                   AVG(avg_height)  AS avg_height, \\\n                   AVG(avg_weight)  AS avg_weight, \\\n                   AVG(avg_age)     AS avg_age,    \\\n                   SUM(num_ath) AS number_of_athletes,        \\\n                   CASE                                \\\n                       WHEN NOC IN (\"FRA\",\"ESP\",\"ITA\",\"POR\",\"GBR\",\"IRL\",\"NED\",\"BEL\",\"DEN\",\"SUI\") THEN \"WES\"\\\n                       WHEN NOC IN (\"POL\",\"ROU\",\"UKR\",\"LAT\",\"BUL\",\"HUN\",\"LTU\",\"LAT\",\"BLR\",\"ALB\",\"SVK\",\"AUT\",\"EST\",\"BIH\",\"BOH\") THEN \"EST\"\\\n                       WHEN NOC=\"USA\" THEN \"USA\"       \\\n                       WHEN NOC=\"EUN\" THEN \"EUN\"       \\\n                       ELSE \"ROW\"\\\n                   END AS NOCSMALL,                     \\\n                   NOC\\\n               FROM                               \\\n                  (SELECT                         \\\n                  Year,                           \\\n                  NOC,                            \\\n                  avg(Height) AS avg_height,      \\\n                  avg(Weight) AS avg_weight,      \\\n                  avg(Age)    AS avg_age,         \\\n                  count(*)    AS num_ath          \\\n                  FROM                            \\\n                    {} AS d                       \\\n                  GROUP BY                        \\\n                    Year,NOC                      \\\n                  ORDER BY                        \\\n                    Year asc) A                   \\\n              GROUP BY                            \\\n                Year,NOC) B                       \\\n           GROUP BY Year, NOCSMALL ;'.format(nameDF),locals())\n    return USA_USSR\n\n\ndef do_USA_USSR(df_M_S,df_F_S,men_women,whatplot):\n    if men_women=='men':\n        nameDF='df_M_S'\n    elif men_women=='women':\n        nameDF='df_F_S'\n    \n        \n    USA_USSR_F = get_df_USA_USSR(df_M_S,df_F_S,nameDF)\n    ax1=yrplot(USA_USSR_F,whatplot)\n    \n    return ax1\n    \n    \n\n\ndo_USA_USSR(df_M_S,df_F_S,'men','avg_weight')\nplt.ylim([65, 90])\n\n['EST', 'EUN', 'ROW', 'USA', 'WES']\n\n\n(65.0, 90.0)\n\n\n\n\n\n\ndo_USA_USSR(df_M_S,df_F_S,'women','avg_weight')\nplt.ylim([50, 70])\n\n['EST', 'EUN', 'ROW', 'USA', 'WES']\n\n\n(50.0, 70.0)\n\n\n\n\n\n\ndo_USA_USSR(df_M_S,df_F_S,'men','avg_height')\n\n['EST', 'EUN', 'ROW', 'USA', 'WES']\n\n\n<AxesSubplot:ylabel='Avg height'>\n\n\n\n\n\n\ndo_USA_USSR(df_M_S,df_F_S,'women','avg_height')\n# plt.ylim([65, 90])\n\n['EST', 'EUN', 'ROW', 'USA', 'WES']\n\n\n<AxesSubplot:ylabel='Avg height'>\n\n\n\n\n\n\ndo_USA_USSR(df_M_S,df_F_S,'men','avg_age')\n# plt.ylim([65, 90])\n\n['EST', 'EUN', 'ROW', 'USA', 'WES']\n\n\n<AxesSubplot:ylabel='Avg age'>\n\n\n\n\n\n\ndo_USA_USSR(df_M_S,df_F_S,'women','avg_age')\nplt.ylim([18, 35])\n\n['EST', 'EUN', 'ROW', 'USA', 'WES']\n\n\n(18.0, 35.0)\n\n\n\n\n\n\ndef number_of_athletes_USA_USSR(df_F_S,df_M_S):\n    testa2=sqldf('\\\n        SELECT                                 \\\n        Year,                              \\\n        NOCSMALL,                          \\\n        count(*) AS number_of_athletes,     \\\n        \"F\" AS Sex                          \\\n    FROM                                   \\\n         (SELECT                           \\\n         athlete_ID,                        \\\n         Year,                             \\\n         CASE                                \\\n            WHEN NOC IN (\"FRA\",\"ESP\",\"ITA\",\"POR\",\"GBR\",\"IRL\",\"NED\",\"BEL\",\"DEN\",\"SUI\") THEN \"WES\"\\\n            WHEN NOC IN (\"POL\",\"ROU\",\"UKR\",\"LAT\",\"BUL\",\"HUN\",\"LTU\",\"LAT\",\"BLR\",\"ALB\",\"SVK\",\"AUT\",\"EST\",\"BIH\",\"BOH\") THEN \"EST\"\\\n            WHEN NOC=\"USA\" THEN \"USA\"       \\\n            WHEN NOC=\"EUN\" THEN \"EUN\"       \\\n            ELSE \"ROW\"\\\n            END AS NOCSMALL                \\\n         from df_F_S                       \\\n         group by athlete_ID,Year               \\\n         order by Year asc) A              \\\n     GROUP BY Year, NOCSMALL               \\\n     UNION ALL                                 \\\n     SELECT                                \\\n        Year,                              \\\n        NOCSMALL,                          \\\n        count(*) AS number_of_athletes,     \\\n        \"M\" AS Sex                          \\\n    FROM                                   \\\n         (SELECT                           \\\n         athlete_ID,                        \\\n         Year,                             \\\n         CASE                                \\\n            WHEN NOC IN (\"FRA\",\"ESP\",\"ITA\",\"POR\",\"GBR\",\"IRL\",\"NED\",\"BEL\",\"DEN\",\"SUI\") THEN \"WES\"\\\n            WHEN NOC IN (\"POL\",\"ROU\",\"UKR\",\"LAT\",\"BUL\",\"HUN\",\"LTU\",\"LAT\",\"BLR\",\"ALB\",\"SVK\",\"AUT\",\"EST\",\"BIH\",\"BOH\") THEN \"EST\"\\\n            WHEN NOC=\"USA\" THEN \"USA\"       \\\n            WHEN NOC=\"EUN\" THEN \"EUN\"       \\\n            ELSE \"ROW\"\\\n            END AS NOCSMALL                \\\n         from df_M_S                       \\\n         group by athlete_ID,Year          \\\n         order by Year asc) A              \\\n     GROUP BY Year, NOCSMALL;',locals()  )\n    return testa2\n\ndef number_of_medals_USA_USSR(df_F_S,df_M_S):\n    testa2=sqldf('\\\n        SELECT                                 \\\n            COUNT(*) AS number_of_medals,\\\n            Year, Sex, NOCSMALL\\\n        FROM \\\n        (SELECT NOCSMALL,Year,Sex,COUNT(*) AS counta\\\n        FROM                                   \\\n         (SELECT                           \\\n         athlete_ID,                        \\\n         event_ID,                          \\\n         \"F\"   AS Sex,                      \\\n         Medal_Gold,Medal_Silver,Medal_Bronze,\\\n         Year,                             \\\n         CASE                                \\\n            WHEN NOC IN (\"FRA\",\"ESP\",\"ITA\",\"POR\",\"GBR\",\"IRL\",\"NED\",\"BEL\",\"DEN\",\"SUI\") THEN \"WES\"\\\n            WHEN NOC IN (\"POL\",\"ROU\",\"UKR\",\"LAT\",\"BUL\",\"HUN\",\"LTU\",\"LAT\",\"BLR\",\"ALB\",\"SVK\",\"AUT\",\"EST\",\"BIH\",\"BOH\") THEN \"EST\"\\\n            WHEN NOC=\"USA\" THEN \"USA\"       \\\n            WHEN NOC=\"EUN\" THEN \"EUN\"       \\\n            ELSE \"ROW\"\\\n            END AS NOCSMALL                \\\n         from df_F_S                       \\\n         WHERE Medal_Gold=1 OR Medal_Silver=1 OR Medal_Bronze=1\\\n         UNION ALL                                 \\\n         SELECT                           \\\n         athlete_ID,                        \\\n         event_ID,                          \\\n         \"M\" AS Sex,                       \\\n         Medal_Gold,Medal_Silver,Medal_Bronze,\\\n         Year,                             \\\n         CASE                                \\\n            WHEN NOC IN (\"FRA\",\"ESP\",\"ITA\",\"POR\",\"GBR\",\"IRL\",\"NED\",\"BEL\",\"DEN\",\"SUI\") THEN \"WES\"\\\n            WHEN NOC IN (\"POL\",\"ROU\",\"UKR\",\"LAT\",\"BUL\",\"HUN\",\"LTU\",\"LAT\",\"BLR\",\"ALB\",\"SVK\",\"AUT\",\"EST\",\"BIH\",\"BOH\") THEN \"EST\"\\\n            WHEN NOC=\"USA\" THEN \"USA\"       \\\n            WHEN NOC=\"EUN\" THEN \"EUN\"       \\\n            ELSE \"ROW\"\\\n            END AS NOCSMALL                \\\n         from df_M_S                       \\\n         WHERE Medal_Gold=1 OR Medal_Silver=1 OR Medal_Bronze=1\\\n         order by Year asc) A\\\n     GROUP BY \\\n         Year, NOCSMALL,event_id,Medal_Gold,Medal_Silver,Medal_Bronze)  AS B\\\n GROUP BY Year, NOCSMALL, Sex\\\n                 ;',locals()  )                                       \n    return testa2\n\n\nUSA_USSR_medals=number_of_medals_USA_USSR(df_F_S,df_M_S)\nUSA_USSR_athletes=number_of_athletes_USA_USSR(df_F_S,df_M_S)\n\n\nUSA_USSR_medals.head()\n\n\n\n\n\n  \n    \n      \n      number_of_medals\n      Year\n      Sex\n      NOCSMALL\n    \n  \n  \n    \n      0\n      11\n      1896\n      M\n      EST\n    \n    \n      1\n      61\n      1896\n      M\n      ROW\n    \n    \n      2\n      19\n      1896\n      M\n      USA\n    \n    \n      3\n      29\n      1896\n      M\n      WES\n    \n    \n      4\n      2\n      1900\n      F\n      EST\n    \n  \n\n\n\n\n\nyrplot(USA_USSR_medals[USA_USSR_medals.Sex=='F'],whatplot= 'number_of_medals')\n\n['EST', 'EUN', 'ROW', 'USA', 'WES']\n\n\n<AxesSubplot:ylabel='Number of medals'>\n\n\n\n\n\n\nyrplot(USA_USSR_medals[USA_USSR_medals.Sex=='M'],whatplot= 'number_of_medals')\n\n['EST', 'EUN', 'ROW', 'USA', 'WES']\n\n\n<AxesSubplot:ylabel='Number of medals'>\n\n\n\n\n\n\nyrplot(USA_USSR_athletes[USA_USSR_athletes.Sex=='F'],whatplot= 'number_of_athletes')\n\n['EST', 'EUN', 'ROW', 'USA', 'WES']\n\n\n<AxesSubplot:ylabel='Number of athletes'>\n\n\n\n\n\n\nyrplot(USA_USSR_athletes[USA_USSR_athletes.Sex=='M'],whatplot= 'number_of_athletes')\n\n['EST', 'EUN', 'ROW', 'USA', 'WES']\n\n\n<AxesSubplot:ylabel='Number of athletes'>"
  },
  {
    "objectID": "posts/olympics/2022-07-29-OlympicsSQL_createCountryDF.html#overview",
    "href": "posts/olympics/2022-07-29-OlympicsSQL_createCountryDF.html#overview",
    "title": "Olympics data with SQL and pandas- creating a country table",
    "section": "Overview",
    "text": "Overview\nThe country table needed extra analysis so I seperated it from the rest of the analysis.\nIt also requires importing some new data, which I will add here too\n\nimport pandas as pd\nfrom pandasql import sqldf\nimport matplotlib.pyplot as plt\nimport re \n\n\ndf= pd.read_csv(\"athlete_events.csv\")\ndf2=pd.read_csv(\"noc_regions.csv\")\n\nThe next line is just to add a unique ID for athletes when the data is split up later\n\ndf= df.reset_index()\ndf.rename(columns={'index':'event_athlete_ID','ID':'athlete_ID'},inplace=True)\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      event_athlete_ID\n      athlete_ID\n      Name\n      Sex\n      Age\n      Height\n      Weight\n      Team\n      NOC\n      Games\n      Year\n      Season\n      City\n      Sport\n      Event\n      Medal\n    \n  \n  \n    \n      0\n      0\n      1\n      A Dijiang\n      M\n      24.0\n      180.0\n      80.0\n      China\n      CHN\n      1992 Summer\n      1992\n      Summer\n      Barcelona\n      Basketball\n      Basketball Men's Basketball\n      NaN\n    \n    \n      1\n      1\n      2\n      A Lamusi\n      M\n      23.0\n      170.0\n      60.0\n      China\n      CHN\n      2012 Summer\n      2012\n      Summer\n      London\n      Judo\n      Judo Men's Extra-Lightweight\n      NaN\n    \n    \n      2\n      2\n      3\n      Gunnar Nielsen Aaby\n      M\n      24.0\n      NaN\n      NaN\n      Denmark\n      DEN\n      1920 Summer\n      1920\n      Summer\n      Antwerpen\n      Football\n      Football Men's Football\n      NaN\n    \n    \n      3\n      3\n      4\n      Edgar Lindenau Aabye\n      M\n      34.0\n      NaN\n      NaN\n      Denmark/Sweden\n      DEN\n      1900 Summer\n      1900\n      Summer\n      Paris\n      Tug-Of-War\n      Tug-Of-War Men's Tug-Of-War\n      Gold\n    \n    \n      4\n      4\n      5\n      Christine Jacoba Aaftink\n      F\n      21.0\n      185.0\n      82.0\n      Netherlands\n      NED\n      1988 Winter\n      1988\n      Winter\n      Calgary\n      Speed Skating\n      Speed Skating Women's 500 metres\n      NaN\n    \n  \n\n\n\n\n\ndf2.head()\n\n\n\n\n\n  \n    \n      \n      NOC\n      region\n      notes\n    \n  \n  \n    \n      0\n      AFG\n      Afghanistan\n      NaN\n    \n    \n      1\n      AHO\n      Curacao\n      Netherlands Antilles\n    \n    \n      2\n      ALB\n      Albania\n      NaN\n    \n    \n      3\n      ALG\n      Algeria\n      NaN\n    \n    \n      4\n      AND\n      Andorra\n      NaN\n    \n  \n\n\n\n\n\nWhat do we use to value to identify a nation\n\nprint('There are {} unique teams and {} unique NOCs in df.\\n     \\\nAnd {} unique NOC values, {} unique regions and {} unique notes in df2.'.format( \\\n    len(pd.unique(df.Team)),len(pd.unique(df.NOC)), \n    len(pd.unique(df2.region)),len(pd.unique(df2.region)),len(pd.unique(df2.notes)) ))\n\n\nThere are 1184 unique teams and 230 unique NOCs in df.\n     And 207 unique NOC values, 207 unique regions and 22 unique notes in df2.\n\n\n1184 Seems a lot of teams to consider, it may be best to stick with using NOC as a unique identifier for a country. We can then probably use regions as the name of the country.\nLet’s have a look at the values of teams,NOC and regions\n\nsqldf(\"SELECT                                \\\n         NOC,                                \\\n         Team,                               \\\n         count(*)                            \\\n       FROM                                  \\\n         df                                  \\\n       GROUP BY                              \\\n         Team, NOC                           \\\n       ORDER BY team DESC                    \\\n       LIMIT 30;\",locals())\n\n\n\n\n\n  \n    \n      \n      NOC\n      Team\n      count(*)\n    \n  \n  \n    \n      0\n      FIN\n      rn-2\n      5\n    \n    \n      1\n      BEL\n      Zut\n      3\n    \n    \n      2\n      ZIM\n      Zimbabwe\n      309\n    \n    \n      3\n      GRE\n      Zefyros\n      2\n    \n    \n      4\n      ZAM\n      Zambia\n      183\n    \n    \n      5\n      YUG\n      Yugoslavia-2\n      10\n    \n    \n      6\n      YUG\n      Yugoslavia-1\n      10\n    \n    \n      7\n      YUG\n      Yugoslavia\n      2558\n    \n    \n      8\n      SUI\n      Ylliam VIII\n      5\n    \n    \n      9\n      SUI\n      Ylliam VII\n      6\n    \n    \n      10\n      SUI\n      Ylliam II\n      5\n    \n    \n      11\n      GBR\n      Yeoman XII\n      3\n    \n    \n      12\n      GBR\n      Yeoman VII\n      3\n    \n    \n      13\n      RSA\n      Yeoman V\n      3\n    \n    \n      14\n      BAH\n      Yeoman\n      4\n    \n    \n      15\n      YEM\n      Yemen\n      32\n    \n    \n      16\n      MYA\n      Yangon\n      2\n    \n    \n      17\n      MEX\n      Xolotl\n      3\n    \n    \n      18\n      FIN\n      Xantippa\n      3\n    \n    \n      19\n      GBR\n      Wolseley-Siddeley-1\n      4\n    \n    \n      20\n      CAN\n      Winnipeg Shamrocks-1\n      12\n    \n    \n      21\n      CAN\n      Windor\n      2\n    \n    \n      22\n      NED\n      Willem-Six\n      3\n    \n    \n      23\n      ARG\n      Wiking\n      5\n    \n    \n      24\n      USA\n      Widgeon\n      2\n    \n    \n      25\n      FRA\n      Whitini Star\n      1\n    \n    \n      26\n      DEN\n      White Lady\n      3\n    \n    \n      27\n      JPN\n      Whisper\n      1\n    \n    \n      28\n      CAN\n      Whirlaway\n      2\n    \n    \n      29\n      USA\n      Western Rowing Club-3\n      6\n    \n  \n\n\n\n\nTeam in df seems to not reflect the country very well. e.g. Whisper is not a country but JPN probably represents Japan.\nSo the use of NOC seems to make sense\nNow let us consider the NOC, region and notes variables\n\nsqldf(\"SELECT                                \\\n         NOC,                                \\\n         Region,                             \\\n         Notes,                              \\\n         count(*)                            \\\n       FROM                                  \\\n         df2                                 \\\n       GROUP BY                              \\\n         NOC, Region, Notes                  \\\n       ORDER BY Region DESC                  \\\n       LIMIT 30;\",locals())\n\n\n\n\n\n  \n    \n      \n      NOC\n      region\n      notes\n      count(*)\n    \n  \n  \n    \n      0\n      RHO\n      Zimbabwe\n      None\n      1\n    \n    \n      1\n      ZIM\n      Zimbabwe\n      None\n      1\n    \n    \n      2\n      ZAM\n      Zambia\n      None\n      1\n    \n    \n      3\n      YAR\n      Yemen\n      North Yemen\n      1\n    \n    \n      4\n      YEM\n      Yemen\n      None\n      1\n    \n    \n      5\n      YMD\n      Yemen\n      South Yemen\n      1\n    \n    \n      6\n      ISV\n      Virgin Islands, US\n      Virgin Islands\n      1\n    \n    \n      7\n      IVB\n      Virgin Islands, British\n      None\n      1\n    \n    \n      8\n      VIE\n      Vietnam\n      None\n      1\n    \n    \n      9\n      VNM\n      Vietnam\n      None\n      1\n    \n    \n      10\n      VEN\n      Venezuela\n      None\n      1\n    \n    \n      11\n      VAN\n      Vanuatu\n      None\n      1\n    \n    \n      12\n      UZB\n      Uzbekistan\n      None\n      1\n    \n    \n      13\n      URU\n      Uruguay\n      None\n      1\n    \n    \n      14\n      UAE\n      United Arab Emirates\n      None\n      1\n    \n    \n      15\n      UKR\n      Ukraine\n      None\n      1\n    \n    \n      16\n      UGA\n      Uganda\n      None\n      1\n    \n    \n      17\n      USA\n      USA\n      None\n      1\n    \n    \n      18\n      GBR\n      UK\n      None\n      1\n    \n    \n      19\n      TKM\n      Turkmenistan\n      None\n      1\n    \n    \n      20\n      TUR\n      Turkey\n      None\n      1\n    \n    \n      21\n      TUN\n      Tunisia\n      None\n      1\n    \n    \n      22\n      TTO\n      Trinidad\n      Trinidad and Tobago\n      1\n    \n    \n      23\n      WIF\n      Trinidad\n      West Indies Federation\n      1\n    \n    \n      24\n      TGA\n      Tonga\n      None\n      1\n    \n    \n      25\n      TOG\n      Togo\n      None\n      1\n    \n    \n      26\n      TLS\n      Timor-Leste\n      None\n      1\n    \n    \n      27\n      THA\n      Thailand\n      None\n      1\n    \n    \n      28\n      TAN\n      Tanzania\n      None\n      1\n    \n    \n      29\n      TJK\n      Tajikistan\n      None\n      1\n    \n  \n\n\n\n\nNOC doesn’t look unique enough for us. For example, - Zimbabwe is RHO and ZIM and this probably just reflects a name change in the country - Yemen has 3 NOC values, presumably reflecting the unification that took place in 1990. Most of these differences are not that important to what I am looking at, where it would be better to have a broader description of a nation\nSo what I will do is - use the region tag as a unique identifier of a country - replace multiple NOC values of a single country with a single NOC value\nThe first step is to identify which regions have multiple NOC values. We can do this again in SQL by creating a new table\n\nd1=sqldf(\"SELECT                               \\\n            NOC,                               \\\n            region,                            \\\n            notes,                             \\\n            count(*)                           \\\n         FROM                                  \\\n            df2                                \\\n         GROUP BY                              \\\n            region                             \\\n         HAVING COUNT(*)>1                     \\\n         ORDER BY count(*) DESC;\",locals())\nd1\n\n\n\n\n\n  \n    \n      \n      NOC\n      region\n      notes\n      count(*)\n    \n  \n  \n    \n      0\n      FRG\n      Germany\n      None\n      4\n    \n    \n      1\n      YAR\n      Yemen\n      North Yemen\n      3\n    \n    \n      2\n      SCG\n      Serbia\n      Serbia and Montenegro\n      3\n    \n    \n      3\n      EUN\n      Russia\n      None\n      3\n    \n    \n      4\n      MAL\n      Malaysia\n      None\n      3\n    \n    \n      5\n      BOH\n      Czech Republic\n      Bohemia\n      3\n    \n    \n      6\n      ROT\n      None\n      Refugee Olympic Team\n      3\n    \n    \n      7\n      RHO\n      Zimbabwe\n      None\n      2\n    \n    \n      8\n      VIE\n      Vietnam\n      None\n      2\n    \n    \n      9\n      TTO\n      Trinidad\n      Trinidad and Tobago\n      2\n    \n    \n      10\n      SYR\n      Syria\n      None\n      2\n    \n    \n      11\n      CRT\n      Greece\n      Crete\n      2\n    \n    \n      12\n      CHN\n      China\n      None\n      2\n    \n    \n      13\n      CAN\n      Canada\n      None\n      2\n    \n    \n      14\n      ANZ\n      Australia\n      Australasia\n      2\n    \n  \n\n\n\n\nWe can then create a table to get - the country, - the new NOC value for each country - all the NOC values that correspond to that country\n\nd2=sqldf(\"SELECT                            \\\n            d1.NOC      AS new_NOC,         \\\n            df2.NOC     AS orig_NOC,        \\\n            df2.region,                     \\\n            df2.notes                       \\\n          FROM                              \\\n            d1                              \\\n          LEFT JOIN                         \\\n            df2                             \\\n          ON                                \\\n            d1.region=df2.region            \\\n          ORDER BY                          \\\n            df2.region DESC;\",locals())\nd2\n\n\n\n\n\n  \n    \n      \n      new_NOC\n      orig_NOC\n      region\n      notes\n    \n  \n  \n    \n      0\n      RHO\n      RHO\n      Zimbabwe\n      None\n    \n    \n      1\n      RHO\n      ZIM\n      Zimbabwe\n      None\n    \n    \n      2\n      YAR\n      YAR\n      Yemen\n      North Yemen\n    \n    \n      3\n      YAR\n      YEM\n      Yemen\n      None\n    \n    \n      4\n      YAR\n      YMD\n      Yemen\n      South Yemen\n    \n    \n      5\n      VIE\n      VIE\n      Vietnam\n      None\n    \n    \n      6\n      VIE\n      VNM\n      Vietnam\n      None\n    \n    \n      7\n      TTO\n      TTO\n      Trinidad\n      Trinidad and Tobago\n    \n    \n      8\n      TTO\n      WIF\n      Trinidad\n      West Indies Federation\n    \n    \n      9\n      SYR\n      SYR\n      Syria\n      None\n    \n    \n      10\n      SYR\n      UAR\n      Syria\n      United Arab Republic\n    \n    \n      11\n      SCG\n      SCG\n      Serbia\n      Serbia and Montenegro\n    \n    \n      12\n      SCG\n      SRB\n      Serbia\n      None\n    \n    \n      13\n      SCG\n      YUG\n      Serbia\n      Yugoslavia\n    \n    \n      14\n      EUN\n      EUN\n      Russia\n      None\n    \n    \n      15\n      EUN\n      RUS\n      Russia\n      None\n    \n    \n      16\n      EUN\n      URS\n      Russia\n      None\n    \n    \n      17\n      MAL\n      MAL\n      Malaysia\n      None\n    \n    \n      18\n      MAL\n      MAS\n      Malaysia\n      None\n    \n    \n      19\n      MAL\n      NBO\n      Malaysia\n      North Borneo\n    \n    \n      20\n      CRT\n      CRT\n      Greece\n      Crete\n    \n    \n      21\n      CRT\n      GRE\n      Greece\n      None\n    \n    \n      22\n      FRG\n      FRG\n      Germany\n      None\n    \n    \n      23\n      FRG\n      GDR\n      Germany\n      None\n    \n    \n      24\n      FRG\n      GER\n      Germany\n      None\n    \n    \n      25\n      FRG\n      SAA\n      Germany\n      None\n    \n    \n      26\n      BOH\n      BOH\n      Czech Republic\n      Bohemia\n    \n    \n      27\n      BOH\n      CZE\n      Czech Republic\n      None\n    \n    \n      28\n      BOH\n      TCH\n      Czech Republic\n      None\n    \n    \n      29\n      CHN\n      CHN\n      China\n      None\n    \n    \n      30\n      CHN\n      HKG\n      China\n      Hong Kong\n    \n    \n      31\n      CAN\n      CAN\n      Canada\n      None\n    \n    \n      32\n      CAN\n      NFL\n      Canada\n      Newfoundland\n    \n    \n      33\n      ANZ\n      ANZ\n      Australia\n      Australasia\n    \n    \n      34\n      ANZ\n      AUS\n      Australia\n      None\n    \n    \n      35\n      ROT\n      None\n      None\n      None\n    \n  \n\n\n\n\nAnd finally replace the values of NOC in df and df2 with the new values for countries with duplicate values\nEasier to do this with Python\n\nfor i,old_NOC in enumerate(d2.orig_NOC):\n    df.loc[df.NOC==old_NOC,'NOC']=d2.loc[i,'new_NOC']\n    df2.loc[df2.NOC==old_NOC,'NOC']=d2.loc[i,'new_NOC']\n    \n\n\n\nGDP data\nWikipedia was used to get data on population and GDP of different countries. The data was imported using Excel’s capability to give it a url to obtain the table, and saved as different tabs in the file CountryData.xlsx. For GDP I selected the World Bank Estimate.\n\nGDP data\nPopulation data\n\nWebsite urls correct as of 23/7/22\n\ndf_GDP = pd.read_excel('CountryData.xlsx',sheet_name=2)\n\ndf_GDP=df_GDP.drop(columns=['IMF[1][12] Estimate','IMF[1][12] Year','United Nations[13] Estimate','United Nations[13] Year','World Bank[14][15] Year'])\ndf_GDP=df_GDP.rename(columns={'World Bank[14][15] Estimate':'GDP','Country/Territory':'Country','UN Region':'Continent'},errors='raise')\n\ndf_GDP.head(10)\n\n\n\n\n\n  \n    \n      \n      Country\n      Continent\n      GDP\n    \n  \n  \n    \n      0\n      World\n      -\n      84,705,567\n    \n    \n      1\n      United States\n      Americas\n      20,936,600\n    \n    \n      2\n      China\n      Asia\n      14,722,731\n    \n    \n      3\n      Japan\n      Asia\n      4,975,415\n    \n    \n      4\n      Germany\n      Europe\n      3,806,060\n    \n    \n      5\n      India\n      Asia\n      2,622,984\n    \n    \n      6\n      United Kingdom\n      Europe\n      2,707,744\n    \n    \n      7\n      France\n      Europe\n      2,603,004\n    \n    \n      8\n      Canada\n      Americas\n      1,643,408\n    \n    \n      9\n      Italy\n      Europe\n      1,886,445\n    \n  \n\n\n\n\nmake column GDP an integer, and remove the comma\n\ndf_GDP['GDP']=[x.replace(',','') for x in df_GDP['GDP']]\n\ndf_GDP['GDP']=pd.to_numeric(df_GDP['GDP'],errors='coerce').fillna(0).astype('int')\n\nWe now need to match the names of countries from the data we have found on the internet with the Olympics data.\nA good example of this is United Kingdom, which can be named several ways (with slightly different meanings) including: UK, Great Britain, Great Britain, Great Britain and Northern Ireland, GB.\nThe function changeDF_country takes the imported dataframes and df2 as inputs and outputs the imported dataframes with corrected NOC and country values\n\nChecks whether country in imported df matches region in df\n\n\nIf so country doesn’t need to be changed and can provide matching NOC value\n\n\nIf not above then see if df2.notes matches the country\n\n\nIf so can return country and NOC matching the notes that match\n\n\nIf not aboves, try some name changes, like United States to USA\n\n\nThen return the matching NOC and country and NOC matching values\n\n\nNone of the above\n\n\nThen return country given and give NOC a name to show there was no match.\nPrint out the name of the country where no match was found. To see if missing anything big or obvious\n\n\ndef getNation(region_to_check):\n       \n    if region_to_check=='United States':\n        region_out = 'USA'\n    elif bool(re.search(r'Germany', region_to_check)):\n        region_out='Germany'\n    elif region_to_check=='United Kingdom':\n        region_out = 'UK'\n    elif region_to_check=='Soviet Union':\n        region_out='Russia'\n    else:\n        region_out=region_to_check\n        print('nothing found for {}'.format(region_out))\n    \n    return region_out\n\ndef changeDF_country(df__,df2):\n    xALL,nocALL=[],[]\n    for i in range(len(df__)):\n        country_check = df__.loc[i,'Country']\n#         print(i,country_check)\n        boolCountry=df2.region==country_check\n        x=df2[boolCountry].region\n        \n        try:\n            x=str(x.iloc[0])\n            df2.loc[boolCountry,'region']=x\n        except:\n            if len(x)<1:\n                boolCountry=df2.notes==df__.loc[i,'Country']\n                x=df2[boolCountry].region\n                try:\n                    x=str(x.iloc[0])\n                    df2.loc[boolCountry,'region']=x\n                except:\n                    if len(x)<1:\n                        country_check=getNation(df__.loc[i,'Country'])\n                        boolCountry=df2.region==country_check\n                        x=df2[boolCountry].region\n                        try:\n                            x=str(x.iloc[0])\n                            df2.loc[boolCountry,'region']=x\n                        except:\n                            x=country_check\n        xALL.append(x)\n        try:\n            nocALL.append(df2[boolCountry].NOC.iloc[0])\n        except:\n            nocALL.append('---')\n            \n#         print('x= ',x)\n#         print('-------------------')\n#         if i==42:\n#             break\n#     print(nocALL)\n    try:\n        df__.insert(1,'NOC',nocALL)\n        df__.insert(1,'Nation',xALL)#,'Nation',xALL})\n    except:\n        print('done allready')\n    return df__\n        \n\n\ndf_GDP=changeDF_country(df_GDP,df2)\n\nnothing found for World\nnothing found for DR Congo\nnothing found for Bolivia\nnothing found for Macau\nnothing found for Congo\nnothing found for North Macedonia\nnothing found for New Caledonia\nnothing found for French Polynesia\nnothing found for Eswatini\nnothing found for Greenland\nnothing found for Curaçao\nnothing found for East Timor\nnothing found for Zanzibar\nnothing found for British Virgin Islands\nnothing found for Northern Mariana Islands\nnothing found for Saint Kitts and Nevis\nnothing found for Saint Vincent and the Grenadines\nnothing found for Sint Maarten\nnothing found for São Tomé and Príncipe\nnothing found for Anguilla\nnothing found for Montserrat\n\n\n\n\nPopulation table\nThe same thing as above but for the population data\n\ndf_population = pd.read_excel('CountryData.xlsx',sheet_name=0)\ndf_population.head(5)\n\n\n\n\n\n  \n    \n      \n      Rank\n      Country / Dependency\n      UN Region\n      Population\n      Percentage of the world\n      Date\n      Source (official or from the United Nations)\n      Notes\n      Column9\n    \n  \n  \n    \n      0\n      –\n      World\n      NaN\n      7965207000\n      1.0000\n      2022-07-20\n      UN projection[2]\n      NaN\n      NaN\n    \n    \n      1\n      1\n      China\n      Asia\n      1412600000\n      0.1770\n      2021-12-31\n      National annual estimate\n      The population figure refers to mainland China...\n      NaN\n    \n    \n      2\n      2\n      India\n      Asia\n      1373761000\n      0.1720\n      2022-03-01\n      Annual national estimate\n      The figure includes the population of Indian-a...\n      NaN\n    \n    \n      3\n      3\n      United States\n      Americas\n      332906919\n      0.0418\n      2022-07-20\n      National population clock\n      The figure includes the 50 states and the Dist...\n      NaN\n    \n    \n      4\n      4\n      Indonesia\n      Asia\n      272248500\n      0.0342\n      2021-07-01\n      National annual estimate\n      NaN\n      NaN\n    \n  \n\n\n\n\n\n\ndf_population.drop(columns=['Percentage of the world','Date','Source (official or from the United Nations)','Column9','Notes'],inplace=True)\ndf_population=df_population.rename(columns={'Country / Dependency':'Country','UN Region':'Continent'},errors='raise')\n\ndf_population\n\n\n\n\n\n  \n    \n      \n      Rank\n      Country\n      Continent\n      Population\n    \n  \n  \n    \n      0\n      –\n      World\n      NaN\n      7965207000\n    \n    \n      1\n      1\n      China\n      Asia\n      1412600000\n    \n    \n      2\n      2\n      India\n      Asia\n      1373761000\n    \n    \n      3\n      3\n      United States\n      Americas\n      332906919\n    \n    \n      4\n      4\n      Indonesia\n      Asia\n      272248500\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      237\n      –\n      Niue\n      Oceania\n      1549\n    \n    \n      238\n      –\n      Tokelau (New Zealand)\n      Oceania\n      1501\n    \n    \n      239\n      195\n      Vatican City\n      Europe\n      825\n    \n    \n      240\n      –\n      Cocos (Keeling) Islands (Australia)\n      Oceania\n      573\n    \n    \n      241\n      –\n      Pitcairn Islands (United Kingdom)\n      Oceania\n      40\n    \n  \n\n242 rows × 4 columns\n\n\n\n\ndf_population=changeDF_country(df_population,df2)\n\nnothing found for World\nnothing found for DR Congo\nnothing found for Bolivia\nnothing found for Hong Kong (China)\nnothing found for Congo\nnothing found for Puerto Rico (United States)\nnothing found for North Macedonia\nnothing found for East Timor\nnothing found for Eswatini\nnothing found for Macau (China)\nnothing found for Western Sahara\nnothing found for Northern Cyprus\nnothing found for Transnistria\nnothing found for French Polynesia (France)\nnothing found for New Caledonia (France)\nnothing found for Abkhazia\nnothing found for São Tomé and Príncipe\nnothing found for Guam (United States)\nnothing found for Curaçao (Netherlands)\nnothing found for Artsakh\nnothing found for Aruba (Netherlands)\nnothing found for Saint Vincent and the Grenadines\nnothing found for Jersey (British Crown Dependency)\nnothing found for U.S. Virgin Islands (United States)\nnothing found for Isle of Man (British Crown Dependency)\nnothing found for Cayman Islands (United Kingdom)\nnothing found for Bermuda (United Kingdom)\nnothing found for Guernsey (British Crown Dependency)\nnothing found for Greenland (Denmark)\nnothing found for Saint Kitts and Nevis\nnothing found for Faroe Islands (Denmark)\nnothing found for South Ossetia\nnothing found for American Samoa (United States)\nnothing found for Northern Mariana Islands (United States)\nnothing found for Turks and Caicos Islands (United Kingdom)\nnothing found for Sint Maarten (Netherlands)\nnothing found for Gibraltar (United Kingdom)\nnothing found for Saint Martin (France)\nnothing found for Åland (Finland)\nnothing found for British Virgin Islands (United Kingdom)\nnothing found for Anguilla (United Kingdom)\nnothing found for Wallis and Futuna (France)\nnothing found for Saint Barthélemy (France)\nnothing found for Saint Helena, Ascension and Tristan da Cunha (United Kingdom)\nnothing found for Saint Pierre and Miquelon (France)\nnothing found for Montserrat (United Kingdom)\nnothing found for Falkland Islands (United Kingdom)\nnothing found for Christmas Island (Australia)\nnothing found for Norfolk Island (Australia)\nnothing found for Niue\nnothing found for Tokelau (New Zealand)\nnothing found for Vatican City\nnothing found for Cocos (Keeling) Islands (Australia)\nnothing found for Pitcairn Islands (United Kingdom)\n\n\n\n\nCombine everything for one country table\nNow we merge all data sets together. I’ll use pandas but this is the same as a SQL left outer join. Because we want to keep all the NOC values in df2 we start with that then join each one after\n\ndf_country=[]\ndf_country=df2.merge(df_population,left_on='NOC',right_on='NOC')\n\ndf_country=df_country.merge(df_GDP,left_on='Nation',right_on='Nation')\ndf_country.drop(columns=['Continent_y','Country_x','Country_y','NOC_y','Rank','notes','region'],inplace=True)\ndf_country=df_country.rename(columns={'NOC_x':'NOC','Continent_x':'Continent'},errors='raise')\n\n# We have multiple versions of Nation so we just take the max one\ndf_country = df_country.groupby('NOC').max()\n\n\ndf_country.to_csv('country')\n\ndf.to_csv(\"athlete_events.csv\")\ndf2.to_csv(\"noc_regions.csv\")"
  },
  {
    "objectID": "posts/olympics/2022-07-29-OlympicsSQL_createDFs.html#introduction",
    "href": "posts/olympics/2022-07-29-OlympicsSQL_createDFs.html#introduction",
    "title": "Olympics data with SQL and pandas- create the tables",
    "section": "Introduction",
    "text": "Introduction\nTwo csv files (representing two different tables) were imported to databricks.\nThe main table (athlete_events) consists of 270,000 rows, whereas the unique names in the table are 135,000, or around half the total.\n\nLots of columns and lots that are objects,\n\nso we want to refine this by reducing columns and making it an integer or something smaller than object if possible\n\nThere are some NaN values, particularly for height/weight at earlier games and also for medals\nAn athlete can be represented in several rows if they do multiple events or at different games (e.g. Christine Jacoba Aaftink). So we may want a seperate ID that incorporate the athlete and the event/games that is unique\nThe TEAM, NOC we only want one identifier and a seperate table for countries\n\nThe first step was to split the table up. - First the users are split up based on whether they are male or female and whether they are in the summer or winter games. So split into 4. - Secondly not all data is needed for these athletes table, so instead of 15 columns this is reduced to 9 - Thirdly, the size of these athlete table is reduced by replacing several variables from string to int to reduce the size. Since for example, there is only a limited number of events.\nAn entity relationship diagram (ERD) of the tables described above was developed as shown below.\nThose highlighted in blue and light blue would require additional data, the darkness of blue representing how much new data is needed.\nLucid Chart was used to produce the ERD\n\nN.B. Most of these are steps not really necessary for this dataset, but I wanted to practice SQL (and pandas). If this was a real world problem I would weigh up the benefits of the splitting in terms of my time and computation to see if it was really necessary.\n\nCreating a country table\nIn a separate page (https://thomashsimm.com/sql/pandas/python/olympics/2022/07/29/OlympicsSQL_createCountryDF.html) I show how I created the country table.\nI also made some slight changes to the two main DataFrames df2 and df. Basically just to change the country label and add unique athlete and athlete + event ids\ndf= df.reset_index()\ndf.rename(columns={'index':'event_athlete_ID','ID':'athlete_ID'},inplace=True)\nThe main part is to get rid of some duplicate NOC values, mostly correct but will not work in some regards e.g. China and Hong Kong.\n\n# This gets region (or countries) which are repeated with different NOC values\nd1=sqldf(\"SELECT                              \\\n       NOC, region,notes,count(*)             \\\n       FROM df2                               \\\n       GROUP BY region                        \\\n       HAVING COUNT(*)>1                      \\\n       ORDER BY count(*) DESC;\",locals())\n\n# this then creates a table with one NOC per region and the original NOC values\n# we'll use the new NOC (one per region) a the new index\nd2=sqldf(\"SELECT                              \\\n       d1.NOC as new_NOC,df2.NOC orig_NOC,df2.region,df2.notes             \\\n       FROM d1                                \\\n       LEFT JOIN df2                          \\\n       ON d1.region=df2.region                \\\n      ORDER BY df2.region DESC;\",locals())\n\n\n# then replace the regions with several NOC values with the new one\nfor i,old_NOC in enumerate(d2.orig_NOC):\n    df.loc[df.NOC==old_NOC,'NOC']=d2.loc[i,'new_NOC']\n    df2.loc[df2.NOC==old_NOC,'NOC']=d2.loc[i,'new_NOC']\n\n\ndf= pd.read_csv(\"athlete_events.csv\")\ntry:\n    df.drop(columns='Unnamed: 0',inplace=True)\nexcept:\n    pass\ndf2=pd.read_csv(\"noc_regions.csv\")"
  },
  {
    "objectID": "posts/olympics/2022-07-29-OlympicsSQL_createDFs.html#look-at-the-data",
    "href": "posts/olympics/2022-07-29-OlympicsSQL_createDFs.html#look-at-the-data",
    "title": "Olympics data with SQL and pandas- create the tables",
    "section": "Look at the data",
    "text": "Look at the data\n\n# !pip install pandasql\nimport pandas as pd\nfrom pandasql import sqldf\nimport matplotlib.pyplot as plt\nimport re \n\ndf\n\n\n\n\n\n  \n    \n      \n      event_athlete_ID\n      athlete_ID\n      Name\n      Sex\n      Age\n      Height\n      Weight\n      Team\n      NOC\n      Games\n      Year\n      Season\n      City\n      Sport\n      Event\n      Medal\n    \n  \n  \n    \n      0\n      0\n      1\n      A Dijiang\n      M\n      24.0\n      180.0\n      80.0\n      China\n      CHN\n      1992 Summer\n      1992\n      Summer\n      Barcelona\n      Basketball\n      Basketball Men's Basketball\n      NaN\n    \n    \n      1\n      1\n      2\n      A Lamusi\n      M\n      23.0\n      170.0\n      60.0\n      China\n      CHN\n      2012 Summer\n      2012\n      Summer\n      London\n      Judo\n      Judo Men's Extra-Lightweight\n      NaN\n    \n    \n      2\n      2\n      3\n      Gunnar Nielsen Aaby\n      M\n      24.0\n      NaN\n      NaN\n      Denmark\n      DEN\n      1920 Summer\n      1920\n      Summer\n      Antwerpen\n      Football\n      Football Men's Football\n      NaN\n    \n    \n      3\n      3\n      4\n      Edgar Lindenau Aabye\n      M\n      34.0\n      NaN\n      NaN\n      Denmark/Sweden\n      DEN\n      1900 Summer\n      1900\n      Summer\n      Paris\n      Tug-Of-War\n      Tug-Of-War Men's Tug-Of-War\n      Gold\n    \n    \n      4\n      4\n      5\n      Christine Jacoba Aaftink\n      F\n      21.0\n      185.0\n      82.0\n      Netherlands\n      NED\n      1988 Winter\n      1988\n      Winter\n      Calgary\n      Speed Skating\n      Speed Skating Women's 500 metres\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      271111\n      271111\n      135569\n      Andrzej ya\n      M\n      29.0\n      179.0\n      89.0\n      Poland-1\n      POL\n      1976 Winter\n      1976\n      Winter\n      Innsbruck\n      Luge\n      Luge Mixed (Men)'s Doubles\n      NaN\n    \n    \n      271112\n      271112\n      135570\n      Piotr ya\n      M\n      27.0\n      176.0\n      59.0\n      Poland\n      POL\n      2014 Winter\n      2014\n      Winter\n      Sochi\n      Ski Jumping\n      Ski Jumping Men's Large Hill, Individual\n      NaN\n    \n    \n      271113\n      271113\n      135570\n      Piotr ya\n      M\n      27.0\n      176.0\n      59.0\n      Poland\n      POL\n      2014 Winter\n      2014\n      Winter\n      Sochi\n      Ski Jumping\n      Ski Jumping Men's Large Hill, Team\n      NaN\n    \n    \n      271114\n      271114\n      135571\n      Tomasz Ireneusz ya\n      M\n      30.0\n      185.0\n      96.0\n      Poland\n      POL\n      1998 Winter\n      1998\n      Winter\n      Nagano\n      Bobsleigh\n      Bobsleigh Men's Four\n      NaN\n    \n    \n      271115\n      271115\n      135571\n      Tomasz Ireneusz ya\n      M\n      34.0\n      185.0\n      96.0\n      Poland\n      POL\n      2002 Winter\n      2002\n      Winter\n      Salt Lake City\n      Bobsleigh\n      Bobsleigh Men's Four\n      NaN\n    \n  \n\n271116 rows × 16 columns\n\n\n\n\ndf.dtypes\n\nevent_athlete_ID      int64\nathlete_ID            int64\nName                 object\nSex                  object\nAge                 float64\nHeight              float64\nWeight              float64\nTeam                 object\nNOC                  object\nGames                object\nYear                  int64\nSeason               object\nCity                 object\nSport                object\nEvent                object\nMedal                object\ndtype: object\n\n\n\ndf.describe()\n\n\n\n\n\n  \n    \n      \n      event_athlete_ID\n      athlete_ID\n      Age\n      Height\n      Weight\n      Year\n    \n  \n  \n    \n      count\n      271116.000000\n      271116.000000\n      261642.000000\n      210945.000000\n      208241.000000\n      271116.000000\n    \n    \n      mean\n      135557.500000\n      68248.954396\n      25.556898\n      175.338970\n      70.702393\n      1978.378480\n    \n    \n      std\n      78264.592128\n      39022.286345\n      6.393561\n      10.518462\n      14.348020\n      29.877632\n    \n    \n      min\n      0.000000\n      1.000000\n      10.000000\n      127.000000\n      25.000000\n      1896.000000\n    \n    \n      25%\n      67778.750000\n      34643.000000\n      21.000000\n      168.000000\n      60.000000\n      1960.000000\n    \n    \n      50%\n      135557.500000\n      68205.000000\n      24.000000\n      175.000000\n      70.000000\n      1988.000000\n    \n    \n      75%\n      203336.250000\n      102097.250000\n      28.000000\n      183.000000\n      79.000000\n      2002.000000\n    \n    \n      max\n      271115.000000\n      135571.000000\n      97.000000\n      226.000000\n      214.000000\n      2016.000000"
  },
  {
    "objectID": "posts/olympics/2022-07-29-OlympicsSQL_createDFs.html#create-all_athletes-table",
    "href": "posts/olympics/2022-07-29-OlympicsSQL_createDFs.html#create-all_athletes-table",
    "title": "Olympics data with SQL and pandas- create the tables",
    "section": "Create all_athletes table",
    "text": "Create all_athletes table\nBecause we are splitting the athlete data based on Summer/Winter and Male/Female we need a folder to be able to join or access different parts of the individual athlete tables.\n\ndf= df.reset_index()\ndf.rename(columns={'index':'event_athlete_ID','ID':'athlete_ID'},inplace=True)\ndf.head(10)\n\n\n\n\n\n  \n    \n      \n      event_athlete_ID\n      event_athlete_ID\n      athlete_ID\n      Name\n      Sex\n      Age\n      Height\n      Weight\n      Team\n      NOC\n      Games\n      Year\n      Season\n      City\n      Sport\n      Event\n      Medal\n    \n  \n  \n    \n      0\n      0\n      0\n      1\n      A Dijiang\n      M\n      24.0\n      180.0\n      80.0\n      China\n      CHN\n      1992 Summer\n      1992\n      Summer\n      Barcelona\n      Basketball\n      Basketball Men's Basketball\n      NaN\n    \n    \n      1\n      1\n      1\n      2\n      A Lamusi\n      M\n      23.0\n      170.0\n      60.0\n      China\n      CHN\n      2012 Summer\n      2012\n      Summer\n      London\n      Judo\n      Judo Men's Extra-Lightweight\n      NaN\n    \n    \n      2\n      2\n      2\n      3\n      Gunnar Nielsen Aaby\n      M\n      24.0\n      NaN\n      NaN\n      Denmark\n      DEN\n      1920 Summer\n      1920\n      Summer\n      Antwerpen\n      Football\n      Football Men's Football\n      NaN\n    \n    \n      3\n      3\n      3\n      4\n      Edgar Lindenau Aabye\n      M\n      34.0\n      NaN\n      NaN\n      Denmark/Sweden\n      DEN\n      1900 Summer\n      1900\n      Summer\n      Paris\n      Tug-Of-War\n      Tug-Of-War Men's Tug-Of-War\n      Gold\n    \n    \n      4\n      4\n      4\n      5\n      Christine Jacoba Aaftink\n      F\n      21.0\n      185.0\n      82.0\n      Netherlands\n      NED\n      1988 Winter\n      1988\n      Winter\n      Calgary\n      Speed Skating\n      Speed Skating Women's 500 metres\n      NaN\n    \n    \n      5\n      5\n      5\n      5\n      Christine Jacoba Aaftink\n      F\n      21.0\n      185.0\n      82.0\n      Netherlands\n      NED\n      1988 Winter\n      1988\n      Winter\n      Calgary\n      Speed Skating\n      Speed Skating Women's 1,000 metres\n      NaN\n    \n    \n      6\n      6\n      6\n      5\n      Christine Jacoba Aaftink\n      F\n      25.0\n      185.0\n      82.0\n      Netherlands\n      NED\n      1992 Winter\n      1992\n      Winter\n      Albertville\n      Speed Skating\n      Speed Skating Women's 500 metres\n      NaN\n    \n    \n      7\n      7\n      7\n      5\n      Christine Jacoba Aaftink\n      F\n      25.0\n      185.0\n      82.0\n      Netherlands\n      NED\n      1992 Winter\n      1992\n      Winter\n      Albertville\n      Speed Skating\n      Speed Skating Women's 1,000 metres\n      NaN\n    \n    \n      8\n      8\n      8\n      5\n      Christine Jacoba Aaftink\n      F\n      27.0\n      185.0\n      82.0\n      Netherlands\n      NED\n      1994 Winter\n      1994\n      Winter\n      Lillehammer\n      Speed Skating\n      Speed Skating Women's 500 metres\n      NaN\n    \n    \n      9\n      9\n      9\n      5\n      Christine Jacoba Aaftink\n      F\n      27.0\n      185.0\n      82.0\n      Netherlands\n      NED\n      1994 Winter\n      1994\n      Winter\n      Lillehammer\n      Speed Skating\n      Speed Skating Women's 1,000 metres\n      NaN\n    \n  \n\n\n\n\n\ndf_all_athletes=df[['event_athlete_ID','athlete_ID','Name','Sex','Season']]\ndf_all_athletes                    \n\n\n\n\n\n  \n    \n      \n      event_athlete_ID\n      event_athlete_ID\n      athlete_ID\n      Name\n      Sex\n      Season\n    \n  \n  \n    \n      0\n      0\n      0\n      1\n      A Dijiang\n      M\n      Summer\n    \n    \n      1\n      1\n      1\n      2\n      A Lamusi\n      M\n      Summer\n    \n    \n      2\n      2\n      2\n      3\n      Gunnar Nielsen Aaby\n      M\n      Summer\n    \n    \n      3\n      3\n      3\n      4\n      Edgar Lindenau Aabye\n      M\n      Summer\n    \n    \n      4\n      4\n      4\n      5\n      Christine Jacoba Aaftink\n      F\n      Winter\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      271111\n      271111\n      271111\n      135569\n      Andrzej ya\n      M\n      Winter\n    \n    \n      271112\n      271112\n      271112\n      135570\n      Piotr ya\n      M\n      Winter\n    \n    \n      271113\n      271113\n      271113\n      135570\n      Piotr ya\n      M\n      Winter\n    \n    \n      271114\n      271114\n      271114\n      135571\n      Tomasz Ireneusz ya\n      M\n      Winter\n    \n    \n      271115\n      271115\n      271115\n      135571\n      Tomasz Ireneusz ya\n      M\n      Winter\n    \n  \n\n271116 rows × 6 columns"
  },
  {
    "objectID": "posts/olympics/2022-07-29-OlympicsSQL_createDFs.html#creating-an-events-table",
    "href": "posts/olympics/2022-07-29-OlympicsSQL_createDFs.html#creating-an-events-table",
    "title": "Olympics data with SQL and pandas- create the tables",
    "section": "Creating an Events Table",
    "text": "Creating an Events Table\nIn this table the individual events are displayed. e.g. 100m Mens Sprint Athletics or Womens Football\n\nprint(\"There are {} unique sports and {} unique events \".format(len(pd.unique(df.Sport)), len(pd.unique(df.Event))))\n\nThere are 66 unique sports and 765 unique events \n\n\nBecause of the way that events are named they won’t be duplicated, e.g. 400m breaststroke swimming will be different from 400m athletics running because the name is prefixed with Athletics Women, Swimming Men etc\nInstead of using one hot encoding (get_dummies for pandas as done with medals) we want a different number for each unique event in one column. To do this we can use factorize\naa=pd.factorize(df.ColumnCheck)\nwill give us a variable where - aa[0] is a list of numbers of length of rows in df, where each value represents a different event - aa[1] is then a list of the events of length of the unique events, aa[1][0] is event = 0, aa[1][1] is event = 1 etc - so below aa[1][0] = ‘Basketball Men’s Basketball’ and each row in the df with this event will have a 0 in aa[0]\n\nevent_details=pd.factorize(df.Event)\nevent_details[1][0:100], event_details[0][1:10]\n\n(Index(['Basketball Men's Basketball', 'Judo Men's Extra-Lightweight',\n        'Football Men's Football', 'Tug-Of-War Men's Tug-Of-War',\n        'Speed Skating Women's 500 metres',\n        'Speed Skating Women's 1,000 metres',\n        'Cross Country Skiing Men's 10 kilometres',\n        'Cross Country Skiing Men's 50 kilometres',\n        'Cross Country Skiing Men's 10/15 kilometres Pursuit',\n        'Cross Country Skiing Men's 4 x 10 kilometres Relay',\n        'Cross Country Skiing Men's 30 kilometres',\n        'Athletics Women's 100 metres',\n        'Athletics Women's 4 x 100 metres Relay', 'Ice Hockey Men's Ice Hockey',\n        'Swimming Men's 400 metres Freestyle', 'Badminton Men's Singles',\n        'Sailing Women's Windsurfer', 'Biathlon Women's 7.5 kilometres Sprint',\n        'Swimming Men's 200 metres Breaststroke',\n        'Swimming Men's 400 metres Breaststroke',\n        'Gymnastics Men's Individual All-Around',\n        'Gymnastics Men's Team All-Around', 'Gymnastics Men's Floor Exercise',\n        'Gymnastics Men's Horse Vault', 'Gymnastics Men's Parallel Bars',\n        'Gymnastics Men's Horizontal Bar', 'Gymnastics Men's Rings',\n        'Gymnastics Men's Pommelled Horse', 'Athletics Men's Shot Put',\n        'Art Competitions Mixed Sculpturing, Unknown Event',\n        'Alpine Skiing Men's Downhill', 'Alpine Skiing Men's Super G',\n        'Alpine Skiing Men's Giant Slalom', 'Alpine Skiing Men's Slalom',\n        'Alpine Skiing Men's Combined', 'Handball Women's Handball',\n        'Weightlifting Women's Super-Heavyweight',\n        'Wrestling Men's Light-Heavyweight, Greco-Roman',\n        'Speed Skating Men's 500 metres', 'Speed Skating Men's 1,500 metres',\n        'Gymnastics Men's Team All-Around, Free System', 'Luge Women's Singles',\n        'Water Polo Men's Water Polo', 'Sailing Mixed Three Person Keelboat',\n        'Hockey Women's Hockey', 'Rowing Men's Lightweight Double Sculls',\n        'Athletics Men's Pole Vault', 'Athletics Men's High Jump',\n        'Sailing Men's Two Person Dinghy', 'Athletics Men's 1,500 metres',\n        'Bobsleigh Men's Four', 'Swimming Men's 100 metres Butterfly',\n        'Swimming Men's 200 metres Butterfly',\n        'Swimming Men's 4 x 100 metres Medley Relay',\n        'Football Women's Football', 'Fencing Men's Foil, Individual',\n        'Fencing Men's epee, Individual', 'Fencing Men's epee, Team',\n        'Speed Skating Men's 5,000 metres', 'Speed Skating Men's 10,000 metres',\n        'Sailing Mixed 8 metres', 'Equestrianism Mixed Jumping, Individual',\n        'Cross Country Skiing Men's 15 kilometres',\n        'Shooting Men's Small-Bore Rifle, Prone, 50 metres',\n        'Shooting Men's Rapid-Fire Pistol, 25 metres', 'Shooting Men's Trap',\n        'Athletics Men's 4 x 100 metres Relay', 'Athletics Men's Long Jump',\n        'Boxing Men's Light-Welterweight', 'Athletics Women's Javelin Throw',\n        'Wrestling Men's Heavyweight, Freestyle', 'Taekwondo Men's Flyweight',\n        'Boxing Men's Heavyweight', 'Athletics Men's 5,000 metres',\n        'Cycling Men's Road Race, Individual', 'Cycling Men's Road Race, Team',\n        'Weightlifting Men's Lightweight', 'Weightlifting Men's Middleweight',\n        'Rowing Men's Coxless Pairs', 'Judo Men's Half-Middleweight',\n        'Taekwondo Women's Flyweight', 'Boxing Men's Flyweight',\n        'Basketball Women's Basketball', 'Diving Men's Platform',\n        'Canoeing Men's Canadian Doubles, 500 metres',\n        'Canoeing Men's Canadian Doubles, 1,000 metres',\n        'Canoeing Men's Kayak Fours, 1,000 metres', 'Handball Men's Handball',\n        'Rowing Women's Coxless Pairs', 'Boxing Men's Middleweight',\n        'Judo Men's Lightweight', 'Boxing Men's Featherweight',\n        'Tennis Men's Doubles', 'Shooting Mixed Skeet',\n        'Wrestling Men's Featherweight, Freestyle',\n        'Sailing Mixed Two Person Heavyweight Dinghy',\n        'Athletics Women's Shot Put', 'Rowing Men's Coxed Eights',\n        'Cycling Women's Sprint', 'Cycling Women's 500 metres Time Trial'],\n       dtype='object'),\n array([1, 2, 3, 4, 5, 4, 5, 4, 5], dtype=int64))\n\n\n\ndf.insert(2,'event_id',event_details[0])\ndf.head(10)\n\n\n\n\n\n  \n    \n      \n      event_athlete_ID\n      event_athlete_ID\n      event_id\n      athlete_ID\n      Name\n      Sex\n      Age\n      Height\n      Weight\n      Team\n      NOC\n      Games\n      Year\n      Season\n      City\n      Sport\n      Event\n      Medal\n    \n  \n  \n    \n      0\n      0\n      0\n      0\n      1\n      A Dijiang\n      M\n      24.0\n      180.0\n      80.0\n      China\n      CHN\n      1992 Summer\n      1992\n      Summer\n      Barcelona\n      Basketball\n      Basketball Men's Basketball\n      NaN\n    \n    \n      1\n      1\n      1\n      1\n      2\n      A Lamusi\n      M\n      23.0\n      170.0\n      60.0\n      China\n      CHN\n      2012 Summer\n      2012\n      Summer\n      London\n      Judo\n      Judo Men's Extra-Lightweight\n      NaN\n    \n    \n      2\n      2\n      2\n      2\n      3\n      Gunnar Nielsen Aaby\n      M\n      24.0\n      NaN\n      NaN\n      Denmark\n      DEN\n      1920 Summer\n      1920\n      Summer\n      Antwerpen\n      Football\n      Football Men's Football\n      NaN\n    \n    \n      3\n      3\n      3\n      3\n      4\n      Edgar Lindenau Aabye\n      M\n      34.0\n      NaN\n      NaN\n      Denmark/Sweden\n      DEN\n      1900 Summer\n      1900\n      Summer\n      Paris\n      Tug-Of-War\n      Tug-Of-War Men's Tug-Of-War\n      Gold\n    \n    \n      4\n      4\n      4\n      4\n      5\n      Christine Jacoba Aaftink\n      F\n      21.0\n      185.0\n      82.0\n      Netherlands\n      NED\n      1988 Winter\n      1988\n      Winter\n      Calgary\n      Speed Skating\n      Speed Skating Women's 500 metres\n      NaN\n    \n    \n      5\n      5\n      5\n      5\n      5\n      Christine Jacoba Aaftink\n      F\n      21.0\n      185.0\n      82.0\n      Netherlands\n      NED\n      1988 Winter\n      1988\n      Winter\n      Calgary\n      Speed Skating\n      Speed Skating Women's 1,000 metres\n      NaN\n    \n    \n      6\n      6\n      6\n      4\n      5\n      Christine Jacoba Aaftink\n      F\n      25.0\n      185.0\n      82.0\n      Netherlands\n      NED\n      1992 Winter\n      1992\n      Winter\n      Albertville\n      Speed Skating\n      Speed Skating Women's 500 metres\n      NaN\n    \n    \n      7\n      7\n      7\n      5\n      5\n      Christine Jacoba Aaftink\n      F\n      25.0\n      185.0\n      82.0\n      Netherlands\n      NED\n      1992 Winter\n      1992\n      Winter\n      Albertville\n      Speed Skating\n      Speed Skating Women's 1,000 metres\n      NaN\n    \n    \n      8\n      8\n      8\n      4\n      5\n      Christine Jacoba Aaftink\n      F\n      27.0\n      185.0\n      82.0\n      Netherlands\n      NED\n      1994 Winter\n      1994\n      Winter\n      Lillehammer\n      Speed Skating\n      Speed Skating Women's 500 metres\n      NaN\n    \n    \n      9\n      9\n      9\n      5\n      5\n      Christine Jacoba Aaftink\n      F\n      27.0\n      185.0\n      82.0\n      Netherlands\n      NED\n      1994 Winter\n      1994\n      Winter\n      Lillehammer\n      Speed Skating\n      Speed Skating Women's 1,000 metres\n      NaN\n    \n  \n\n\n\n\n\nevent_details=pd.factorize(df.Event)\ndf_event = pd.DataFrame(event_details[1])\ndf_event\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      0\n      Basketball Men's Basketball\n    \n    \n      1\n      Judo Men's Extra-Lightweight\n    \n    \n      2\n      Football Men's Football\n    \n    \n      3\n      Tug-Of-War Men's Tug-Of-War\n    \n    \n      4\n      Speed Skating Women's 500 metres\n    \n    \n      ...\n      ...\n    \n    \n      760\n      Weightlifting Men's All-Around Dumbbell Contest\n    \n    \n      761\n      Archery Men's Au Chapelet, 33 metres\n    \n    \n      762\n      Archery Men's Au Cordon Dore, 33 metres\n    \n    \n      763\n      Archery Men's Target Archery, 28 metres, Indiv...\n    \n    \n      764\n      Aeronautics Mixed Aeronautics\n    \n  \n\n765 rows × 1 columns\n\n\n\n\n\ndf_event = df[['Sport','Event','Sex','Season']]\n\nevent_details=pd.factorize(df.Sport)\ndf_event.insert(0,'sport_id',event_details[0])\n\nevent_details=pd.factorize(df.Event)\ndf_event.insert(0,'event_id',event_details[0])\n\n\ndf_event\n\n\n\n\n\n  \n    \n      \n      event_id\n      sport_id\n      Sport\n      Event\n      Sex\n      Season\n    \n  \n  \n    \n      0\n      0\n      0\n      Basketball\n      Basketball Men's Basketball\n      M\n      Summer\n    \n    \n      1\n      1\n      1\n      Judo\n      Judo Men's Extra-Lightweight\n      M\n      Summer\n    \n    \n      2\n      2\n      2\n      Football\n      Football Men's Football\n      M\n      Summer\n    \n    \n      3\n      3\n      3\n      Tug-Of-War\n      Tug-Of-War Men's Tug-Of-War\n      M\n      Summer\n    \n    \n      4\n      4\n      4\n      Speed Skating\n      Speed Skating Women's 500 metres\n      F\n      Winter\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      271111\n      461\n      18\n      Luge\n      Luge Mixed (Men)'s Doubles\n      M\n      Winter\n    \n    \n      271112\n      418\n      48\n      Ski Jumping\n      Ski Jumping Men's Large Hill, Individual\n      M\n      Winter\n    \n    \n      271113\n      419\n      48\n      Ski Jumping\n      Ski Jumping Men's Large Hill, Team\n      M\n      Winter\n    \n    \n      271114\n      50\n      22\n      Bobsleigh\n      Bobsleigh Men's Four\n      M\n      Winter\n    \n    \n      271115\n      50\n      22\n      Bobsleigh\n      Bobsleigh Men's Four\n      M\n      Winter\n    \n  \n\n271116 rows × 6 columns\n\n\n\n\ndf_event = df_event.drop_duplicates().reset_index(drop=True)\ndf_event#[df_event.Sex=='F'].head(30)\n\n\n\n\n\n  \n    \n      \n      event_id\n      sport_id\n      Sport\n      Event\n      Sex\n      Season\n    \n  \n  \n    \n      0\n      0\n      0\n      Basketball\n      Basketball Men's Basketball\n      M\n      Summer\n    \n    \n      1\n      1\n      1\n      Judo\n      Judo Men's Extra-Lightweight\n      M\n      Summer\n    \n    \n      2\n      2\n      2\n      Football\n      Football Men's Football\n      M\n      Summer\n    \n    \n      3\n      3\n      3\n      Tug-Of-War\n      Tug-Of-War Men's Tug-Of-War\n      M\n      Summer\n    \n    \n      4\n      4\n      4\n      Speed Skating\n      Speed Skating Women's 500 metres\n      F\n      Winter\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      824\n      221\n      10\n      Sailing\n      Sailing Mixed 7 metres\n      F\n      Summer\n    \n    \n      825\n      333\n      10\n      Sailing\n      Sailing Mixed 6 metres\n      F\n      Summer\n    \n    \n      826\n      764\n      65\n      Aeronautics\n      Aeronautics Mixed Aeronautics\n      M\n      Summer\n    \n    \n      827\n      677\n      13\n      Art Competitions\n      Art Competitions Mixed Sculpturing, Medals And...\n      F\n      Summer\n    \n    \n      828\n      648\n      13\n      Art Competitions\n      Art Competitions Mixed Unknown Event\n      F\n      Summer\n    \n  \n\n829 rows × 6 columns\n\n\n\n\nAn additional columns in event_table\nLets add a column representing if the sport is a team sport or individual. We can’t do this on the unique members of a team in that event because the team can have multiple members in an individual event. Instead, we can look for how many people took the gold medal. Should work for most circumstances as the gold shouldn’t be shared- so if 2 people won gold it should represent a team sport of 2 people.\nThis is a little convoluted so I’ll do it in two steps in SQL. One way to calculate and one to join this new table back in with df_event\n\ndf_event_temp=sqldf(\"SELECT                   \\\n      event_id,event,num_athletes    \\\n      FROM                                    \\\n      (SELECT                                 \\\n          event,                              \\\n          event_id,                           \\\n          COUNT(*)    as num_athletes         \\\n      FROM df                                 \\\n      WHERE Medal='Gold'                      \\\n      GROUP BY Team, event_id, Games          \\\n      ORDER BY event_id                       \\\n      )                                       \\\n      GROUP BY event_id                       \\\n                    ;\",   locals())                               \n\ndf_event_temp\n\n\n\n\n\n  \n    \n      \n      event_id\n      event\n      num_athletes\n    \n  \n  \n    \n      0\n      0\n      Basketball Men's Basketball\n      12\n    \n    \n      1\n      1\n      Judo Men's Extra-Lightweight\n      1\n    \n    \n      2\n      2\n      Football Men's Football\n      16\n    \n    \n      3\n      3\n      Tug-Of-War Men's Tug-Of-War\n      6\n    \n    \n      4\n      4\n      Speed Skating Women's 500 metres\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      745\n      760\n      Weightlifting Men's All-Around Dumbbell Contest\n      1\n    \n    \n      746\n      761\n      Archery Men's Au Chapelet, 33 metres\n      1\n    \n    \n      747\n      762\n      Archery Men's Au Cordon Dore, 33 metres\n      1\n    \n    \n      748\n      763\n      Archery Men's Target Archery, 28 metres, Indiv...\n      1\n    \n    \n      749\n      764\n      Aeronautics Mixed Aeronautics\n      1\n    \n  \n\n750 rows × 3 columns\n\n\n\n\ndf_event_temp      = sqldf(\"SELECT                                \\\n                      d.event_id,                            \\\n                      d.sport_id,                            \\\n                      d.Sport,                               \\\n                      d.Event,                               \\\n                      d.Sex,                                 \\\n                      d.Season,                              \\\n                      t.num_athletes                         \\\n                FROM df_event  as d                          \\\n                LEFT JOIN                                    \\\n                    df_event_temp  as t                      \\\n                ON                                           \\\n                    t.event_id=d.event_id                    \\\n                ORDER BY d.event_id                          \\\n                      ;\",locals())\n\n\n\n# df_event_temp[((df_event_temp.Sex=='F') & (df_event_temp.Season=='Winter'))].head(30)\n\ndf_event=df_event_temp\n\nAnd make the last column int not float (CAST didn’t seem to work perhaps due to NaN values?)\n\ndf_event.fillna(0,inplace=True)\ndf_event = df_event.astype({'num_athletes':'int'})\n\n\n# sanity check\ndf_event[((df_event['num_athletes']==4) & (df_event['Season']=='Winter'))]\n\n\n\n\n\n  \n    \n      \n      event_id\n      sport_id\n      Sport\n      Event\n      Sex\n      Season\n      num_athletes\n    \n  \n  \n    \n      9\n      9\n      5\n      Cross Country Skiing\n      Cross Country Skiing Men's 4 x 10 kilometres R...\n      M\n      Winter\n      4\n    \n    \n      53\n      50\n      22\n      Bobsleigh\n      Bobsleigh Men's Four\n      M\n      Winter\n      4\n    \n    \n      215\n      204\n      40\n      Nordic Combined\n      Nordic Combined Men's Team\n      M\n      Winter\n      4\n    \n    \n      238\n      226\n      11\n      Biathlon\n      Biathlon Men's 4 x 7.5 kilometres Relay\n      M\n      Winter\n      4\n    \n    \n      274\n      259\n      11\n      Biathlon\n      Biathlon Mixed 2 x 6 kilometres and 2 x 7.5 ki...\n      M\n      Winter\n      4\n    \n    \n      275\n      259\n      11\n      Biathlon\n      Biathlon Mixed 2 x 6 kilometres and 2 x 7.5 ki...\n      F\n      Winter\n      4\n    \n    \n      330\n      310\n      11\n      Biathlon\n      Biathlon Women's 4 x 7.5 kilometres Relay\n      F\n      Winter\n      4\n    \n    \n      451\n      419\n      48\n      Ski Jumping\n      Ski Jumping Men's Large Hill, Team\n      M\n      Winter\n      4\n    \n    \n      470\n      435\n      52\n      Short Track Speed Skating\n      Short Track Speed Skating Men's 5,000 metres R...\n      M\n      Winter\n      4\n    \n    \n      478\n      443\n      11\n      Biathlon\n      Biathlon Women's 4 x 6 kilometres Relay\n      F\n      Winter\n      4\n    \n    \n      504\n      466\n      5\n      Cross Country Skiing\n      Cross Country Skiing Women's 4 x 5 kilometres ...\n      F\n      Winter\n      4\n    \n    \n      510\n      472\n      22\n      Bobsleigh\n      Bobsleigh Men's Four/Five\n      M\n      Winter\n      4\n    \n    \n      533\n      492\n      52\n      Short Track Speed Skating\n      Short Track Speed Skating Women's 3,000 metres...\n      F\n      Winter\n      4\n    \n    \n      615\n      572\n      18\n      Luge\n      Luge Mixed Team Relay\n      M\n      Winter\n      4\n    \n    \n      616\n      572\n      18\n      Luge\n      Luge Mixed Team Relay\n      F\n      Winter\n      4\n    \n    \n      640\n      594\n      59\n      Military Ski Patrol\n      Military Ski Patrol Men's Military Ski Patrol\n      M\n      Winter\n      4\n    \n  \n\n\n\n\n\ndf[df.event_id==572][['Sport','Event','Sex']].tail(6),df_event[df_event.event_id==572]\n\n(       Sport                  Event Sex\n 255927  Luge  Luge Mixed Team Relay   M\n 258882  Luge  Luge Mixed Team Relay   M\n 262369  Luge  Luge Mixed Team Relay   F\n 267369  Luge  Luge Mixed Team Relay   M\n 268477  Luge  Luge Mixed Team Relay   M\n 270261  Luge  Luge Mixed Team Relay   M,\n      event_id  sport_id Sport                  Event Sex  Season  num_athletes\n 615       572        18  Luge  Luge Mixed Team Relay   M  Winter             4\n 616       572        18  Luge  Luge Mixed Team Relay   F  Winter             4)"
  },
  {
    "objectID": "posts/olympics/2022-07-29-OlympicsSQL_createDFs.html#add-a-games-table",
    "href": "posts/olympics/2022-07-29-OlympicsSQL_createDFs.html#add-a-games-table",
    "title": "Olympics data with SQL and pandas- create the tables",
    "section": "Add a Games table",
    "text": "Add a Games table\nThe games table is used to give information about a particular Olympic games including - Where it was staged - Is it a summer or winter games - The year it was staged\nUsing the same methodology as before we want to - create a unique id for the games - replace this in the athlete tables - add a new table with the unique id and additional information about the particular games\nThis is more complex than the events table because we want to add additional data about the cities where the games were held. This data is obtained from wikipedia as before. So some of the methodology used in creating the country table is used here.\nThere’s a strange thing that there are two summer games in one year\n\ndf=df.sort_values(by=['Year','City'])\n\n\n# event_details=pd.factorize(pd.lib.fast_zip([df.Games, df.City]))\ntuples = df[['Games', 'City']].apply(tuple, axis=1)\nevent_details = pd.factorize( tuples )\nevent_details[1][0:10], event_details[0][1:10],len(event_details[1]),len(event_details[0])\n\n(Index([   ('1896 Summer', 'Athina'),     ('1900 Summer', 'Paris'),\n        ('1904 Summer', 'St. Louis'),    ('1906 Summer', 'Athina'),\n           ('1908 Summer', 'London'), ('1912 Summer', 'Stockholm'),\n        ('1920 Summer', 'Antwerpen'),  ('1924 Winter', 'Chamonix'),\n            ('1924 Summer', 'Paris'), ('1928 Summer', 'Amsterdam')],\n       dtype='object'),\n array([0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64),\n 52,\n 271116)\n\n\n\ndf.insert(3,'games_id',event_details[0])\ndf\n\n\n\n\n\n  \n    \n      \n      event_athlete_ID\n      event_athlete_ID\n      event_id\n      games_id\n      athlete_ID\n      Name\n      Sex\n      Age\n      Height\n      Weight\n      Team\n      NOC\n      Games\n      Year\n      Season\n      City\n      Sport\n      Event\n      Medal\n    \n  \n  \n    \n      3079\n      3079\n      3079\n      191\n      0\n      1724\n      Aristidis Akratopoulos\n      M\n      NaN\n      NaN\n      NaN\n      Greece\n      CRT\n      1896 Summer\n      1896\n      Summer\n      Athina\n      Tennis\n      Tennis Men's Singles\n      NaN\n    \n    \n      3080\n      3080\n      3080\n      92\n      0\n      1724\n      Aristidis Akratopoulos\n      M\n      NaN\n      NaN\n      NaN\n      Greece-3\n      CRT\n      1896 Summer\n      1896\n      Summer\n      Athina\n      Tennis\n      Tennis Men's Doubles\n      NaN\n    \n    \n      3081\n      3081\n      3081\n      191\n      0\n      1725\n      Konstantinos \"Kostas\" Akratopoulos\n      M\n      NaN\n      NaN\n      NaN\n      Greece\n      CRT\n      1896 Summer\n      1896\n      Summer\n      Athina\n      Tennis\n      Tennis Men's Singles\n      NaN\n    \n    \n      3082\n      3082\n      3082\n      92\n      0\n      1725\n      Konstantinos \"Kostas\" Akratopoulos\n      M\n      NaN\n      NaN\n      NaN\n      Greece-3\n      CRT\n      1896 Summer\n      1896\n      Summer\n      Athina\n      Tennis\n      Tennis Men's Doubles\n      NaN\n    \n    \n      7348\n      7348\n      7348\n      100\n      0\n      4113\n      Anastasios Andreou\n      M\n      NaN\n      NaN\n      NaN\n      Greece\n      CRT\n      1896 Summer\n      1896\n      Summer\n      Athina\n      Athletics\n      Athletics Men's 110 metres Hurdles\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      271024\n      271024\n      271024\n      15\n      51\n      135528\n      Marc Zwiebler\n      M\n      32.0\n      181.0\n      75.0\n      Germany\n      FRG\n      2016 Summer\n      2016\n      Summer\n      Rio de Janeiro\n      Badminton\n      Badminton Men's Singles\n      NaN\n    \n    \n      271053\n      271053\n      271053\n      11\n      51\n      135547\n      Viktoriya Viktorovna Zyabkina\n      F\n      23.0\n      174.0\n      62.0\n      Kazakhstan\n      KAZ\n      2016 Summer\n      2016\n      Summer\n      Rio de Janeiro\n      Athletics\n      Athletics Women's 100 metres\n      NaN\n    \n    \n      271054\n      271054\n      271054\n      174\n      51\n      135547\n      Viktoriya Viktorovna Zyabkina\n      F\n      23.0\n      174.0\n      62.0\n      Kazakhstan\n      KAZ\n      2016 Summer\n      2016\n      Summer\n      Rio de Janeiro\n      Athletics\n      Athletics Women's 200 metres\n      NaN\n    \n    \n      271055\n      271055\n      271055\n      12\n      51\n      135547\n      Viktoriya Viktorovna Zyabkina\n      F\n      23.0\n      174.0\n      62.0\n      Kazakhstan\n      KAZ\n      2016 Summer\n      2016\n      Summer\n      Rio de Janeiro\n      Athletics\n      Athletics Women's 4 x 100 metres Relay\n      NaN\n    \n    \n      271110\n      271110\n      271110\n      82\n      51\n      135568\n      Olga Igorevna Zyuzkova\n      F\n      33.0\n      171.0\n      69.0\n      Belarus\n      BLR\n      2016 Summer\n      2016\n      Summer\n      Rio de Janeiro\n      Basketball\n      Basketball Women's Basketball\n      NaN\n    \n  \n\n271116 rows × 19 columns\n\n\n\n\ndf[['games_id','Games','City']].groupby(['games_id','Games']).count()\n\n\n\n\n\n  \n    \n      \n      \n      City\n    \n    \n      games_id\n      Games\n      \n    \n  \n  \n    \n      0\n      1896 Summer\n      380\n    \n    \n      1\n      1900 Summer\n      1936\n    \n    \n      2\n      1904 Summer\n      1301\n    \n    \n      3\n      1906 Summer\n      1733\n    \n    \n      4\n      1908 Summer\n      3101\n    \n    \n      5\n      1912 Summer\n      4040\n    \n    \n      6\n      1920 Summer\n      4292\n    \n    \n      7\n      1924 Winter\n      460\n    \n    \n      8\n      1924 Summer\n      5233\n    \n    \n      9\n      1928 Summer\n      4992\n    \n    \n      10\n      1928 Winter\n      582\n    \n    \n      11\n      1932 Winter\n      352\n    \n    \n      12\n      1932 Summer\n      2969\n    \n    \n      13\n      1936 Summer\n      6506\n    \n    \n      14\n      1936 Winter\n      895\n    \n    \n      15\n      1948 Summer\n      6405\n    \n    \n      16\n      1948 Winter\n      1075\n    \n    \n      17\n      1952 Summer\n      8270\n    \n    \n      18\n      1952 Winter\n      1088\n    \n    \n      19\n      1956 Winter\n      1307\n    \n    \n      20\n      1956 Summer\n      4829\n    \n    \n      21\n      1956 Summer\n      298\n    \n    \n      22\n      1960 Summer\n      8119\n    \n    \n      23\n      1960 Winter\n      1116\n    \n    \n      24\n      1964 Winter\n      1778\n    \n    \n      25\n      1964 Summer\n      7702\n    \n    \n      26\n      1968 Winter\n      1891\n    \n    \n      27\n      1968 Summer\n      8588\n    \n    \n      28\n      1972 Summer\n      10304\n    \n    \n      29\n      1972 Winter\n      1655\n    \n    \n      30\n      1976 Winter\n      1861\n    \n    \n      31\n      1976 Summer\n      8641\n    \n    \n      32\n      1980 Winter\n      1746\n    \n    \n      33\n      1980 Summer\n      7191\n    \n    \n      34\n      1984 Summer\n      9454\n    \n    \n      35\n      1984 Winter\n      2134\n    \n    \n      36\n      1988 Winter\n      2639\n    \n    \n      37\n      1988 Summer\n      12037\n    \n    \n      38\n      1992 Winter\n      3436\n    \n    \n      39\n      1992 Summer\n      12977\n    \n    \n      40\n      1994 Winter\n      3160\n    \n    \n      41\n      1996 Summer\n      13780\n    \n    \n      42\n      1998 Winter\n      3605\n    \n    \n      43\n      2000 Summer\n      13821\n    \n    \n      44\n      2002 Winter\n      4109\n    \n    \n      45\n      2004 Summer\n      13443\n    \n    \n      46\n      2006 Winter\n      4382\n    \n    \n      47\n      2008 Summer\n      13602\n    \n    \n      48\n      2010 Winter\n      4402\n    \n    \n      49\n      2012 Summer\n      12920\n    \n    \n      50\n      2014 Winter\n      4891\n    \n    \n      51\n      2016 Summer\n      13688\n    \n  \n\n\n\n\n\n# Load the games table \ndf_games = pd.read_excel('CitiesOlympics.xlsx',sheet_name=0)\n\n# then sort by year and city like did with df, reset the index\ndf_games=df_games.sort_values(by=['Year','City']).reset_index(drop=True)\n# and replace games_id with new ordered index\ndf_games['games_id']=df_games.index\n\n## sanity check to see if the two tables for games match\n# sanity\ndtemp=df[['games_id','Games','City','Year']].groupby(['games_id','Games','City']).count()\ndtemp.reset_index(inplace=True)\n\npd.concat([df_games[['City','Year','Summer']], dtemp ], axis=1)\n\n# \n\n\n\n\n\n  \n    \n      \n      City\n      Year\n      Summer\n      games_id\n      Games\n      City\n      Year\n    \n  \n  \n    \n      0\n      Athens\n      1896\n      1\n      0\n      1896 Summer\n      Athina\n      380\n    \n    \n      1\n      Paris\n      1900\n      1\n      1\n      1900 Summer\n      Paris\n      1936\n    \n    \n      2\n      St. Louis\n      1904\n      1\n      2\n      1904 Summer\n      St. Louis\n      1301\n    \n    \n      3\n      Athens\n      1906\n      1\n      3\n      1906 Summer\n      Athina\n      1733\n    \n    \n      4\n      London\n      1908\n      1\n      4\n      1908 Summer\n      London\n      3101\n    \n    \n      5\n      Stockholm\n      1912\n      1\n      5\n      1912 Summer\n      Stockholm\n      4040\n    \n    \n      6\n      Antwerp\n      1920\n      1\n      6\n      1920 Summer\n      Antwerpen\n      4292\n    \n    \n      7\n      Chamonix\n      1924\n      0\n      7\n      1924 Winter\n      Chamonix\n      460\n    \n    \n      8\n      Paris\n      1924\n      1\n      8\n      1924 Summer\n      Paris\n      5233\n    \n    \n      9\n      Amsterdam\n      1928\n      1\n      9\n      1928 Summer\n      Amsterdam\n      4992\n    \n    \n      10\n      St. Moritz\n      1928\n      0\n      10\n      1928 Winter\n      Sankt Moritz\n      582\n    \n    \n      11\n      Lake Placid\n      1932\n      0\n      11\n      1932 Winter\n      Lake Placid\n      352\n    \n    \n      12\n      Los Angeles\n      1932\n      1\n      12\n      1932 Summer\n      Los Angeles\n      2969\n    \n    \n      13\n      Berlin\n      1936\n      1\n      13\n      1936 Summer\n      Berlin\n      6506\n    \n    \n      14\n      Garmisch-Partenkirchen\n      1936\n      0\n      14\n      1936 Winter\n      Garmisch-Partenkirchen\n      895\n    \n    \n      15\n      London\n      1948\n      1\n      15\n      1948 Summer\n      London\n      6405\n    \n    \n      16\n      St. Moritz\n      1948\n      0\n      16\n      1948 Winter\n      Sankt Moritz\n      1075\n    \n    \n      17\n      Helsinki\n      1952\n      1\n      17\n      1952 Summer\n      Helsinki\n      8270\n    \n    \n      18\n      Oslo\n      1952\n      0\n      18\n      1952 Winter\n      Oslo\n      1088\n    \n    \n      19\n      Cortina d'Ampezzo\n      1956\n      0\n      19\n      1956 Winter\n      Cortina d'Ampezzo\n      1307\n    \n    \n      20\n      Melbourne\n      1956\n      1\n      20\n      1956 Summer\n      Melbourne\n      4829\n    \n    \n      21\n      Stockholm\n      1956\n      0\n      21\n      1956 Summer\n      Stockholm\n      298\n    \n    \n      22\n      Rome\n      1960\n      1\n      22\n      1960 Summer\n      Roma\n      8119\n    \n    \n      23\n      Squaw Valley\n      1960\n      0\n      23\n      1960 Winter\n      Squaw Valley\n      1116\n    \n    \n      24\n      Innsbruck\n      1964\n      0\n      24\n      1964 Winter\n      Innsbruck\n      1778\n    \n    \n      25\n      Tokyo\n      1964\n      1\n      25\n      1964 Summer\n      Tokyo\n      7702\n    \n    \n      26\n      Grenoble\n      1968\n      0\n      26\n      1968 Winter\n      Grenoble\n      1891\n    \n    \n      27\n      Mexico City\n      1968\n      1\n      27\n      1968 Summer\n      Mexico City\n      8588\n    \n    \n      28\n      Munich\n      1972\n      1\n      28\n      1972 Summer\n      Munich\n      10304\n    \n    \n      29\n      Sapporo\n      1972\n      0\n      29\n      1972 Winter\n      Sapporo\n      1655\n    \n    \n      30\n      Innsbruck\n      1976\n      0\n      30\n      1976 Winter\n      Innsbruck\n      1861\n    \n    \n      31\n      Montreal\n      1976\n      1\n      31\n      1976 Summer\n      Montreal\n      8641\n    \n    \n      32\n      Lake Placid\n      1980\n      0\n      32\n      1980 Winter\n      Lake Placid\n      1746\n    \n    \n      33\n      Moscow\n      1980\n      1\n      33\n      1980 Summer\n      Moskva\n      7191\n    \n    \n      34\n      Los Angeles\n      1984\n      1\n      34\n      1984 Summer\n      Los Angeles\n      9454\n    \n    \n      35\n      Sarajevo\n      1984\n      0\n      35\n      1984 Winter\n      Sarajevo\n      2134\n    \n    \n      36\n      Calgary\n      1988\n      0\n      36\n      1988 Winter\n      Calgary\n      2639\n    \n    \n      37\n      Seoul\n      1988\n      1\n      37\n      1988 Summer\n      Seoul\n      12037\n    \n    \n      38\n      Albertville\n      1992\n      0\n      38\n      1992 Winter\n      Albertville\n      3436\n    \n    \n      39\n      Barcelona\n      1992\n      1\n      39\n      1992 Summer\n      Barcelona\n      12977\n    \n    \n      40\n      Lillehammer\n      1994\n      0\n      40\n      1994 Winter\n      Lillehammer\n      3160\n    \n    \n      41\n      Atlanta\n      1996\n      1\n      41\n      1996 Summer\n      Atlanta\n      13780\n    \n    \n      42\n      Nagano\n      1998\n      0\n      42\n      1998 Winter\n      Nagano\n      3605\n    \n    \n      43\n      Sydney\n      2000\n      1\n      43\n      2000 Summer\n      Sydney\n      13821\n    \n    \n      44\n      Salt Lake City\n      2002\n      0\n      44\n      2002 Winter\n      Salt Lake City\n      4109\n    \n    \n      45\n      Athens\n      2004\n      1\n      45\n      2004 Summer\n      Athina\n      13443\n    \n    \n      46\n      Turin\n      2006\n      0\n      46\n      2006 Winter\n      Torino\n      4382\n    \n    \n      47\n      Beijing\n      2008\n      1\n      47\n      2008 Summer\n      Beijing\n      13602\n    \n    \n      48\n      Vancouver\n      2010\n      0\n      48\n      2010 Winter\n      Vancouver\n      4402\n    \n    \n      49\n      London\n      2012\n      1\n      49\n      2012 Summer\n      London\n      12920\n    \n    \n      50\n      Sochi\n      2014\n      0\n      50\n      2014 Winter\n      Sochi\n      4891\n    \n    \n      51\n      Rio de Janeiro\n      2016\n      1\n      51\n      2016 Summer\n      Rio de Janeiro\n      13688\n    \n  \n\n\n\n\n\nimport re\n\n# starts with a digit (at start of string) or '.'- goes on for undefinable length  \nregex_pattern=r'^[\\d|.]*'\n# starts with a comma then spaces then digits or '.'\nregex_pattern2=',\\s*[\\d|.|]*'\n\ntest_string = df_games.iloc[1,-2]\nprint(test_string)\na=re.search(regex_pattern2, test_string)\n\nfor i in range(len(df_games)):\n#     print(df_games.iloc[i,-2])\n    try:\n        df_games.iloc[i,-1]= re.search(regex_pattern2,df_games.iloc[i,-2])[0][2:] \n        df_games.iloc[i,-2]= re.search(regex_pattern,df_games.iloc[i,-2])[0] \n    except:\n        pass\n\n48.8566° N, 2.3522°\n\n\n\ndef getNation(region_to_check):\n    import re    \n    if region_to_check=='United States':\n        region_out = 'USA'\n    elif bool(re.search(r'Germany', region_to_check)):\n        region_out='Germany'\n    elif region_to_check=='United Kingdom':\n        region_out = 'UK'\n    elif region_to_check=='Soviet Union':\n        region_out='Russia'\n    else:\n        region_out=region_to_check\n        print('nothing found for {}'.format(region_out))\n    \n    return region_out\n\nfor i in range(len(df_games)):\n    x=df2[df2.region==df_games.iloc[i,2]].region\n    if len(x)<1:\n        x=df2[df2.notes==df_games.iloc[i,2]].region\n        if len(x)<1:\n            x=getNation(df_games.iloc[i,2])\n#     \n    try:\n        df_games.iloc[i,2]=str(x.iloc[0]) \n    except:\n        df_games.iloc[i,2]=str(x) \n#     print(x)\n#     if len(x)<1:\n#         print('----------------------------------')\n        \n        \n\n\ndf_games.iloc[10:20,:]\n\n\n\n\n\n  \n    \n      \n      games_id\n      City\n      Country\n      Year\n      Region\n      Summer\n      Winter\n      Latitude\n      Longitude\n    \n  \n  \n    \n      10\n      10\n      St. Moritz\n      Switzerland\n      1928\n      Europe\n      0\n      1\n      46.4908\n      9.8355\n    \n    \n      11\n      11\n      Lake Placid\n      USA\n      1932\n      North America\n      0\n      1\n      27.2931\n      81.3629\n    \n    \n      12\n      12\n      Los Angeles\n      USA\n      1932\n      North America\n      1\n      0\n      34.0522\n      118.2437\n    \n    \n      13\n      13\n      Berlin\n      Germany\n      1936\n      Europe\n      1\n      0\n      52.5200\n      13.4050\n    \n    \n      14\n      14\n      Garmisch-Partenkirchen\n      Germany\n      1936\n      Europe\n      0\n      1\n      47.4919\n      11.0948\n    \n    \n      15\n      15\n      London\n      UK\n      1948\n      Europe\n      1\n      0\n      51.5072\n      0.1276\n    \n    \n      16\n      16\n      St. Moritz\n      Switzerland\n      1948\n      Europe\n      0\n      1\n      46.4908\n      9.8355\n    \n    \n      17\n      17\n      Helsinki\n      Finland\n      1952\n      Europe\n      1\n      0\n      60.1699\n      24.9384\n    \n    \n      18\n      18\n      Oslo\n      Norway\n      1952\n      Europe\n      0\n      1\n      59.9139\n      10.7522\n    \n    \n      19\n      19\n      Cortina d'Ampezzo\n      Italy\n      1956\n      Europe\n      0\n      1\n      46.5405\n      12.1357\n    \n  \n\n\n\n\n\ndf.groupby(['games_id','Year','Games','City']).count().reset_index()\n\n\n\n\n\n  \n    \n      \n      games_id\n      Year\n      Games\n      City\n      event_athlete_ID\n      event_athlete_ID\n      event_id\n      athlete_ID\n      Name\n      Sex\n      Age\n      Height\n      Weight\n      Team\n      NOC\n      Season\n      Sport\n      Event\n      Medal\n    \n  \n  \n    \n      0\n      0\n      1896\n      1896 Summer\n      Athina\n      380\n      380\n      380\n      380\n      380\n      380\n      217\n      46\n      49\n      380\n      380\n      380\n      380\n      380\n      143\n    \n    \n      1\n      1\n      1900\n      1900 Summer\n      Paris\n      1936\n      1936\n      1936\n      1936\n      1936\n      1936\n      1146\n      116\n      79\n      1936\n      1936\n      1936\n      1936\n      1936\n      604\n    \n    \n      2\n      2\n      1904\n      1904 Summer\n      St. Louis\n      1301\n      1301\n      1301\n      1301\n      1301\n      1301\n      1027\n      213\n      147\n      1301\n      1301\n      1301\n      1301\n      1301\n      486\n    \n    \n      3\n      3\n      1906\n      1906 Summer\n      Athina\n      1733\n      1733\n      1733\n      1733\n      1733\n      1733\n      990\n      257\n      205\n      1733\n      1733\n      1733\n      1733\n      1733\n      458\n    \n    \n      4\n      4\n      1908\n      1908 Summer\n      London\n      3101\n      3101\n      3101\n      3101\n      3101\n      3101\n      2452\n      475\n      483\n      3101\n      3101\n      3101\n      3101\n      3101\n      831\n    \n    \n      5\n      5\n      1912\n      1912 Summer\n      Stockholm\n      4040\n      4040\n      4040\n      4040\n      4040\n      4040\n      3884\n      721\n      596\n      4040\n      4040\n      4040\n      4040\n      4040\n      941\n    \n    \n      6\n      6\n      1920\n      1920 Summer\n      Antwerpen\n      4292\n      4292\n      4292\n      4292\n      4292\n      4292\n      3447\n      767\n      471\n      4292\n      4292\n      4292\n      4292\n      4292\n      1308\n    \n    \n      7\n      7\n      1924\n      1924 Winter\n      Chamonix\n      460\n      460\n      460\n      460\n      460\n      460\n      403\n      89\n      41\n      460\n      460\n      460\n      460\n      460\n      130\n    \n    \n      8\n      8\n      1924\n      1924 Summer\n      Paris\n      5233\n      5233\n      5233\n      5233\n      5233\n      5233\n      4148\n      885\n      649\n      5233\n      5233\n      5233\n      5233\n      5233\n      832\n    \n    \n      9\n      9\n      1928\n      1928 Summer\n      Amsterdam\n      4992\n      4992\n      4992\n      4992\n      4992\n      4992\n      4119\n      853\n      670\n      4992\n      4992\n      4992\n      4992\n      4992\n      734\n    \n    \n      10\n      10\n      1928\n      1928 Winter\n      Sankt Moritz\n      582\n      582\n      582\n      582\n      582\n      582\n      492\n      122\n      48\n      582\n      582\n      582\n      582\n      582\n      89\n    \n    \n      11\n      11\n      1932\n      1932 Winter\n      Lake Placid\n      352\n      352\n      352\n      352\n      352\n      352\n      329\n      196\n      55\n      352\n      352\n      352\n      352\n      352\n      92\n    \n    \n      12\n      12\n      1932\n      1932 Summer\n      Los Angeles\n      2969\n      2969\n      2969\n      2969\n      2969\n      2969\n      2662\n      1017\n      495\n      2969\n      2969\n      2969\n      2969\n      2969\n      647\n    \n    \n      13\n      13\n      1936\n      1936 Summer\n      Berlin\n      6506\n      6506\n      6506\n      6506\n      6506\n      6506\n      6304\n      1056\n      909\n      6506\n      6506\n      6506\n      6506\n      6506\n      917\n    \n    \n      14\n      14\n      1936\n      1936 Winter\n      Garmisch-Partenkirchen\n      895\n      895\n      895\n      895\n      895\n      895\n      884\n      136\n      78\n      895\n      895\n      895\n      895\n      895\n      108\n    \n    \n      15\n      15\n      1948\n      1948 Summer\n      London\n      6405\n      6405\n      6405\n      6405\n      6405\n      6405\n      5233\n      1053\n      1040\n      6405\n      6405\n      6405\n      6405\n      6405\n      852\n    \n    \n      16\n      16\n      1948\n      1948 Winter\n      Sankt Moritz\n      1075\n      1075\n      1075\n      1075\n      1075\n      1075\n      1071\n      116\n      111\n      1075\n      1075\n      1075\n      1075\n      1075\n      135\n    \n    \n      17\n      17\n      1952\n      1952 Summer\n      Helsinki\n      8270\n      8270\n      8270\n      8270\n      8270\n      8270\n      7993\n      2038\n      2038\n      8270\n      8270\n      8270\n      8270\n      8270\n      897\n    \n    \n      18\n      18\n      1952\n      1952 Winter\n      Oslo\n      1088\n      1088\n      1088\n      1088\n      1088\n      1088\n      1088\n      150\n      149\n      1088\n      1088\n      1088\n      1088\n      1088\n      136\n    \n    \n      19\n      19\n      1956\n      1956 Winter\n      Cortina d'Ampezzo\n      1307\n      1307\n      1307\n      1307\n      1307\n      1307\n      1282\n      344\n      342\n      1307\n      1307\n      1307\n      1307\n      1307\n      150\n    \n    \n      20\n      20\n      1956\n      1956 Summer\n      Melbourne\n      4829\n      4829\n      4829\n      4829\n      4829\n      4829\n      4256\n      2234\n      2232\n      4829\n      4829\n      4829\n      4829\n      4829\n      857\n    \n    \n      21\n      21\n      1956\n      1956 Summer\n      Stockholm\n      298\n      298\n      298\n      298\n      298\n      298\n      258\n      108\n      106\n      298\n      298\n      298\n      298\n      298\n      36\n    \n    \n      22\n      22\n      1960\n      1960 Summer\n      Roma\n      8119\n      8119\n      8119\n      8119\n      8119\n      8119\n      7906\n      7738\n      7675\n      8119\n      8119\n      8119\n      8119\n      8119\n      911\n    \n    \n      23\n      23\n      1960\n      1960 Winter\n      Squaw Valley\n      1116\n      1116\n      1116\n      1116\n      1116\n      1116\n      1108\n      536\n      512\n      1116\n      1116\n      1116\n      1116\n      1116\n      147\n    \n    \n      24\n      24\n      1964\n      1964 Winter\n      Innsbruck\n      1778\n      1778\n      1778\n      1778\n      1778\n      1778\n      1765\n      1369\n      1348\n      1778\n      1778\n      1778\n      1778\n      1778\n      186\n    \n    \n      25\n      25\n      1964\n      1964 Summer\n      Tokyo\n      7702\n      7702\n      7702\n      7702\n      7702\n      7702\n      7659\n      7430\n      7424\n      7702\n      7702\n      7702\n      7702\n      7702\n      1029\n    \n    \n      26\n      26\n      1968\n      1968 Winter\n      Grenoble\n      1891\n      1891\n      1891\n      1891\n      1891\n      1891\n      1872\n      1833\n      1817\n      1891\n      1891\n      1891\n      1891\n      1891\n      199\n    \n    \n      27\n      27\n      1968\n      1968 Summer\n      Mexico City\n      8588\n      8588\n      8588\n      8588\n      8588\n      8588\n      8489\n      8493\n      8493\n      8588\n      8588\n      8588\n      8588\n      8588\n      1057\n    \n    \n      28\n      28\n      1972\n      1972 Summer\n      Munich\n      10304\n      10304\n      10304\n      10304\n      10304\n      10304\n      10211\n      10018\n      9928\n      10304\n      10304\n      10304\n      10304\n      10304\n      1215\n    \n    \n      29\n      29\n      1972\n      1972 Winter\n      Sapporo\n      1655\n      1655\n      1655\n      1655\n      1655\n      1655\n      1652\n      1640\n      1642\n      1655\n      1655\n      1655\n      1655\n      1655\n      199\n    \n    \n      30\n      30\n      1976\n      1976 Winter\n      Innsbruck\n      1861\n      1861\n      1861\n      1861\n      1861\n      1861\n      1850\n      1343\n      1302\n      1861\n      1861\n      1861\n      1861\n      1861\n      211\n    \n    \n      31\n      31\n      1976\n      1976 Summer\n      Montreal\n      8641\n      8641\n      8641\n      8641\n      8641\n      8641\n      8600\n      8283\n      8280\n      8641\n      8641\n      8641\n      8641\n      8641\n      1320\n    \n    \n      32\n      32\n      1980\n      1980 Winter\n      Lake Placid\n      1746\n      1746\n      1746\n      1746\n      1746\n      1746\n      1745\n      1388\n      1374\n      1746\n      1746\n      1746\n      1746\n      1746\n      218\n    \n    \n      33\n      33\n      1980\n      1980 Summer\n      Moskva\n      7191\n      7191\n      7191\n      7191\n      7191\n      7191\n      7005\n      6961\n      6967\n      7191\n      7191\n      7191\n      7191\n      7191\n      1384\n    \n    \n      34\n      34\n      1984\n      1984 Summer\n      Los Angeles\n      9454\n      9454\n      9454\n      9454\n      9454\n      9454\n      9249\n      9032\n      9031\n      9454\n      9454\n      9454\n      9454\n      9454\n      1476\n    \n    \n      35\n      35\n      1984\n      1984 Winter\n      Sarajevo\n      2134\n      2134\n      2134\n      2134\n      2134\n      2134\n      2123\n      1958\n      1954\n      2134\n      2134\n      2134\n      2134\n      2134\n      222\n    \n    \n      36\n      36\n      1988\n      1988 Winter\n      Calgary\n      2639\n      2639\n      2639\n      2639\n      2639\n      2639\n      2635\n      2024\n      2018\n      2639\n      2639\n      2639\n      2639\n      2639\n      263\n    \n    \n      37\n      37\n      1988\n      1988 Summer\n      Seoul\n      12037\n      12037\n      12037\n      12037\n      12037\n      12037\n      11931\n      11719\n      11730\n      12037\n      12037\n      12037\n      12037\n      12037\n      1582\n    \n    \n      38\n      38\n      1992\n      1992 Winter\n      Albertville\n      3436\n      3436\n      3436\n      3436\n      3436\n      3436\n      3435\n      2785\n      2783\n      3436\n      3436\n      3436\n      3436\n      3436\n      318\n    \n    \n      39\n      39\n      1992\n      1992 Summer\n      Barcelona\n      12977\n      12977\n      12977\n      12977\n      12977\n      12977\n      12934\n      10453\n      10473\n      12977\n      12977\n      12977\n      12977\n      12977\n      1712\n    \n    \n      40\n      40\n      1994\n      1994 Winter\n      Lillehammer\n      3160\n      3160\n      3160\n      3160\n      3160\n      3160\n      3158\n      2973\n      2971\n      3160\n      3160\n      3160\n      3160\n      3160\n      331\n    \n    \n      41\n      41\n      1996\n      1996 Summer\n      Atlanta\n      13780\n      13780\n      13780\n      13780\n      13780\n      13780\n      13772\n      11909\n      11959\n      13780\n      13780\n      13780\n      13780\n      13780\n      1842\n    \n    \n      42\n      42\n      1998\n      1998 Winter\n      Nagano\n      3605\n      3605\n      3605\n      3605\n      3605\n      3605\n      3603\n      3521\n      3519\n      3605\n      3605\n      3605\n      3605\n      3605\n      440\n    \n    \n      43\n      43\n      2000\n      2000 Summer\n      Sydney\n      13821\n      13821\n      13821\n      13821\n      13821\n      13821\n      13820\n      13698\n      13695\n      13821\n      13821\n      13821\n      13821\n      13821\n      2004\n    \n    \n      44\n      44\n      2002\n      2002 Winter\n      Salt Lake City\n      4109\n      4109\n      4109\n      4109\n      4109\n      4109\n      4109\n      4080\n      4062\n      4109\n      4109\n      4109\n      4109\n      4109\n      478\n    \n    \n      45\n      45\n      2004\n      2004 Summer\n      Athina\n      13443\n      13443\n      13443\n      13443\n      13443\n      13443\n      13443\n      13407\n      13406\n      13443\n      13443\n      13443\n      13443\n      13443\n      2001\n    \n    \n      46\n      46\n      2006\n      2006 Winter\n      Torino\n      4382\n      4382\n      4382\n      4382\n      4382\n      4382\n      4382\n      4376\n      4366\n      4382\n      4382\n      4382\n      4382\n      4382\n      526\n    \n    \n      47\n      47\n      2008\n      2008 Summer\n      Beijing\n      13602\n      13602\n      13602\n      13602\n      13602\n      13602\n      13600\n      13451\n      13443\n      13602\n      13602\n      13602\n      13602\n      13602\n      2048\n    \n    \n      48\n      48\n      2010\n      2010 Winter\n      Vancouver\n      4402\n      4402\n      4402\n      4402\n      4402\n      4402\n      4402\n      4400\n      4378\n      4402\n      4402\n      4402\n      4402\n      4402\n      520\n    \n    \n      49\n      49\n      2012\n      2012 Summer\n      London\n      12920\n      12920\n      12920\n      12920\n      12920\n      12920\n      12920\n      12752\n      12560\n      12920\n      12920\n      12920\n      12920\n      12920\n      1941\n    \n    \n      50\n      50\n      2014\n      2014 Winter\n      Sochi\n      4891\n      4891\n      4891\n      4891\n      4891\n      4891\n      4891\n      4871\n      4673\n      4891\n      4891\n      4891\n      4891\n      4891\n      597\n    \n    \n      51\n      51\n      2016\n      2016 Summer\n      Rio de Janeiro\n      13688\n      13688\n      13688\n      13688\n      13688\n      13688\n      13688\n      13512\n      13465\n      13688\n      13688\n      13688\n      13688\n      13688\n      2023"
  },
  {
    "objectID": "posts/olympics/2022-07-29-OlympicsSQL_createDFs.html#create-some-smaller-athlete-tables",
    "href": "posts/olympics/2022-07-29-OlympicsSQL_createDFs.html#create-some-smaller-athlete-tables",
    "title": "Olympics data with SQL and pandas- create the tables",
    "section": "Create some smaller athlete tables",
    "text": "Create some smaller athlete tables\nFinally the main athletes df file can be split the up by sex and season, since the id’s for different fields have been added\n\nWe don’t really need sex and Season so we can drop these\nWe can also reorder the index (not the athlete ID)\nIf Medals = NaN this probably means they didn’t get one. So we can replace medals with Gold, Silver and Bronze (one-hot encoding)\nInstead of Games, Year, City lets replace with a uniques INT id. And put that data in another dataframe for cities\n\n\n# split and Reset index, so it increases 1,2,3,4,etc\n\ndf_M_S = df[(df.Sex=='M') & (df.Season=='Summer')]\ndf_M_S.reset_index(inplace = True,drop=True)\n\ndf_F_S = df[(df.Sex=='F') & (df.Season=='Summer')]\ndf_F_S.reset_index(inplace = True,drop=True)\n\ndf_M_W = df[(df.Sex=='M') & (df.Season=='Winter')]\ndf_M_W.reset_index(inplace = True,drop=True)\n\ndf_F_W = df[(df.Sex=='F') & (df.Season=='Winter')]\ndf_F_W.reset_index(inplace = True,drop=True)\n\n\n# Look at the data\n\ndf_M_W.head(10)\n\n\n\n\n\n  \n    \n      \n      event_athlete_ID\n      event_athlete_ID\n      event_id\n      games_id\n      athlete_ID\n      Name\n      Sex\n      Age\n      Height\n      Weight\n      Team\n      NOC\n      Games\n      Year\n      Season\n      City\n      Sport\n      Event\n      Medal\n    \n  \n  \n    \n      0\n      672\n      672\n      13\n      7\n      391\n      Clarence John Abel\n      M\n      23.0\n      185.0\n      102.0\n      United States\n      USA\n      1924 Winter\n      1924\n      Winter\n      Chamonix\n      Ice Hockey\n      Ice Hockey Men's Ice Hockey\n      Silver\n    \n    \n      1\n      1791\n      1791\n      205\n      7\n      992\n      Josef Adolf\n      M\n      25.0\n      NaN\n      NaN\n      Czechoslovakia\n      BOH\n      1924 Winter\n      1924\n      Winter\n      Chamonix\n      Nordic Combined\n      Nordic Combined Men's Individual\n      NaN\n    \n    \n      2\n      1951\n      1951\n      300\n      7\n      1077\n      Xavier Affentranger\n      M\n      26.0\n      NaN\n      NaN\n      Switzerland\n      SUI\n      1924 Winter\n      1924\n      Winter\n      Chamonix\n      Ski Jumping\n      Ski Jumping Men's Normal Hill, Individual\n      NaN\n    \n    \n      3\n      1952\n      1952\n      385\n      7\n      1077\n      Xavier Affentranger\n      M\n      26.0\n      NaN\n      NaN\n      Switzerland\n      SUI\n      1924 Winter\n      1924\n      Winter\n      Chamonix\n      Cross Country Skiing\n      Cross Country Skiing Men's 18 kilometres\n      NaN\n    \n    \n      4\n      1953\n      1953\n      205\n      7\n      1077\n      Xavier Affentranger\n      M\n      26.0\n      NaN\n      NaN\n      Switzerland\n      SUI\n      1924 Winter\n      1924\n      Winter\n      Chamonix\n      Nordic Combined\n      Nordic Combined Men's Individual\n      NaN\n    \n    \n      5\n      2397\n      2397\n      305\n      7\n      1341\n      Johan Petter hln (Andersson-)\n      M\n      44.0\n      NaN\n      NaN\n      Sweden\n      SWE\n      1924 Winter\n      1924\n      Winter\n      Chamonix\n      Curling\n      Curling Men's Curling\n      Silver\n    \n    \n      6\n      4040\n      4040\n      300\n      7\n      2329\n      Louis Albert\n      M\n      25.0\n      NaN\n      NaN\n      France\n      FRA\n      1924 Winter\n      1924\n      Winter\n      Chamonix\n      Ski Jumping\n      Ski Jumping Men's Normal Hill, Individual\n      NaN\n    \n    \n      7\n      4249\n      4249\n      472\n      7\n      2431\n      Henri Eugne Aldebert\n      M\n      43.0\n      NaN\n      NaN\n      France-1\n      FRA\n      1924 Winter\n      1924\n      Winter\n      Chamonix\n      Bobsleigh\n      Bobsleigh Men's Four/Five\n      NaN\n    \n    \n      8\n      5103\n      5103\n      13\n      7\n      2902\n      Karl Ruben Allinger\n      M\n      32.0\n      NaN\n      NaN\n      Sweden\n      SWE\n      1924 Winter\n      1924\n      Winter\n      Chamonix\n      Ice Hockey\n      Ice Hockey Men's Ice Hockey\n      NaN\n    \n    \n      9\n      5174\n      5174\n      7\n      7\n      2939\n      Ernst Alm\n      M\n      23.0\n      174.0\n      NaN\n      Sweden\n      SWE\n      1924 Winter\n      1924\n      Winter\n      Chamonix\n      Cross Country Skiing\n      Cross Country Skiing Men's 50 kilometres\n      NaN\n    \n  \n\n\n\n\n\n\n\n\ndf_M_S = df_M_S.drop(columns=['Name','Sex','Season','City','Games','Sport','Event'])\ndf_F_S = df_F_S.drop(columns=['Name','Sex','Season','City','Games','Sport','Event'])\ndf_M_W = df_M_W.drop(columns=['Name','Sex','Season','City','Games','Sport','Event'])\ndf_F_W = df_F_W.drop(columns=['Name','Sex','Season','City','Games','Sport','Event'])\ndf_F_W.head(5)\n\n\n\n\n\n  \n    \n      \n      event_athlete_ID\n      event_athlete_ID\n      event_id\n      games_id\n      athlete_ID\n      Age\n      Height\n      Weight\n      Team\n      NOC\n      Year\n      Medal\n    \n  \n  \n    \n      0\n      30620\n      30620\n      268\n      7\n      15776\n      22.0\n      165.0\n      NaN\n      France\n      FRA\n      1924\n      NaN\n    \n    \n      1\n      30621\n      30621\n      242\n      7\n      15776\n      22.0\n      165.0\n      NaN\n      France-1\n      FRA\n      1924\n      Bronze\n    \n    \n      2\n      63714\n      63714\n      242\n      7\n      32641\n      25.0\n      NaN\n      NaN\n      Austria\n      AUT\n      1924\n      Gold\n    \n    \n      3\n      94058\n      94058\n      268\n      7\n      47618\n      11.0\n      155.0\n      45.0\n      Norway\n      NOR\n      1924\n      NaN\n    \n    \n      4\n      94533\n      94533\n      242\n      7\n      47845\n      NaN\n      NaN\n      NaN\n      Belgium\n      BEL\n      1924\n      NaN\n    \n  \n\n\n\n\n\nSeperate out medals\n\ndf_F_W = pd.get_dummies(df_F_W,columns=['Medal'])\ndf_M_W = pd.get_dummies(df_M_W,columns=['Medal'])\ndf_F_S = pd.get_dummies(df_F_S,columns=['Medal'])\ndf_M_S = pd.get_dummies(df_M_S,columns=['Medal'])\ndf_M_S.head()\n\n\n\n\n\n  \n    \n      \n      event_athlete_ID\n      event_athlete_ID\n      event_id\n      games_id\n      athlete_ID\n      Age\n      Height\n      Weight\n      Team\n      NOC\n      Year\n      Medal_Bronze\n      Medal_Gold\n      Medal_Silver\n    \n  \n  \n    \n      0\n      3079\n      3079\n      191\n      0\n      1724\n      NaN\n      NaN\n      NaN\n      Greece\n      CRT\n      1896\n      0\n      0\n      0\n    \n    \n      1\n      3080\n      3080\n      92\n      0\n      1724\n      NaN\n      NaN\n      NaN\n      Greece-3\n      CRT\n      1896\n      0\n      0\n      0\n    \n    \n      2\n      3081\n      3081\n      191\n      0\n      1725\n      NaN\n      NaN\n      NaN\n      Greece\n      CRT\n      1896\n      0\n      0\n      0\n    \n    \n      3\n      3082\n      3082\n      92\n      0\n      1725\n      NaN\n      NaN\n      NaN\n      Greece-3\n      CRT\n      1896\n      0\n      0\n      0\n    \n    \n      4\n      7348\n      7348\n      100\n      0\n      4113\n      NaN\n      NaN\n      NaN\n      Greece\n      CRT\n      1896\n      0\n      0\n      0"
  },
  {
    "objectID": "posts/olympics/2022-07-29-OlympicsSQL_createDFs.html#save-it",
    "href": "posts/olympics/2022-07-29-OlympicsSQL_createDFs.html#save-it",
    "title": "Olympics data with SQL and pandas- create the tables",
    "section": "Save it",
    "text": "Save it\n\ndf_F_S.to_csv('athlete_F_S')\ndf_F_W.to_csv('athlete_F_W')\ndf_M_S.to_csv('athlete_M_S')\ndf_M_W.to_csv('athlete_M_W')\n\ndf_all_athletes.to_csv('all_athletes')\ndf_country.to_csv('country')\ndf_event.to_csv('event')\ndf_games.to_csv('games')\ndf_population.to_csv('population')"
  },
  {
    "objectID": "posts/olympics/2022-07-29-OlympicsSQL_GDPpopulation.html#introduction",
    "href": "posts/olympics/2022-07-29-OlympicsSQL_GDPpopulation.html#introduction",
    "title": "Olympics data with SQL and pandas- GDP and population",
    "section": "Introduction",
    "text": "Introduction\nDue to the global importance of the Olympics, in 2020 there was a broadcast audience of more than 3 billion, I was interested to explore whether countries with the most medals will reflect global politics. And to see if the countries with most influence get more medals.\nThere are two relatively easy to obtain metrics that can be used to define the importance of a nation internationally, GDP and population.\nGross domestic product (GDP) is a monetary measure of the market value of all the final goods and services produced in a specific time period by countries. GDP is often used as a metric for international comparisons as well as a broad measure of economic progress. It is often considered to be the “world’s most powerful statistical indicator of national development and progress”. GDP Wikipedia.\nThe population of a country is an important parameter in assessing the global importance of a nation. Furthermore, the more people in a country the greater the pool of potential athletes.\nIn the final part of this page I will briefly look at how the Cold War was reflected in the Olympics data."
  },
  {
    "objectID": "posts/olympics/2022-07-29-OlympicsSQL_GDPpopulation.html#pre-analysis",
    "href": "posts/olympics/2022-07-29-OlympicsSQL_GDPpopulation.html#pre-analysis",
    "title": "Olympics data with SQL and pandas- GDP and population",
    "section": "Pre-analysis",
    "text": "Pre-analysis\n\nGet medals won by country and year\nWhat we want to end up with is a new table with- - Nation - Number of medals - Event or year of the event - Then the details of nations we’re studying: - GDP - Population - Continent\nTo get a unique medal count for this, we can do a count by grouping on - Nation and Year But also on: - Medal type (Gold, Silver, Bronze) - Event id As we only know if a medal is unique for an athlete if the same medal type does not exist for another athlete in the same event at the same games (year). This is to avoid duplication for team sports. If it is a mixed event then we would need to do the grouping on data of male and female athletes.\nFrom inside out\n\nUse UNION to join the males and female summer athletes\n\nGet NOC, medal types, year, event_id\n\nUse GROUP BY to get unique medals and COUNT\nUse GROUP BY to get medals for countries in each event/year, SUM so we are adding up events\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pandasql import sqldf\nimport copy\nimport numpy as np\nimport scipy.stats\n\n\ndf_F_S =pd.read_csv('data/athlete_F_S')\n# df_F_W=pd.read_csv('data/athlete_F_W')\ndf_M_S=pd.read_csv('data/athlete_M_S')\n# df_M_W=pd.read_csv('data/athlete_M_W')\n\ndf_all_athletes= pd.read_csv('data/all_athletes')\ndf_country= pd.read_csv('data/country')\ndf_event= pd.read_csv('data/event')\n# df_games= pd.read_csv('data/games')\n# df_population= pd.read_csv('data/population')\n\ndf_country = df_country.groupby('NOC').max()\ndf_country.head(10)\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      Nation\n      Continent\n      Population\n      GDP\n    \n    \n      NOC\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      AFG\n      0\n      Afghanistan\n      Asia\n      32890171\n      19807\n    \n    \n      ALB\n      1\n      Albania\n      Europe\n      2829741\n      14800\n    \n    \n      ALG\n      2\n      Algeria\n      Africa\n      45400000\n      145164\n    \n    \n      AND\n      3\n      Andorra\n      Europe\n      79535\n      3155\n    \n    \n      ANG\n      4\n      Angola\n      Africa\n      33086278\n      62307\n    \n    \n      ANT\n      5\n      Antigua\n      Americas\n      99337\n      1415\n    \n    \n      ANZ\n      7\n      Australia\n      Oceania\n      25921518\n      1330901\n    \n    \n      ARG\n      8\n      Argentina\n      Americas\n      47327407\n      383067\n    \n    \n      ARM\n      9\n      Armenia\n      Asia\n      2963900\n      12645\n    \n    \n      AUT\n      10\n      Austria\n      Europe\n      9027999\n      428965\n    \n  \n\n\n\n\n\ndf_by_year=\\\n     sqldf('SELECT                                                 \\\n              NOC,                                                 \\\n              Year,                                                \\\n              SUM(counta)              AS num_medals               \\\n                FROM(SELECT                                        \\\n                    NOC,                                           \\\n                    Year,                                          \\\n                    event_id,                                      \\\n                    Medal_Bronze, Medal_Gold, Medal_Silver,        \\\n                    COUNT(*) as counta                             \\\n                        FROM(SELECT                                \\\n                            NOC,                                   \\\n                            Year,                                  \\\n                            event_id,                              \\\n                            Medal_Bronze, Medal_Gold, Medal_Silver \\\n                        FROM                                       \\\n                            df_F_S                                 \\\n                        WHERE Medal_Bronze=1 OR Medal_Gold=1 OR Medal_Silver \\\n                        UNION                                      \\\n                        SELECT                                     \\\n                            NOC,                                   \\\n                            Year,                                  \\\n                            event_id,                              \\\n                            Medal_Bronze, Medal_Gold, Medal_Silver \\\n                        FROM                                       \\\n                            df_M_S                                 \\\n                        WHERE Medal_Bronze=1 OR Medal_Gold=1 OR Medal_Silver) as MF                  \\\n                    GROUP BY                                       \\\n                        NOC, Year, event_id, Medal_Bronze, Medal_Gold, Medal_Silver\\\n                    ORDER BY                                       \\\n                        NOC) as inner                              \\\n            GROUP BY                                               \\\n                NOC, Year;',locals()) \n\ndf_by_year[df_by_year.Year==2016].sort_values('num_medals',ascending=False).head(10)\n\n\n\n\n\n  \n    \n      \n      NOC\n      Year\n      num_medals\n    \n  \n  \n    \n      1246\n      USA\n      2016\n      121\n    \n    \n      246\n      CHN\n      2016\n      70\n    \n    \n      527\n      GBR\n      2016\n      67\n    \n    \n      415\n      EUN\n      2016\n      56\n    \n    \n      471\n      FRA\n      2016\n      42\n    \n    \n      497\n      FRG\n      2016\n      42\n    \n    \n      706\n      JPN\n      2016\n      41\n    \n    \n      38\n      ANZ\n      2016\n      29\n    \n    \n      669\n      ITA\n      2016\n      28\n    \n    \n      230\n      CAN\n      2016\n      22\n    \n  \n\n\n\n\n\n\nJoin with the country table\nNow just join to the country table\n\ndf_=\\\n     sqldf('SELECT                              \\\n              NOC,                              \\\n              Nation,                           \\\n              Continent,                        \\\n              SUM(num_medals)  AS number_of_medals,                  \\\n              Population,                       \\\n              GDP                               \\\n            FROM(                               \\\n                SELECT *                        \\\n                FROM                            \\\n                    df_country AS c             \\\n                INNER JOIN                      \\\n                    df_by_year AS d             \\\n                ON                              \\\n                    c.NOC=d.NOC                 \\\n                WHERE                           \\\n                    d.Year>=2008)               \\\n            GROUP BY                            \\\n                NOC,Continent                   \\\n            ORDER BY                            \\\n                num_medals desc;',locals())\n               \ndf_                    \n\n\n\n\n\n  \n    \n      \n      NOC\n      Nation\n      Continent\n      number_of_medals\n      Population\n      GDP\n    \n  \n  \n    \n      0\n      USA\n      USA\n      Americas\n      334\n      332906919\n      20936600\n    \n    \n      1\n      CHN\n      China\n      Asia\n      259\n      1412600000\n      14722731\n    \n    \n      2\n      EUN\n      Russia\n      Europe\n      210\n      147190000\n      1483498\n    \n    \n      3\n      GBR\n      UK\n      Europe\n      180\n      67081234\n      2707744\n    \n    \n      4\n      ANZ\n      Australia\n      Oceania\n      110\n      25921518\n      1330901\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      99\n      TUN\n      Tunisia\n      Africa\n      7\n      11746695\n      39236\n    \n    \n      100\n      UAE\n      United Arab Emirates\n      Asia\n      1\n      9282410\n      421142\n    \n    \n      101\n      UGA\n      Uganda\n      Africa\n      1\n      42885900\n      37372\n    \n    \n      102\n      VEN\n      Venezuela\n      Americas\n      5\n      28705000\n      482359\n    \n    \n      103\n      VIE\n      Vietnam\n      Asia\n      3\n      98505400\n      271158\n    \n  \n\n104 rows × 6 columns"
  },
  {
    "objectID": "posts/olympics/2022-07-29-OlympicsSQL_GDPpopulation.html#correlation-in-population-and-gdp",
    "href": "posts/olympics/2022-07-29-OlympicsSQL_GDPpopulation.html#correlation-in-population-and-gdp",
    "title": "Olympics data with SQL and pandas- GDP and population",
    "section": "Correlation in Population and GDP",
    "text": "Correlation in Population and GDP\nFirst create a function to display correlations: - produce scatter plots - find which countries are poor correlation - find some correlation stats\n\ndef modname(string):\n    string=''.join([string[0].upper(),string[1:].lower()])\n    string=string.replace('_',' ')\n    return string\ndef doFigAddOns(nomX,nomY,fontsize):\n        plt.ylabel(modname(nomY), fontsize=fontsize)\n        plt.xlabel(nomX, fontsize=fontsize)\n        plt.grid(True)\n        \ndef scatter_combo(df_temp,xval,yval):\n    \n    df_temp=df_temp.set_index('Nation')\n    fontsize=14\n    \n    df_temp=copy.copy(df_temp)\n    # remove 0 values\n    df_temp=df_temp[df_temp.loc[:,xval]!=0]\n    \n    # drop NaNs \n    df_temp=df_temp.dropna()\n    \n    # values in millions\n    df_temp.loc[:,xval]=df_temp.loc[:,xval]/1e6\n    \n    # the names of the columns\n    nomX=xval#(df_temp.columns[xval])\n    nomY=yval#modname(df_temp.columns[yval])\n    \n    # define the x and y values\n    X=df_temp.loc[:,xval]\n    Y=df_temp.loc[:,yval]\n    \n    # do some plots\n    fig,ax=plt.subplots(figsize=(7,15))\n    plt.rcParams['font.size'] = '13'\n    \n    # Fig A\n    ax1=plt.subplot(3,1,1)\n    ax1.plot(X ,Y,'+b',markeredgewidth=2,ms=10)\n    m,b=np.polyfit(X,Y,1)\n    Ypred=X*m+b\n    plt.plot(X,Ypred, '--k');\n    doFigAddOns(nomX,nomY,fontsize)\n    \n    \n    \n    # Fig B\n    ax3=plt.subplot(3,1,2)\n    ax3.plot(X ,Y,'+b',markeredgewidth=2,ms=10)\n    X=X.sort_values()\n    Ypred=X*m+b\n    plt.plot(X,Ypred, '--k');\n    plt.yscale('log')\n    plt.xscale('log')\n    doFigAddOns(nomX,nomY,fontsize) \n\n    # Fig C\n    diffa = df_temp.loc[:,yval]-Ypred\n    ax2=plt.subplot(3,1,3)\n    ax2.plot(X,diffa, 'xk',markeredgewidth=2,ms=10);\n    plt.xscale('log')\n    doFigAddOns(nomX,'Difference between \\nlinear fit',fontsize)\n\n    plt.subplots_adjust(hspace=0.35)\n    \n    # because sorted x, need to get original x back\n    X=df_temp.loc[:,xval]\n\n    # Get some correlation values, and what are worst countries\n    corr=scipy.stats.pearsonr(X, Y)\n    print('Correlation of  = {:.2f} ({:.0e}) \\n \\\n           And m= {:.3f} b={:.1f}          '.format(corr[0],corr[1],m,b))\n    \n    country_hi_corr=(diffa).sort_values(ascending=False).index[0:10]\n    print('Countries worst correlation where overachive',diffa[country_hi_corr])\n    \n    country_low_corr=(diffa).sort_values().index[0:10]\n    print('Countries worst correlation where unachieve',diffa[country_low_corr])\n\n    d={'Underachieve':country_low_corr,'Overachieve':country_hi_corr}\n    return pd.DataFrame(data=d)\n\ndef getCorr(df_temp, xval,yval):\n    \n    df_temp=copy.copy(df_temp)\n    # remove 0 values\n    df_temp=df_temp[df_temp.loc[:,xval]!=0]\n    \n    # drop NaNs \n    df_temp=df_temp.dropna()\n    \n    # define the x and y values\n    X=df_temp.loc[:,xval]\n    Y=df_temp.loc[:,yval]\n    \n    # Get some correlation values, and what are worst countries\n    corr=scipy.stats.pearsonr(X, Y)\n    print('Pearson Correlation of  = {:.2f} ({:.0e})'.format(corr[0],corr[1]))\n    # Get some correlation values, and what are worst countries\n    corr=scipy.stats.spearmanr(X, Y)\n    print('Spearman Correlation of  = {:.2f} ({:.0e})'.format(corr[0],corr[1]))\n\n\npopCorr=scatter_combo(df_,'GDP','number_of_medals')\npopCorr\n\nCorrelation of  = 0.84 (4e-29) \n            And m= 17.195 b=14.2          \nCountries worst correlation where overachive Nation\nRussia         170.270535\nUK             119.219670\nAustralia       72.894435\nFrance          59.020671\nGermany         47.334168\nUkraine         41.104002\nSouth Korea     37.742411\nItaly           36.341876\nCuba            33.005895\nKazakhstan      26.858922\ndtype: float64\nCountries worst correlation where unachieve Nation\nIndia                  -48.322884\nUSA                    -40.224834\nSaudi Arabia           -25.259275\nIndonesia              -22.420333\nUnited Arab Emirates   -20.462293\nPhilippines            -19.436562\nIsrael                 -18.132356\nMexico                 -17.725355\nAustria                -17.596809\nChile                  -17.570066\ndtype: float64\n\n\n\n\n\n\n  \n    \n      \n      Underachieve\n      Overachieve\n    \n  \n  \n    \n      0\n      India\n      Russia\n    \n    \n      1\n      USA\n      UK\n    \n    \n      2\n      Saudi Arabia\n      Australia\n    \n    \n      3\n      Indonesia\n      France\n    \n    \n      4\n      United Arab Emirates\n      Germany\n    \n    \n      5\n      Philippines\n      Ukraine\n    \n    \n      6\n      Israel\n      South Korea\n    \n    \n      7\n      Mexico\n      Italy\n    \n    \n      8\n      Austria\n      Cuba\n    \n    \n      9\n      Chile\n      Kazakhstan\n    \n  \n\n\n\n\n\n\n\n\nGDPCorr=scatter_combo(df_,'Population','number_of_medals')\nGDPCorr\n\nCorrelation of  = 0.42 (7e-06) \n            And m= 0.113 b=20.6          \nCountries worst correlation where overachive Nation\nUSA            275.692534\nRussia         172.705692\nUK             151.769692\nGermany         96.889859\nFrance          89.679993\nAustralia       86.426756\nChina           78.529401\nJapan           69.159606\nItaly           55.694604\nSouth Korea     53.504928\ndtype: float64\nCountries worst correlation where unachieve Nation\nIndia          -165.076115\nIndonesia       -41.444198\nNigeria         -40.164409\nPhilippines     -32.332446\nVietnam         -28.785832\nEgypt           -26.362309\nSudan           -24.687085\nUganda          -24.492699\nSaudi Arabia    -23.601957\nCameroon        -22.395235\ndtype: float64\n\n\n\n\n\n\n  \n    \n      \n      Underachieve\n      Overachieve\n    \n  \n  \n    \n      0\n      India\n      USA\n    \n    \n      1\n      Indonesia\n      Russia\n    \n    \n      2\n      Nigeria\n      UK\n    \n    \n      3\n      Philippines\n      Germany\n    \n    \n      4\n      Vietnam\n      France\n    \n    \n      5\n      Egypt\n      Australia\n    \n    \n      6\n      Sudan\n      China\n    \n    \n      7\n      Uganda\n      Japan\n    \n    \n      8\n      Saudi Arabia\n      Italy\n    \n    \n      9\n      Cameroon\n      South Korea\n    \n  \n\n\n\n\n\n\n\n\nscatter_combo(df_[df_['Continent']=='Europe'],'Population','number_of_medals')\n\nCorrelation of  = 0.93 (2e-16) \n            And m= 1.498 b=5.2          \nCountries worst correlation where overachive Nation\nUK                74.302826\nHungary           23.300286\nNetherlands       23.269039\nBelarus           19.808781\nDenmark           17.002417\nFrance            11.114933\nCroatia            9.991807\nCzech Republic     5.060035\nSlovenia           4.658317\nLithuania          4.630427\ndtype: float64\nCountries worst correlation where unachieve Nation\nPoland     -31.142356\nSpain      -24.255647\nPortugal   -16.682378\nRussia     -15.733456\nAustria    -14.709259\nRomania    -12.930475\nBelgium    -11.649525\nItaly      -10.448382\nGreece      -8.813758\nUkraine     -8.812069\ndtype: float64\n\n\n\n\n\n\n  \n    \n      \n      Underachieve\n      Overachieve\n    \n  \n  \n    \n      0\n      Poland\n      UK\n    \n    \n      1\n      Spain\n      Hungary\n    \n    \n      2\n      Portugal\n      Netherlands\n    \n    \n      3\n      Russia\n      Belarus\n    \n    \n      4\n      Austria\n      Denmark\n    \n    \n      5\n      Romania\n      France\n    \n    \n      6\n      Belgium\n      Croatia\n    \n    \n      7\n      Italy\n      Czech Republic\n    \n    \n      8\n      Greece\n      Slovenia\n    \n    \n      9\n      Ukraine\n      Lithuania\n    \n  \n\n\n\n\n\n\n\n\nscatter_combo(df_[df_['Continent']!='Europe'],'Population','number_of_medals')\n\nCorrelation of  = 0.49 (2e-05) \n            And m= 0.109 b=14.0          \nCountries worst correlation where overachive Nation\nUSA            283.666410\nAustralia       93.134297\nChina           90.957076\nJapan           76.277923\nSouth Korea     60.318993\nCanada          40.727568\nCuba            33.748715\nKazakhstan      27.859642\nNew Zealand     25.400672\nAzerbaijan      19.852149\ndtype: float64\nCountries worst correlation where unachieve Nation\nIndia          -152.808654\nIndonesia       -33.720543\nNigeria         -32.669700\nPhilippines     -25.269105\nVietnam         -21.778878\nEgypt           -19.334337\nSudan           -17.902477\nUganda          -17.715178\nSaudi Arabia    -16.856911\nCameroon        -15.694183\ndtype: float64\n\n\n\n\n\n\n  \n    \n      \n      Underachieve\n      Overachieve\n    \n  \n  \n    \n      0\n      India\n      USA\n    \n    \n      1\n      Indonesia\n      Australia\n    \n    \n      2\n      Nigeria\n      China\n    \n    \n      3\n      Philippines\n      Japan\n    \n    \n      4\n      Vietnam\n      South Korea\n    \n    \n      5\n      Egypt\n      Canada\n    \n    \n      6\n      Sudan\n      Cuba\n    \n    \n      7\n      Uganda\n      Kazakhstan\n    \n    \n      8\n      Saudi Arabia\n      New Zealand\n    \n    \n      9\n      Cameroon\n      Azerbaijan\n    \n  \n\n\n\n\n\n\n\n\nscatter_combo(df_[df_['Continent']=='Europe'],'GDP','number_of_medals')\n\nCorrelation of  = 0.80 (6e-09) \n            And m= 45.027 b=10.1          \nCountries worst correlation where overachive Nation\nRussia            133.114656\nUK                 47.990590\nUkraine            40.906665\nBelarus            26.198814\nHungary            25.932286\nCroatia             8.392025\nCzech Republic      4.946635\nDenmark             4.919196\nNetherlands         3.836572\nSerbia              2.527421\ndtype: float64\nCountries worst correlation where unachieve Nation\nGermany       -54.463232\nSwitzerland   -25.766716\nAustria       -25.402938\nBelgium       -22.291826\nIreland       -18.937224\nPortugal      -16.500704\nSpain         -15.776437\nFinland       -14.300792\nItaly         -12.028820\nSweden        -10.294891\ndtype: float64\n\n\n\n\n\n\n  \n    \n      \n      Underachieve\n      Overachieve\n    \n  \n  \n    \n      0\n      Germany\n      Russia\n    \n    \n      1\n      Switzerland\n      UK\n    \n    \n      2\n      Austria\n      Ukraine\n    \n    \n      3\n      Belgium\n      Belarus\n    \n    \n      4\n      Ireland\n      Hungary\n    \n    \n      5\n      Portugal\n      Croatia\n    \n    \n      6\n      Spain\n      Czech Republic\n    \n    \n      7\n      Finland\n      Denmark\n    \n    \n      8\n      Italy\n      Netherlands\n    \n    \n      9\n      Sweden\n      Serbia\n    \n  \n\n\n\n\n\n\n\n\nscatter_combo(df_[df_['Continent']!='Europe'],'GDP','number_of_medals')\n\nCorrelation of  = 0.94 (2e-33) \n            And m= 16.234 b=8.7          \nCountries worst correlation where overachive Nation\nAustralia      79.660038\nSouth Korea    44.796081\nCuba           38.591086\nKazakhstan     32.508245\nKenya          28.660696\nNew Zealand    27.815933\nAzerbaijan     25.573605\nCanada         23.586944\nJamaica        23.041050\nBrazil         19.812142\ndtype: float64\nCountries worst correlation where unachieve Nation\nIndia                  -40.315038\nSaudi Arabia           -19.100124\nIndonesia              -15.916697\nUSA                    -14.609781\nUnited Arab Emirates   -14.571357\nPhilippines            -13.602977\nIsrael                 -12.259868\nChile                  -11.840843\nVenezuela              -11.565125\nMexico                 -11.204664\ndtype: float64\n\n\n\n\n\n\n  \n    \n      \n      Underachieve\n      Overachieve\n    \n  \n  \n    \n      0\n      India\n      Australia\n    \n    \n      1\n      Saudi Arabia\n      South Korea\n    \n    \n      2\n      Indonesia\n      Cuba\n    \n    \n      3\n      USA\n      Kazakhstan\n    \n    \n      4\n      United Arab Emirates\n      Kenya\n    \n    \n      5\n      Philippines\n      New Zealand\n    \n    \n      6\n      Israel\n      Azerbaijan\n    \n    \n      7\n      Chile\n      Canada\n    \n    \n      8\n      Venezuela\n      Jamaica\n    \n    \n      9\n      Mexico\n      Brazil\n    \n  \n\n\n\n\n\n\n\n\ndf_GDP=copy.copy(df_[['Nation','Continent','GDP']])\n\ndf_GDP = df_GDP.sort_values('GDP',ascending=False)\ndf_GDP\nrich_list = df_GDP.head(52).index\n# # index_not_rich=[i for i,country in enumerate(df_.index) if (country not in rich_list) and (df_.iloc[i,2]!='Europe') and (df_.index[i]!='India')]\n# # index_rich=[i for i,country in enumerate(df_.index) if ( (df_.iloc[i,2]!='Europe') )]\nindex_rich=[i for i,country in enumerate(df_.index) if (country in rich_list) ]\nindex_not_rich=[i for i,country in enumerate(df_.index) if (country not in rich_list) ]\n\nprint('         Index Rich- Population')\ngetCorr(df_.iloc[index_rich,:],'Population','number_of_medals')\nprint('         Index Not Rich- Population')\ngetCorr(df_.iloc[index_not_rich,:],'Population','number_of_medals')\nprint('         Index Rich- GDP')\ngetCorr(df_.iloc[index_rich,:],'GDP','number_of_medals')\nprint('         Index Not Rich- GDP')\ngetCorr(df_.iloc[index_not_rich,:],'GDP','number_of_medals')\n\n         Index Rich- Population\nPearson Correlation of  = 0.37 (6e-03)\nSpearman Correlation of  = 0.30 (3e-02)\n         Index Not Rich- Population\nPearson Correlation of  = 0.17 (2e-01)\nSpearman Correlation of  = 0.15 (3e-01)\n         Index Rich- GDP\nPearson Correlation of  = 0.84 (7e-15)\nSpearman Correlation of  = 0.50 (2e-04)\n         Index Not Rich- GDP\nPearson Correlation of  = 0.32 (2e-02)\nSpearman Correlation of  = 0.38 (6e-03)\n\n\n\nprint('         Index Europe- GDP')\ngetCorr(df_[df_['Continent']=='Europe'],'GDP','number_of_medals')\nprint('         Index Not Europe- GDP')\ngetCorr(df_[df_['Continent']!='Europe'],'GDP','number_of_medals')\n\nprint('         Index Europe- Population')\ngetCorr(df_[df_['Continent']=='Europe'],'Population','number_of_medals')\nprint('         Index Not Europe- Population')\ngetCorr(df_[df_['Continent']!='Europe'],'Population','number_of_medals')\n\n         Index Europe- GDP\nPearson Correlation of  = 0.80 (6e-09)\nSpearman Correlation of  = 0.71 (1e-06)\n         Index Not Europe- GDP\nPearson Correlation of  = 0.94 (2e-33)\nSpearman Correlation of  = 0.51 (1e-05)\n         Index Europe- Population\nPearson Correlation of  = 0.93 (2e-16)\nSpearman Correlation of  = 0.81 (3e-09)\n         Index Not Europe- Population\nPearson Correlation of  = 0.49 (2e-05)\nSpearman Correlation of  = 0.42 (3e-04)\n\n\n\nprint('GDP'),getCorr(df_,'GDP','number_of_medals')\nprint('Population'),getCorr(df_,'Population','number_of_medals')\n\nGDP\nPearson Correlation of  = 0.84 (4e-29)\nSpearman Correlation of  = 0.60 (2e-11)\nPopulation\nPearson Correlation of  = 0.42 (7e-06)\nSpearman Correlation of  = 0.41 (1e-05)\n\n\n(None, None)"
  },
  {
    "objectID": "posts/olympics/2022-07-29-OlympicsSQL_GDPpopulation.html#overview-of-gdp-and-population",
    "href": "posts/olympics/2022-07-29-OlympicsSQL_GDPpopulation.html#overview-of-gdp-and-population",
    "title": "Olympics data with SQL and pandas- GDP and population",
    "section": "Overview of GDP and population",
    "text": "Overview of GDP and population\n\nHypothesis: GDP is correlated with Medals, and Population is too but less so\n\n\nMethods Used\nLooking at number of medals against GDP and population, for a period from 2008-2016 inclusive. Using population and GDP data from ~2020. - scatter plots - to visualize the relationships between medals and GDP, and medals and population. Both show a correlation - Pearson & Spearman correlation - To quatify the correlation seen. Pearson looks for a linear relationship whereas Spearman considers the ordering of the variables. The Pearson results showed a moderate to strong relationship for GDP and a weak to moderate relationship for population. The Spearman results were similar (more correlation for GDP) but the correlation values were lower. - I considered looking at the changes with time, but there were a lot of factors affecting country participation and the data was harder to obtain easily\n\n\nOverview\nResults were mainly as expected and hence PROVED.\n\nGDP\n\n\nALL = strong AND significant\nEurope = strong AND significant\nNOT Europe = very strong AND significant\n\n\nPopulation\n\n\nALL = moderate AND significant\nEurope = very strong AND significant\nNOT Europe = moderate AND significant\n\nUsing a line fit of the data (GDP or Population to Medals) gives an indication of which countries are reducing the correlation. If the country has more medals than the fit then it is overachieve and if it has less it is underachieveing. The table below shows which countries are under and overachieveing for GDP and population.\n\n\nGDPCorr.rename(columns={'Overachieve':'GDP - Overachieve','Underachieve':'GDP - Underachieve'},inplace=True)\npopCorr.rename(columns={'Overachieve':'Population - Overachieve','Underachieve':'Population - Underachieve'},inplace=True)\n\ncorrComb=pd.concat([GDPCorr,popCorr],axis=1)\ncorrComb\n\n\n\n\n\n  \n    \n      \n      GDP - Underachieve\n      GDP - Overachieve\n      Population - Underachieve\n      Population - Overachieve\n    \n  \n  \n    \n      0\n      India\n      USA\n      India\n      Russia\n    \n    \n      1\n      Indonesia\n      Russia\n      USA\n      UK\n    \n    \n      2\n      Nigeria\n      UK\n      Saudi Arabia\n      Australia\n    \n    \n      3\n      Philippines\n      Germany\n      Indonesia\n      France\n    \n    \n      4\n      Vietnam\n      France\n      United Arab Emirates\n      Germany\n    \n    \n      5\n      Egypt\n      Australia\n      Philippines\n      Ukraine\n    \n    \n      6\n      Sudan\n      China\n      Israel\n      South Korea\n    \n    \n      7\n      Uganda\n      Japan\n      Mexico\n      Italy\n    \n    \n      8\n      Saudi Arabia\n      Italy\n      Austria\n      Cuba\n    \n    \n      9\n      Cameroon\n      South Korea\n      Chile\n      Kazakhstan\n    \n  \n\n\n\n\nSo based on the above it may make sense to group countries based on their GDP, into a rich and poor list. But as shown above this doesn’t increase the correlation for either group.\nThe only useful metric to increase the correlation was to group the countries into those from Europe. This is probably due to the similarity of countries within Europe (and outside): size, GDP but also culturally; and it may be because the European countries have much greater participation at the Olympics (both over currently and historically).\n\n\nGuide on Correlation\n\npd.DataFrame({'Correlation':['1','0.8 - 1.0','0.6 - 0.8','0.4 - 0.6','0.2 - 0.4','0 - 0.2'],\\\n   'Interpretation':['Pefect','Strong to Perfect','Moderate to Very Strong','Moderate to strong','Weak to moderate','Zero to weak']})\n\n\n\n\n\n\n  \n    \n      \n      Correlation\n      Interpretation\n    \n  \n  \n    \n      0\n      1\n      Pefect\n    \n    \n      1\n      0.8 - 1.0\n      Strong to Perfect\n    \n    \n      2\n      0.6 - 0.8\n      Moderate to Very Strong\n    \n    \n      3\n      0.4 - 0.6\n      Moderate to strong\n    \n    \n      4\n      0.2 - 0.4\n      Weak to moderate\n    \n    \n      5\n      0 - 0.2\n      Zero to weak\n    \n  \n\n\n\n\nTo determine whether the correlation between variables is significant, compare the p-value to your significance level. Usually, a significance level (denoted as α or alpha) of 0.05 works well. An α of 0.05 (5e-2) indicates that the risk of concluding that a correlation exists—when, actually, no correlation exists—is 5%. The p-value tells you whether the correlation coefficient is significantly different from 0. (A coefficient of 0 indicates that there is no linear relationship.)\n\n- P-value ≤ α (5e-2): The correlation is statistically significant\n    If the p-value is less than or equal to the significance level, then you can conclude that the correlation is different from 0. \n- P-value > α (5e-2): The correlation is not statistically significant\n    If the p-value is greater than the significance level, then you cannot conclude that the correlation is different from 0. \n    \nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC6107969/\nhttps://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/regression/how-to/correlation/interpret-the-results/"
  },
  {
    "objectID": "posts/olympics/2022-07-29-OlympicsSQL_GDPpopulation.html#number-of-athletes",
    "href": "posts/olympics/2022-07-29-OlympicsSQL_GDPpopulation.html#number-of-athletes",
    "title": "Olympics data with SQL and pandas- GDP and population",
    "section": "Number of athletes",
    "text": "Number of athletes\nA metric that would make sense to correlate with medals would be the number of athletes each nation sends to an Olympics. This is because, for the most part, participation is done on merit. That is athletes have to qualify against athletes from other nations.\nSo the metrics we want to look at are: - the number of athletes per nation attending a particular games - the number of medals per nation per games\nNB I will just use male athletes for simplicity\n\nChange in average athletes per continent\nFirst, let us look if the greater number of athletes from Europe is also reflected in the average number of athletes each nation sends within each continent.\nAs shown below European countries send around twice as many athletes per nation as other continents.\n\ntempa= sqldf('SELECT                                     \\\n            Year,                                        \\\n             Continent,                                  \\\n             AVG(numbers)  as ath_per_nation             \\\n        FROM(SELECT                                      \\\n            c.Continent,                                 \\\n            a.Year,\\\n            COUNT(*)           AS numbers                \\\n            FROM                                         \\\n                df_country as c                          \\\n            LEFT JOIN                                    \\\n                df_M_S as a                              \\\n            ON                                           \\\n                a.NOC = c.NOC                            \\\n            GROUP BY                                     \\\n                a.Year,c.Continent,c.Nation              \\\n            ORDER BY                                     \\\n                Year asc) A                              \\\n            GROUP BY                                     \\\n                 Year,Continent;',locals())\n\nplt.subplots(figsize=(8,5))\ncola=['g>--','<r-.','bo-','mv-','kp:']\nfor i,continent in enumerate(tempa.Continent.unique()):\n    x=tempa[tempa.Continent==continent]['Year']\n    y=tempa[tempa.Continent==continent]['ath_per_nation']\n    plt.plot(x,y,cola[i]);\nplt.legend(tempa.Continent.unique());\nplt.ylim([-5,200])\nplt.grid(True)\nplt.xlabel('Year')\nplt.ylabel('Number of male athletes\\n per nation');\n\n\n\n\n\n\nAthletes per nation VS medals per nation\nTo look at this correlation we need to do the following steps: - Join athletes with country tables - Group by Nation and whether they have a medal - Count this, this will give number of events with a medal - Then need to remove nations that didn’t get any medals - Get scatter and correlation of these - Group by continent and do the same\n\ntempa= sqldf('SELECT                                     \\\n             Continent,                                  \\\n             Nation,                                     \\\n             Medal,                                      \\\n             SUM(numbers)   AS numbers                   \\\n        FROM(SELECT                                      \\\n            c.Continent,                                 \\\n            c.Nation,                                    \\\n            a.Medal,                                     \\\n            COUNT(*)           AS numbers                \\\n            FROM                                         \\\n                df_country as c                          \\\n            LEFT JOIN                                    \\\n                df_M_S as a                              \\\n            ON                                           \\\n                a.NOC = c.NOC                            \\\n            WHERE Year>2003                              \\\n            GROUP BY                                     \\\n                c.Continent,c.Nation,a.Medal_Gold,a.Medal_Silver,a.Medal_Bronze) A          \\\n            GROUP BY                                     \\\n                Continent, Nation,Medal;',locals())\n\ntempa2= sqldf('SELECT                                     \\\n             Nation,                                     \\\n             COUNT(numbers)   AS numbers                 \\\n        FROM(SELECT                                      \\\n            a.athlete_ID,                                \\\n            c.Nation,                                    \\\n            COUNT(*)           AS numbers                \\\n            FROM                                         \\\n                df_country as c                          \\\n            LEFT JOIN                                    \\\n                df_F_S as a                              \\\n            ON                                           \\\n                a.NOC = c.NOC                            \\\n            WHERE Year=2016                              \\\n            GROUP BY                                     \\\n                c.NOC,a.athlete_ID,a.Year) A             \\\n            GROUP BY                                     \\\n                Nation;',locals())\n\ntempa3= tempa[tempa.Medal==1]\ntempa4=sqldf('\\\n    SELECT A.Nation,A.numbers as num_medals,B.numbers as num_ath                 \\\n    FROM tempa3 as A                              \\\n    LEFT JOIN tempa2 as B                         \\\n    ON A.Nation=B.Nation                          \\\n;',locals())\ntempa4\n\n\n\n\n\n  \n    \n      \n      Nation\n      num_medals\n      num_ath\n    \n  \n  \n    \n      0\n      Algeria\n      4\n      10.0\n    \n    \n      1\n      Botswana\n      1\n      3.0\n    \n    \n      2\n      Egypt\n      9\n      37.0\n    \n    \n      3\n      Eritrea\n      1\n      1.0\n    \n    \n      4\n      Ethiopia\n      12\n      20.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      95\n      UK\n      245\n      159.0\n    \n    \n      96\n      Ukraine\n      51\n      117.0\n    \n    \n      97\n      Australia\n      251\n      212.0\n    \n    \n      98\n      Fiji\n      13\n      17.0\n    \n    \n      99\n      New Zealand\n      44\n      97.0\n    \n  \n\n100 rows × 3 columns\n\n\n\n\ntempa4=tempa4.dropna()\nx=tempa4.num_ath\ny=tempa4.num_medals\n\nplt.subplots(figsize=(8,5))\nplt.scatter(x,y)\na=scipy.stats.spearmanr(x,y)\nb=scipy.stats.pearsonr(x,y)\nprint('Spearman correlation = {:.2f} ({:.0e}) \\nand Pearson correlation= {:.2f} ({:.0e})'\\\n      .format(a[0],a[1],b[0],b[1]))\n\nplt.grid(True)\nplt.xlabel('Athletes per nation')\nplt.ylabel('Medals per nation');\n\np=np.poly1d( np.polyfit(x,y,2) )\nxx=np.arange(0,300,2)\nyy=p(np.arange(0,300,2))\n\nplt.plot(xx,yy);\n\nSpearman correlation = 0.83 (3e-26) \nand Pearson correlation= 0.87 (5e-32)\n\n\n\n\n\n\n\nOverview Medals VS Athletes per nation\nAs was expected there is a good correlation between the number of athletes a nation sends and the number of medals they get\nWhat may also have been expected and shown in the data, is that the relationship is not linear. Instead the more athletes a nation sends the greater the medals/athlete ratio.\ni.e. If a nation sends more athletes it is more likely that a higher proportion of them will win medals"
  },
  {
    "objectID": "posts/olympics/2022-07-29-OlympicsSQL_GDPpopulation.html#cold-war",
    "href": "posts/olympics/2022-07-29-OlympicsSQL_GDPpopulation.html#cold-war",
    "title": "Olympics data with SQL and pandas- GDP and population",
    "section": "Cold War",
    "text": "Cold War\nThe Cold War was a period of geopolitical tension between the United States and the Soviet Union and their respective allies, the Western Bloc and the Eastern Bloc, which began following World War II and ended in the early 1990s.\nThe conflict was based around the ideological and geopolitical struggle for global influence by these two superpowers.\nThe Soviet Union competed at the Olympics from 1952-1988. The Russian Empire had previously competed at the 1900, 1908 and 1912 Olympics games. In these games the best they ranked was 12th. In contrast the USA competed from the start of the Olympics and all subsequent games. In 1952 they were the most succesful nation, coming 1st in the medals table in 8 out of the previous 11 games (and second in the other 3).\nThe figures below show how during the Cold War period, The Soviet Union was able to compete with USA and in some cases beat them in the medals table. After this Cold War period the medals obtained by both the USA and the Soviet Union fell with respect to the totals from the European nations. Furthermore, USA reasserted it’s dominance after the Cold War period.\nWikipedia Cold War\n\ndef number_of_athletes_USA_USSR(df_F_S,df_M_S):\n    testa2=sqldf('\\\n        SELECT                                 \\\n        Year,                              \\\n        NOCSMALL,                          \\\n        count(*) AS number_of_athletes,     \\\n        \"F\" AS Sex                          \\\n    FROM                                   \\\n         (SELECT                           \\\n         athlete_ID,                        \\\n         Year,                             \\\n         CASE                                \\\n            WHEN NOC IN (\"FRA\",\"ESP\",\"ITA\",\"POR\",\"GBR\",\"IRL\",\"NED\",\"BEL\",\"DEN\",\"SUI\") THEN \"WES\"\\\n            WHEN NOC IN (\"POL\",\"ROU\",\"UKR\",\"LAT\",\"BUL\",\"HUN\",\"LTU\",\"LAT\",\"BLR\",\"ALB\",\"SVK\",\"AUT\",\"EST\",\"BIH\",\"BOH\") THEN \"EST\"\\\n            WHEN NOC=\"USA\" THEN \"USA\"       \\\n            WHEN NOC=\"EUN\" THEN \"EUN\"       \\\n            ELSE \"ROW\"\\\n            END AS NOCSMALL                \\\n         from df_F_S                       \\\n         group by athlete_ID,Year               \\\n         order by Year asc) A              \\\n     GROUP BY Year, NOCSMALL               \\\n     UNION ALL                                 \\\n     SELECT                                \\\n        Year,                              \\\n        NOCSMALL,                          \\\n        count(*) AS number_of_athletes,     \\\n        \"M\" AS Sex                          \\\n    FROM                                   \\\n         (SELECT                           \\\n         athlete_ID,                        \\\n         Year,                             \\\n         CASE                                \\\n            WHEN NOC IN (\"FRA\",\"ESP\",\"ITA\",\"POR\",\"GBR\",\"IRL\",\"NED\",\"BEL\",\"DEN\",\"SUI\") THEN \"WES\"\\\n            WHEN NOC IN (\"POL\",\"ROU\",\"UKR\",\"LAT\",\"BUL\",\"HUN\",\"LTU\",\"LAT\",\"BLR\",\"ALB\",\"SVK\",\"AUT\",\"EST\",\"BIH\",\"BOH\") THEN \"EST\"\\\n            WHEN NOC=\"USA\" THEN \"USA\"       \\\n            WHEN NOC=\"EUN\" THEN \"EUN\"       \\\n            ELSE \"ROW\"\\\n            END AS NOCSMALL                \\\n         from df_M_S                       \\\n         group by athlete_ID,Year          \\\n         order by Year asc) A              \\\n     GROUP BY Year, NOCSMALL;',locals()  )\n    return testa2\n\ndef number_of_medals_USA_USSR(df_F_S,df_M_S):\n    testa2=sqldf('\\\n        SELECT                                 \\\n            COUNT(*) AS number_of_medals,\\\n            Year, Sex, NOCSMALL\\\n        FROM \\\n        (SELECT NOCSMALL,Year,Sex,COUNT(*) AS counta\\\n        FROM                                   \\\n         (SELECT                           \\\n         athlete_ID,                        \\\n         event_ID,                          \\\n         \"F\"   AS Sex,                      \\\n         Medal_Gold,Medal_Silver,Medal_Bronze,\\\n         Year,                             \\\n         CASE                                \\\n            WHEN NOC IN (\"FRA\",\"ESP\",\"ITA\",\"POR\",\"GBR\",\"IRL\",\"NED\",\"BEL\",\"DEN\",\"SUI\") THEN \"WES\"\\\n            WHEN NOC IN (\"POL\",\"ROU\",\"UKR\",\"LAT\",\"BUL\",\"HUN\",\"LTU\",\"LAT\",\"BLR\",\"ALB\",\"SVK\",\"AUT\",\"EST\",\"BIH\",\"BOH\") THEN \"EST\"\\\n            WHEN NOC=\"USA\" THEN \"USA\"       \\\n            WHEN NOC=\"EUN\" THEN \"EUN\"       \\\n            ELSE \"ROW\"\\\n            END AS NOCSMALL                \\\n         from df_F_S                       \\\n         WHERE Medal_Gold=1 OR Medal_Silver=1 OR Medal_Bronze=1\\\n         UNION ALL                                 \\\n         SELECT                           \\\n         athlete_ID,                        \\\n         event_ID,                          \\\n         \"M\" AS Sex,                       \\\n         Medal_Gold,Medal_Silver,Medal_Bronze,\\\n         Year,                             \\\n         CASE                                \\\n            WHEN NOC IN (\"FRA\",\"ESP\",\"ITA\",\"POR\",\"GBR\",\"IRL\",\"NED\",\"BEL\",\"DEN\",\"SUI\") THEN \"WES\"\\\n            WHEN NOC IN (\"POL\",\"ROU\",\"UKR\",\"LAT\",\"BUL\",\"HUN\",\"LTU\",\"LAT\",\"BLR\",\"ALB\",\"SVK\",\"AUT\",\"EST\",\"BIH\",\"BOH\") THEN \"EST\"\\\n            WHEN NOC=\"USA\" THEN \"USA\"       \\\n            WHEN NOC=\"EUN\" THEN \"EUN\"       \\\n            ELSE \"ROW\"\\\n            END AS NOCSMALL                \\\n         from df_M_S                       \\\n         WHERE Medal_Gold=1 OR Medal_Silver=1 OR Medal_Bronze=1\\\n         order by Year asc) A\\\n     GROUP BY \\\n         Year, NOCSMALL,event_id,Medal_Gold,Medal_Silver,Medal_Bronze)  AS B\\\n GROUP BY Year, NOCSMALL, Sex\\\n                 ;',locals()  )                                       \n    return testa2\n\n\nUSA_USSR_medals=number_of_medals_USA_USSR(df_F_S,df_M_S)\nUSA_USSR_athletes=number_of_athletes_USA_USSR(df_F_S,df_M_S)\n\n\ndef yrplot(df__,whatplot= 'avg_weight'): \n    \n    countries=['EST', 'EUN' ,'ROW', 'USA' ,'WES']\n    \n#     df__.NOCSMALL.unique()\n#     countries=np.sort(countries)\n    print(countries)\n    cola=['>','o','+','*','<']\n    colur=[[1,0.6,.6],[1,0,0],[.5,.5,.5],[0,0,1],[.6,.6,1]]\n#     ['EST' 'EUN' 'ROW' 'USA' 'WES']\n#     'EST','USA','WES','ROW','EUN'\n\n    fig,ax1=plt.subplots(figsize=(8,5))\n    \n    for i,country in enumerate(countries):\n        if country!='ROW':\n            ax1.plot(df__[df__.NOCSMALL==country].Year,\\\n                 df__[df__.NOCSMALL==country][whatplot],\\\n                 marker=cola[i],linestyle='None',color=colur[i]\\\n                 ,markersize=10)\n\n\n    def doPlot(df_F,avgNo,whatplot,country,col,lw,ax1):\n        bb = df_F[df__.NOCSMALL==country].Year.rolling(avgNo).mean()\n        cc = df_F[df__.NOCSMALL==country][whatplot]\n        cc = cc.rolling(avgNo).mean()\n        ax1.plot(bb,cc,linewidth=lw,color=col)\n        return ax1\n\n    for i,country in enumerate(countries):\n        if country!='ROW':\n            ax1=doPlot(df__,avgNo=3,whatplot=whatplot,country=country,col=colur[i],lw=3,ax1=ax1)\n    \n    lega = ['East Europe','Russia','USA','West Europe']\n    plt.legend(lega)\n    plt.grid(True)\n    plt.ylabel(modname(whatplot))\n    \n    return ax1\n\n\nyrplot(USA_USSR_medals[USA_USSR_medals.Sex=='F'],whatplot= 'number_of_medals')\n\n\n\nyrplot(USA_USSR_medals[USA_USSR_medals.Sex=='M'],whatplot= 'number_of_medals')"
  },
  {
    "objectID": "posts/olympics/2022-07-29-Olympics_HomeEvent.html#overview",
    "href": "posts/olympics/2022-07-29-Olympics_HomeEvent.html#overview",
    "title": "Olympics data with SQL and pandas- home games",
    "section": "Overview",
    "text": "Overview\nThis is part of a project that looks at an Olympics dataset.\nIn this part the hypothesis considered is:\nAt a home Olympic games a nation will on average obtain more medals than at other games\n\nBut can we quantify this effect?\nAre there any residual effects before and after the games?\nWhat about a home continent games?"
  },
  {
    "objectID": "posts/olympics/2022-07-29-Olympics_HomeEvent.html#methodology",
    "href": "posts/olympics/2022-07-29-Olympics_HomeEvent.html#methodology",
    "title": "Olympics data with SQL and pandas- home games",
    "section": "Methodology",
    "text": "Methodology\nTo be able to answer the question, the steps taken are as follows. Note the data and tables created in a previous page.\n\nCreate table of medal athletes as athlete\n\nMedals\nYear\nNOC\nOthers? Sex, number_of_athletes\n\nJoin athlete table to country table to get the country added to athlete\n\nNow the next step is trickier. There are several ways we could look at the effect of a home game and the nearness to it. The way I like to do this is to visualise the plot(s) I would like and work back from that.\nThe plot I’d like is: - x-axis = year from games (0 = at the games, +ve after and -ve before) - y-axis = number of medals\nIf we normalise the y-axis we can put all the events together to get an effect of games, before and after. We can also pick out individual games or do the averageing on different time-periods.\nSo to achieve this we need a vector for each games of: - Year from games - Number of medals\nThere are a number of potential problems with this: 1. If a nation holds two games close to each other 1. Normally games are every 4 years but how do we deal with exceptions to this? 1. What do we do when there is a lack of data before or after the games? e.g. games at the start of Olympics or current games, or if a nation stops partipation 1. When looking at the effect of continent, the approach would need to be adjusted based on there being fewer continents and most games being in Europe\n\ncan be solved by reducing the times to +- 20 years, and because we are averageing any exceptions should be covered\na way to solve this is to fit the data across the years/medals data we have with a function then use the function to give us values on a set scale (e.g. -20 to 20 in steps of 4 yrs)\na bit trickier, may we fit values that have positive and negative values up to 20 yrs first. Then if an event has a gap in years fill with the average values below and above 0 yrs\nfor this maybe just look at a partcular game based on recent games that are not in the same continent"
  },
  {
    "objectID": "posts/olympics/2022-07-29-Olympics_HomeEvent.html#some-starting-code",
    "href": "posts/olympics/2022-07-29-Olympics_HomeEvent.html#some-starting-code",
    "title": "Olympics data with SQL and pandas- home games",
    "section": "Some Starting Code",
    "text": "Some Starting Code\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pandasql import sqldf\nimport copy\nimport numpy as np\nimport scipy.stats\n\n\ndf_F_S =pd.read_csv('data/athlete_F_S')\ndf_F_W=pd.read_csv('data/athlete_F_W')\ndf_M_S=pd.read_csv('data/athlete_M_S')\ndf_M_W=pd.read_csv('data/athlete_M_W')\n\ndf_all_athletes= pd.read_csv('data/all_athletes')\ndf_country= pd.read_csv('data/country')\n# df_event= pd.read_csv('data/event')\ndf_games= pd.read_csv('data/games')\n# df_population= pd.read_csv('data/population')\n\ndf_country = df_country.groupby('NOC').max()\n# df_country.head(10)\n\n\ndf_games.head()\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      games_id\n      City\n      Country\n      Year\n      Region\n      Summer\n      Winter\n      Latitude\n      Longitude\n    \n  \n  \n    \n      0\n      0\n      0\n      Athens\n      Greece\n      1896\n      Europe\n      1\n      0\n      37.9838\n      23.7275\n    \n    \n      1\n      1\n      1\n      Paris\n      France\n      1900\n      Europe\n      1\n      0\n      48.8566\n      2.3522\n    \n    \n      2\n      2\n      2\n      St. Louis\n      USA\n      1904\n      North America\n      1\n      0\n      38.6270\n      90.1994\n    \n    \n      3\n      3\n      3\n      Athens\n      Greece\n      1906\n      Europe\n      1\n      0\n      37.9838\n      23.7275\n    \n    \n      4\n      4\n      4\n      London\n      UK\n      1908\n      Europe\n      1\n      0\n      51.5072\n      0.1276"
  },
  {
    "objectID": "posts/olympics/2022-07-29-Olympics_HomeEvent.html#create-a-table-of-country-medals",
    "href": "posts/olympics/2022-07-29-Olympics_HomeEvent.html#create-a-table-of-country-medals",
    "title": "Olympics data with SQL and pandas- home games",
    "section": "Create a table of Country & Medals",
    "text": "Create a table of Country & Medals\nThis first step is to create a table of athletes data that includes both women and men athletes from the summer games, we want the following columns: - The Olympic Year - The Nation - The Continent of the Nation - The number of medals the nation got that year\nBasically just UNION to join the male and female athletes- with some additions so we get the total number of medals per country\nFollowed by a JOIN on the country table to get more information about the countries.\n\ndef number_of_medals(df_F_S,df_M_S):\n    testa2=sqldf('\\\n        SELECT                                                         \\\n            COUNT(*)              AS number_of_medals,                 \\\n            Year, NOC                                                  \\\n        FROM                                                           \\\n        (SELECT NOC,Year,COUNT(*) AS counta                            \\\n           FROM                                                        \\\n             (SELECT                                                   \\\n             athlete_ID,                                               \\\n             event_ID,                                                 \\\n             Medal_Gold,Medal_Silver,Medal_Bronze,                     \\\n             Year,                                                     \\\n             NOC                                                       \\\n             from df_F_S                                               \\\n             WHERE Medal_Gold=1 OR Medal_Silver=1 OR Medal_Bronze=1    \\\n             UNION ALL                                                 \\\n             SELECT                                                    \\\n             athlete_ID,                                               \\\n             event_ID,                                                 \\\n             Medal_Gold,Medal_Silver,Medal_Bronze,                     \\\n             Year,                                                     \\\n             NOC                                                       \\\n             from df_M_S                                               \\\n             WHERE Medal_Gold=1 OR Medal_Silver=1 OR Medal_Bronze=1    \\\n             order by Year asc) A                                      \\\n           GROUP BY                                                    \\\n             Year, NOC,event_id,Medal_Gold,Medal_Silver,Medal_Bronze)  AS B\\\n         GROUP BY Year, NOC                                            \\\n                 ;',locals()  )                                       \n    return testa2\n\ndef join_country(df_,df_country):\n    testa2=sqldf('\\\n        SELECT                             \\\n            c.NOC,c.Nation,c.Continent,    \\\n            a.number_of_medals,            \\\n            a.Year                         \\\n        FROM                               \\\n            df_ AS a                       \\\n        INNER JOIN                         \\\n            df_country AS c                \\\n        ON                                 \\\n            c.NOC=a.NOC                    \\\n        GROUP BY                           \\\n            c.NOC,a.Year                   \\\n     ;',locals()  ) \n    return testa2\n\ndf_medals=number_of_medals(df_F_S,df_M_S)\ndf_medals2=join_country(df_medals,df_country)\ndf_medals2.head()\n\n\n\n\n\n  \n    \n      \n      NOC\n      Nation\n      Continent\n      number_of_medals\n      Year\n    \n  \n  \n    \n      0\n      AFG\n      Afghanistan\n      Asia\n      1\n      2008\n    \n    \n      1\n      AFG\n      Afghanistan\n      Asia\n      1\n      2012\n    \n    \n      2\n      ALG\n      Algeria\n      Africa\n      2\n      1984\n    \n    \n      3\n      ALG\n      Algeria\n      Africa\n      2\n      1992\n    \n    \n      4\n      ALG\n      Algeria\n      Africa\n      3\n      1996"
  },
  {
    "objectID": "posts/olympics/2022-07-29-Olympics_HomeEvent.html#find-the-change-in-medals-around-a-home-games",
    "href": "posts/olympics/2022-07-29-Olympics_HomeEvent.html#find-the-change-in-medals-around-a-home-games",
    "title": "Olympics data with SQL and pandas- home games",
    "section": "Find the change in medals around a home games",
    "text": "Find the change in medals around a home games\nThe easy solution here would be to take an average of the medals a country obtains obtains at a home games versus non-home games. However, this wouldn’t work mainly due to the changes in medals with time (they increase).\nWe could correct for overall changes in the number of medals by normalising based on the total number of medals in a games. This would be better, but we may need an extra step to account for the lack of diversity of nations in earlier years too.\nBut also we wouldn’t be able to see what effect a home games had before and after the games were held.\nTo observe how medal count is effected before and after a games, and to somewhat reduce the increase in medals with time, the following steps are taken.\n\nScoll through the games table for each summer games\nFor each game add data to the following variables (of length of the number of games) for each particular games\n\nnation the nation hosting the particular games\nmedals the number of medals for each games for the nation hosting the particular games, in all the other games\nyears the difference in years between the particular games and all other games\n\n\nFor example:\nUSA 1984, Los Angeles\n\nnation[20] is ‘USA’\nyears[20] is [-88, -84, -80, -78, -76, -72, -64, -60, -56, -52, -48, -36, -32,-28, -24, -20, -16, -12,  -8,   0,   4,   8,  12,  16,  20,  24, 28,  32]\n\nthe 0 being the 1984 games, and the 32 being the 2016 games\n\nmedals[20] is [ 19,  54, 230,  23,  46,  63,  95,  99,  56, 110,  57,  84,  76, 74,  71,  90, 107,   94,  94, 173,  94, 108, 101,  91, 101, 110, 103, 121]\n\nthe 173 being the number of medals obtained in 1984 and 121 in 2016\n\nCreate a vector for each games of medals against years around the home game.\n\n\nSince not all games are seperated by 4 years and some games can be missed the data needs to be interpolated onto a range in steps of 4 years. Outer bounds of +-28 years ae chosen\nWhen data is missing because the full range of data can’t be obtained (i.e. for recent games or ones near the start of Olympics) replace any missing values with 0\n\n\nAdd up all the vectors for different games\n\n\nignore games where the range isn’t full (-32->32)\nthis gives the average effect of a home games\n\n\n# Step 1 and 2\n\n# Since we just want the summer games\ndf_games=df_games[df_games.Summer==1].reset_index()\n\nmedals=[]\nyears=[]\nnation=[]\nfor i in range(len(df_games)):\n    country=df_games.loc[i,'Country']\n    year=df_games.loc[i,'Year']\n    \n    nation.append(country)\n    medals.append(df_medals2[df_medals2.Nation==country].number_of_medals.values)\n    years.append(df_medals2[df_medals2.Nation==country].Year.values - year)\n    \n\n\n\"\"\"\nStep 3\nFunction to interpolate number of medals in years around a games\nTakes as input x=years and y=number of medals and outputs a new x and y values \nthat have been interpolated between -28 to +28 years in steps of 4 years\nFor games where the full range can't be obtained (e.g. an event close to 2016 \nwill have missing data for years after it) the data is interpolated to the nearest \n4 years and missing data replaced with zeroes.\n\"\"\"\n\ndef do_interp(x,y):\n    from scipy.interpolate import interp1d\n    \n    xx,yy=x,y\n    xhi,xlo=28,-28\n    \n    xnew=np.arange(xlo,xhi+4,4)\n    ynew=np.zeros(np.shape(xnew))\n    # here make adjustments if the whole range doesn't exist to go to nearest 4 yrs\n    # or if inbetween go outside to next one i.e. 11 years->12 years, 8->8,-5->-8\n    # lower years are dealt with separately to later years then combined\n    # and normalised\n    maxx,minx=np.max(x),np.min(x)\n    if maxx<xhi:\n        xhi= 4*(np.ceil(maxx/4))\n        xx=xx[1:]\n        yy=yy[1:]\n    if minx>xlo:\n        xlo= -4*(np.ceil(abs(minx)/4))\n        xx=xx[:-1]\n        yy=yy[:-1]\n        \n    cond =((xnew<=xhi) & (xnew>=xlo))\n    try:\n        f2 = interp1d(x, y, kind='cubic')\n        ynew[cond]=f2(xnew[cond])\n    except:\n        f2 = interp1d(xx, yy, kind='cubic')\n        cond =((xnew<=xhi-4) & (xnew>=xlo+4))\n        ynew[cond]=f2(xnew[cond])\n    \n    ynew=ynew/max(ynew)\n    return xnew,ynew\n\n\n# Step 3\n\nXY=[]\nfor i,year_range in enumerate(years):\n    x=year_range\n    y=medals[i]\n    xnew,ynew=do_interp(x,y)\n    XY.append(np.array(ynew))\n    \n\n\nnp.shape(XY)\n\n(29, 15)\n\n\n\n# If values are less than 0 put as just above zero- can occur when \n\n# make XY a numpy array\nXY = np.array(XY)\n\n# if interpolation is not good\nXY[XY<0]=0\n\n\n\n\n# create vectors for y-data either side of home-event\nyallL = np.zeros((8))\nyallR = np.zeros((8))\n\n# scroll through each games\n# find if the low years or high years have zeros in them\n# if they don't include them in the sum \nfor i,ygames in enumerate(XY):\n    \n    if i>11:\n        if 0 not in ygames[-8:]:\n            yallL[-8:]=yallL[-8:]+ygames[-8:]\n        \n        if 0 not in ygames[0:8]:\n            yallR[0:8]=yallR[0:8]+ygames[0:8]\n            \n\n# normalise\nyallL=yallL/max(yallL)\nyallR=yallR/max(yallR)\n\n# combine low and high years and only use home game year once\nyall=np.concatenate([yallR[0:-1],yallL])\n\n# plot the results\nplt.subplots(figsize=(6,4))\nplt.plot(xnew,yall/max(yall),'ok-')\n\nplt.ylim([0 ,1.05])\nplt.grid(True)\nplt.ylabel('Normalise medals won',fontsize=14)\nplt.xlabel('Years either side of a home games',fontsize=14)\n# plt.plot(xnew,yall2/max(yall2),'m+--')\n\nText(0.5, 0, 'Years either side of a home games')"
  },
  {
    "objectID": "posts/olympics/2022-07-29-Olympics_HomeEvent.html#plot-the-effect-of-home-games-for-different-games",
    "href": "posts/olympics/2022-07-29-Olympics_HomeEvent.html#plot-the-effect-of-home-games-for-different-games",
    "title": "Olympics data with SQL and pandas- home games",
    "section": "Plot the effect of home games for different games",
    "text": "Plot the effect of home games for different games\nLook at the individual games plots\n\n\nstartnum=11\n\ncola=['mo--','gs:','bv-.']\nfor i2 in range(6):\n    fig,ax=plt.subplots()\n    vals = np.arange(startnum,startnum+3,1,dtype=int)\n    i1=0\n    for i in vals: \n        plt.plot(xnew,XY[i,:],cola[i1])\n        plt.ylim([-.2 ,1])\n        plt.grid(True)\n        plt.legend([ nation[ii] + ' ' + str(df_games.loc[ii,'Year']) for jj,ii in enumerate(vals)]);\n        plt.ylabel('Normalise medals won')\n        plt.xlabel('Years either side of a home games')\n        i1=i1+1\n    startnum=startnum+3"
  },
  {
    "objectID": "posts/olympics/2022-07-29-Olympics_HomeEvent.html#continent-home-games",
    "href": "posts/olympics/2022-07-29-Olympics_HomeEvent.html#continent-home-games",
    "title": "Olympics data with SQL and pandas- home games",
    "section": "Continent home games",
    "text": "Continent home games\nThe Olympics have mainly been held in Europe and North America. Because of this it is not possible to do the same analysis as for home country games.\nInstead we can look at the difference if a games is a home continent or not.\n\ncontinentYorN=sqldf('\\\n    SELECT                                     \\\n    home_continent,                            \\\n    Year,                                      \\\n    COUNT(*)         AS num_countries,         \\\n    AVG(avg_medals)  AS avg_medals             \\\n    FROM                                       \\\n      (SELECT                                  \\\n            Nation,home_continent, Year,       \\\n            AVG(number_of_medals)  AS avg_medals\\\n        FROM                                   \\\n            (SELECT                            \\\n               a.*,                            \\\n               CASE WHEN                       \\\n                  c.Region =a.Continent THEN 1 \\\n               ELSE 0 END AS home_continent    \\\n            FROM                               \\\n                df_medals2 AS a                \\\n            INNER JOIN                         \\\n                df_games AS c                  \\\n            ON                                 \\\n                c.Year=a.Year                  \\\n            WHERE c.Year> 1951) as inner       \\\n        GROUP BY                               \\\n            Nation, Year ) as midder           \\\n    GROUP BY home_continent,Year               \\\n     ;',locals()  ) \n\n\ncontinentYorN.head()\n\n\n\n\n\n  \n    \n      \n      home_continent\n      Year\n      num_countries\n      avg_medals\n    \n  \n  \n    \n      0\n      0\n      1952\n      20\n      7.450000\n    \n    \n      1\n      0\n      1956\n      36\n      11.944444\n    \n    \n      2\n      0\n      1960\n      21\n      7.095238\n    \n    \n      3\n      0\n      1964\n      34\n      13.470588\n    \n    \n      4\n      0\n      1968\n      35\n      11.285714\n    \n  \n\n\n\n\n\nfig,ax=plt.subplots(figsize=(8,5))\n\n\n\nplt.plot(continentYorN[continentYorN.home_continent==1].Year,continentYorN[continentYorN.home_continent==1].avg_medals,'rs--')\nplt.plot(continentYorN[continentYorN.home_continent==0].Year,continentYorN[continentYorN.home_continent==0].avg_medals,'ob--')\n\nplt.grid(True)\nplt.ylabel('Average medals')\nplt.xlabel('Games year')\n\n# plt.plot([df_games[df_games.Region=='Asia'].Year.values,df_games[df_games.Region=='Asia'].Year.values],[0,35],color=[1,.8,.8]);\n\nplt.plot([0,0],'m--');\nplt.plot([0,0],'g-.');\n\nplt.plot([df_games[df_games.Region=='Americas'].Year.values,df_games[df_games.Region=='Americas'].Year.values],[0,35],'m--');\n\n\nplt.plot([df_games[df_games.Region=='Europe'].Year.values,df_games[df_games.Region=='Europe'].Year.values],[0,35],'g-.');\n\n\n# plt.plot([df_games[df_games.Region=='Oceania'].Year.values,df_games[df_games.Region=='Oceania'].Year.values],[0,35],color=[0.7,1,.7]);\n\nplt.xlim([1950,2020])\n\nplt.legend(['Home continent','Another continent','Americas','Europe'])\n\n<matplotlib.legend.Legend at 0x1f4339ed580>\n\n\n\n\n\n\nNot a good measure\nInstead need following columns\n\ntotal or average medals per continent\nHome Continent\nYear\n\n\n\n\"\"\"\nA check on the inner part of the SQL  statement\n\n\"\"\"\n\nsqldf('\\\n    SELECT *   FROM                            \\\n        (SELECT                                \\\n               a.*,                            \\\n               CASE WHEN                       \\\n                  c.Region =a.Continent THEN 1 \\\n               ELSE 0 END AS home_continent    \\\n            FROM                               \\\n                df_medals2 AS a                \\\n            INNER JOIN                         \\\n                df_games AS c                  \\\n            ON                                 \\\n                c.Year=a.Year                  \\\n            WHERE c.Year> 1951) as inner       \\\n     ;',locals()  ) \n\n\n\n\n\n\n  \n    \n      \n      NOC\n      Nation\n      Continent\n      number_of_medals\n      Year\n      home_continent\n    \n  \n  \n    \n      0\n      ANZ\n      Australia\n      Oceania\n      11\n      1952\n      0\n    \n    \n      1\n      ARG\n      Argentina\n      Americas\n      5\n      1952\n      0\n    \n    \n      2\n      AUT\n      Austria\n      Europe\n      2\n      1952\n      1\n    \n    \n      3\n      BEL\n      Belgium\n      Europe\n      4\n      1952\n      1\n    \n    \n      4\n      BOH\n      Czech Republic\n      Europe\n      13\n      1952\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      960\n      UKR\n      Ukraine\n      Europe\n      11\n      2016\n      0\n    \n    \n      961\n      USA\n      USA\n      Americas\n      121\n      2016\n      0\n    \n    \n      962\n      UZB\n      Uzbekistan\n      Asia\n      13\n      2016\n      0\n    \n    \n      963\n      VEN\n      Venezuela\n      Americas\n      3\n      2016\n      0\n    \n    \n      964\n      VIE\n      Vietnam\n      Asia\n      2\n      2016\n      0\n    \n  \n\n965 rows × 6 columns\n\n\n\n\n\"\"\"\nThe complete SQL statement\n\n\"\"\"\n\ncontinentYorN2=sqldf('\\\n    SELECT \\\n      Year, sum(number_of_medals) AS num_medals, Continent,home_continent\\\n          FROM                                 \\\n        (SELECT                                \\\n               a.*,                            \\\n               CASE WHEN                       \\\n                  c.Region =a.Continent THEN 1 \\\n               ELSE 0 END AS home_continent    \\\n            FROM                               \\\n                df_medals2 AS a                \\\n            INNER JOIN                         \\\n                df_games AS c                  \\\n            ON                                 \\\n                c.Year=a.Year                  \\\n            WHERE c.Year> 1951) as inner       \\\n        GROUP BY Continent,Year, home_continent\\\n        ORDER BY Year\\\n     ;',locals()  ) \n\ncontinentYorN2.head()\n\n\n\n\n\n  \n    \n      \n      Year\n      num_medals\n      Continent\n      home_continent\n    \n  \n  \n    \n      0\n      1952\n      11\n      Africa\n      0\n    \n    \n      1\n      1952\n      100\n      Americas\n      0\n    \n    \n      2\n      1952\n      24\n      Asia\n      0\n    \n    \n      3\n      1952\n      308\n      Europe\n      1\n    \n    \n      4\n      1952\n      14\n      Oceania\n      0\n    \n  \n\n\n\n\n\nplt.subplots(figsize=(8,5))\nregion='Asia'\ncond=((continentYorN2.Continent==region)&(continentYorN2.home_continent==1))\nplt.plot(continentYorN2[cond].Year,continentYorN2[cond].num_medals,'or',markersize=10)\ncond=((continentYorN2.Continent==region)&(continentYorN2.home_continent==0))\nplt.plot(continentYorN2[cond].Year,continentYorN2[cond].num_medals,':or')\n\nregion='Europe'\ncond=((continentYorN2.Continent==region)&(continentYorN2.home_continent==1))\nplt.plot(continentYorN2[cond].Year,continentYorN2[cond].num_medals,'bo',markersize=10)\ncond=((continentYorN2.Continent==region)&(continentYorN2.home_continent==0))\nplt.plot(continentYorN2[cond].Year,continentYorN2[cond].num_medals,'b--o')\n\nregion='Americas'\ncond=((continentYorN2.Continent==region)&(continentYorN2.home_continent==1))\nplt.plot(continentYorN2[cond].Year,continentYorN2[cond].num_medals,'m<',markersize=10)\ncond=((continentYorN2.Continent==region)&(continentYorN2.home_continent==0))\nplt.plot(continentYorN2[cond].Year,continentYorN2[cond].num_medals,'m-o')\n\nregion='Oceania'\ncond=((continentYorN2.Continent==region)&(continentYorN2.home_continent==1))\nplt.plot(continentYorN2[cond].Year,continentYorN2[cond].num_medals,'cv',markersize=10)\ncond=((continentYorN2.Continent==region)&(continentYorN2.home_continent==0))\nplt.plot(continentYorN2[cond].Year,continentYorN2[cond].num_medals,'c-o')\n\nplt.legend(['Asia','','Europe','','Americas','','Oceania',''])\nplt.grid(True)\n\n\n\n\n\nregion='Europe'\ncond=((continentYorN2.Continent==region)&(continentYorN2.home_continent==1))\nasum=np.average(continentYorN2[cond].num_medals)\ncond=((continentYorN2.Continent==region)&(continentYorN2.home_continent==0))\nbsum=np.average(continentYorN2[cond].num_medals)\n\nprint('For {} the average medals at home continent = {:.0f} and for away continent = {:.0f}\\n \\\n      With a ratio of {:.2f}'.format(region,asum,bsum,asum/bsum))\n\n\nregion='Asia'\ncond=((continentYorN2.Continent==region)&(continentYorN2.home_continent==1))\nasum=np.average(continentYorN2[cond].num_medals)\ncond=((continentYorN2.Continent==region)&(continentYorN2.home_continent==0))\nbsum=np.average(continentYorN2[cond].num_medals)\n\nprint('For {} the average medals at home continent = {:.0f} and for away continent = {:.0f}\\n \\\n      With a ratio of {:.2f}'.format(region,asum,bsum,asum/bsum))\n\nregion='Americas'\ncond=((continentYorN2.Continent==region)&(continentYorN2.home_continent==1))\nasum=np.average(continentYorN2[cond].num_medals)\ncond=((continentYorN2.Continent==region)&(continentYorN2.home_continent==0))\nbsum=np.average(continentYorN2[cond].num_medals)\n\nprint('For {} the average medals at home continent = {:.0f} and for away continent = {:.0f}\\n \\\n      With a ratio of {:.2f}'.format(region,asum,bsum,asum/bsum))\n\nregion='Oceania'\ncond=((continentYorN2.Continent==region)&(continentYorN2.home_continent==1))\nasum=np.average(continentYorN2[cond].num_medals)\ncond=((continentYorN2.Continent==region)&(continentYorN2.home_continent==0))\nbsum=np.average(continentYorN2[cond].num_medals)\n\nprint('For {} the average medals at home continent = {:.0f} and for away continent = {:.0f}\\n \\\n      With a ratio of {:.2f}'.format(region,asum,bsum,asum/bsum))\n\nFor Europe the average medals at home continent = 420 and for away continent = 396\n       With a ratio of 1.06\nFor Asia the average medals at home continent = 121 and for away continent = 103\n       With a ratio of 1.18\nFor Americas the average medals at home continent = 177 and for away continent = 130\n       With a ratio of 1.36\nFor Oceania the average medals at home continent = 50 and for away continent = 32\n       With a ratio of 1.57\n\n\nAgain the above stats are not really great as\n\ndon’t take account of changes in the average number of medals with time for a nation\ngames in Oceania and Americas are often also home games\n\nThe best metric uses Europe and suggests a less than 6% increase for a home continent"
  },
  {
    "objectID": "posts/olympics/2022-07-31-OlympicsSQL.html#introduction",
    "href": "posts/olympics/2022-07-31-OlympicsSQL.html#introduction",
    "title": "Olympics data with SQL and pandas",
    "section": "Introduction",
    "text": "Introduction\nThis project looks to understand the changing nature of the Olympics and how it reflects changes in athletes, sporting activities, and global politics. The Olympic Games are considered the world’s foremost sports competition with more than 200 nations participating [1,2], and in Tokyo in 2020 there was a broadcast audience of more than 3 billion with estimates of 3 out of 4 people following the Olympics [3].\nSince it’s inception over 100 years ago many changes have occured. In global politics countries have split and unified, populations have changed and the power distribution across nations fluctuated. In society there have been changes in the rights and roles of women. Finally, in sport there has been a move from amateur atheletes to professionalism and a change in the popularity of different sports.\nDue to it’s global importance, the question this work looks to answer is if data on the Olympics reflect the changes that have occured in the world.\n1.“Overview of Olympic Games”. Encyclopaedia Britannica. Retrieved 4 June 2008\n2.Olympic Games- Wikipedia\n3.Tokyo 2020 audience & insights report December 2021"
  },
  {
    "objectID": "posts/olympics/2022-07-31-OlympicsSQL.html#the-data",
    "href": "posts/olympics/2022-07-31-OlympicsSQL.html#the-data",
    "title": "Olympics data with SQL and pandas",
    "section": "The Data",
    "text": "The Data\nThe most important part of any analysis is the data. In Olympics data with SQL and pandas- create the tables I present the data to be analysed and do some initial processing.\nThe main thing here is to seperate the data into useable tables for analysis, as summarised in the entity relationship diagram (ERD) below."
  },
  {
    "objectID": "posts/olympics/2022-07-31-OlympicsSQL.html#analysis",
    "href": "posts/olympics/2022-07-31-OlympicsSQL.html#analysis",
    "title": "Olympics data with SQL and pandas",
    "section": "Analysis",
    "text": "Analysis\nBased on a brief analysis of the data three broad questions to be investigated were posed:\n\nWhat are the characteristics of athletes? How does this change with time, and can it be linked with societal or global changes?\nWhat countries do better at the Olympics? Is there a way to quantify this?\nWhat is the influence of a games being a home event?\n\nIn the following parts these are explore in more detail.\n\nAthlete Analysis\nI did some initial plots on the changes in the characteristics of athletes given in the data, height, weight and age, of athletes attending the Olympics by year (see below).\nFrom these plots I was really intrigued as to what may be the cause of these changes.\nMainly what was happening between 1960 and 1980 were there seemed to be changes in each of the parameters?\nMy initial thought was this could be related to some combination of - a switch from amateurs to professionals - the Cold War between USA and USSR - an after effect of WWII\nOlympics data with SQL and pandas- height weight and age\n\n\nNation Analysis\nDue to the global importance of the Olympics, in 2020 there was a broadcast audience of more than 3 billion, I was interested to explore whether countries with the most medals will reflect global politics. And to see if the countries with most influence get more medals.\nOlympics data with SQL and pandas- GDP and population\n\n\nGames Analysis\nIn this part the hypothesis considered is:\nAt a home Olympic games a nation will on average obtain more medals than at other games\n\nBut can we quantify this effect?\nAre there any residual effects before and after the games?\nWhat about a home continent games?\n\nOlympics data with SQL and pandas- home games"
  },
  {
    "objectID": "posts/olympics/2022-07-31-OlympicsSQL.html#presentation",
    "href": "posts/olympics/2022-07-31-OlympicsSQL.html#presentation",
    "title": "Olympics data with SQL and pandas",
    "section": "Presentation",
    "text": "Presentation\nTo present this data in a unified form the following presentation was produced.\nThis is a hypothetical presentation:\n\nWho\nThe audience is a fictional research group at Swansea University (UK) called the Sports History Group.\nThis group is a cross-departmental, working across the History and Sports Science department. The group consists of two lecturers (one in each department), three post doctoral researchers, five PhD students and three Masters students.\n\n\nWhy\nThe work I am presenting has overlap with several of the reserachers/students.\nThe main goal is a scoping exercise with one of the post doctoral researchers and the two lecturers who have identified a grant proposal. The Olympics commitee have put out a grant application. The aim of this is to produce a report on the influence the Olympics has had on Geo-Politics and on Athletes and Sport in general. With guidance on what the Olympics can do in the future to maintain and enhance its globally importance, and how it can positively impact Olympic athletes.\n\n\nWhat / How\nMore details are in the presentation"
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#what-is-the-cloud",
    "href": "posts/others/2020-01-01-AWS.html#what-is-the-cloud",
    "title": "ThomasHSimm",
    "section": "What is the Cloud?",
    "text": "What is the Cloud?\nIn the past, companies and organizations hosted and maintained hardware such as compute, storage, and networking equipment in their own data centers. They needed to allocate entire infrastructure departments to take care of them, resulting in a costly operation that made some workloads and experimentation impossible.\nAs internet usage became more widespread, the demand for compute, storage, and networking equipment increased. For some companies and organizations, the cost of maintaining a large physical presence was unsustainable. To solve this problem, cloud computing was created.\nCloud computing is the on-demand delivery of IT resources over the internet with pay-as-you-go pricing. You no longer have to manage and maintain your own hardware in your own data centers. Companies like AWS own and maintain these data centers and provide virtualized data center technologies and services to users over the internet.\nTo help differentiate between running workloads on-premises versus in the cloud, consider the scenario where your developers need to deploy a new feature on your application. Before they deploy, the team wants to test the feature in a separate quality assurance (QA) environment that has the exact same configurations as production.\nIf you run your application on-premises, creating this additional environment requires you to buy and install hardware, connect the necessary cabling, provision power, install operating systems, and more. All of these tasks can be time-consuming and take days to perform. Meanwhile, the new product feature’s time-to-market is increasing and your developers are waiting for this environment.\nIf you ran your application in the cloud, you can replicate the entire environment as often as needed in a matter of minutes or even seconds. Instead of physically installing hardware and connecting cabling, you can logically manage your physical infrastructure over the internet.\nUsing cloud computing not only saves you time from the set-up perspective, but it also removes the undifferentiated heavy lifting. If you look at any application, you’ll see that some of the aspects of it are very important to your business, like the code. However, there are other aspects that are no different than any other application you might make: for instance the compute the code runs on. By removing repetitive common tasks that don’t differentiate your business, like installing virtual machines, or storing backups, you can focus on what is strategically unique to your business and let AWS handle the tasks that are time consuming and don’t separate you from your competitors.\nSo where does AWS fit into all of this? Well AWS simply just provides cloud computing services. Those IT resources mentioned in the cloud computing definition are AWS services in this case. We’ll need to use these AWS services to architect a scalable, highly available, and cost effective infrastructure to host our corporate directory application. This way we can get our corporate directory app out into the world quickly, without having to manage any heavy-duty physical hardware. There are the six main advantages to running your workloads on AWS."
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#the-six-benefits-of-cloud-computing",
    "href": "posts/others/2020-01-01-AWS.html#the-six-benefits-of-cloud-computing",
    "title": "ThomasHSimm",
    "section": "The Six Benefits of Cloud Computing",
    "text": "The Six Benefits of Cloud Computing\n\nPay as you go. Instead of investing in data centers and hardware before you know how you are going to use them, you pay only when you use computing resources, and pay only for how much you use.\nBenefit from massive economies of scale. By using cloud computing, you can achieve a lower cost than you can get on your own. Because usage from hundreds of thousands of customers is aggregated in the cloud, AWS can achieve higher economies of scale, which translates into lower pay as-you-go prices.\nStop guessing capacity. Eliminate guessing on your infrastructure capacity needs. When you make a capacity decision prior to deploying an application, you often end up either sitting on expensive idle resources or dealing with limited capacity. With cloud computing, these problems go away. You can access as much or as little capacity as you need, and scale up and down as required with only a few minutes notice.\nIncrease speed and agility. IT resources are only a click away, which means that you reduce the time to make those resources available to your developers from weeks to just minutes. This results in a dramatic increase in agility for the organization since the cost and time it takes to experiment and develop is significantly lower.\nStop spending money running and maintaining data centers. Focus on projects that differentiate your business, not the infrastructure. Cloud computing lets you focus on your customers, rather than on the heavy lifting of racking, stacking, and powering physical infrastructure. This is often referred to as undifferentiated heavy lifting.\nGo global in minutes. Easily deploy your application in multiple Regions around the world with just a few clicks. This means you can provide lower latency and a better experience for your customers at a minimal cost.\n\n\nhttps://aws.amazon.com/what-is-cloud-computing/\nhttp://docs.aws.amazon.com/whitepapers/latest/aws-overview/types-of-cloud-computing.html\nhttps://aws.amazon.com/what-is-aws/\n\n\nAWS Global Infrastructure\n\nInfrastructure, like data centers and networking connectivity, still exists as the foundation of every cloud application. In AWS, this physical infrastructure makes up the AWS Global Infrastructure, in the form of Availability Zones and Regions.\nRegions are geographic locations worldwide where AWS hosts its data centers. AWS Regions are named after the location where they reside. For example, in the United States, there is a Region in Northern Virginia called the Northern Virginia Region and a Region in Oregon called the Oregon Region. There are Regions in Asia Pacific, Canada, Europe, the Middle East, and South America, and AWS continues to expand to meet the needs of its customers.\nEach AWS Region is associated with a geographical name and a Region code.\nfor example, - us-east-1: This is the first Region created in the east of the US. The geographical name for this Region is N. Virginia. - ap-northeast-1: The first Region created in the northeast of Asia Pacific. The geographical name for this Region is Tokyo.\n\nChoose the Right AWS Region\nConsider four main aspects when deciding which AWS Region to host your applications and workloads: latency, price, service availability, and compliance.\n\nLatency. If your application is sensitive to latency, choose a Region that is close to your user base. This helps prevent long wait times for your customers. Synchronous applications such as gaming, telephony, WebSockets, and IoT are significantly affected by higher latency, but even asynchronous workloads, such as ecommerce applications, can suffer from an impact on user connectivity.\nPrice. Due to the local economy and the physical nature of operating data centers, prices may vary from one Region to another. The pricing in a Region can be impacted by internet connectivity, prices of imported pieces of equipment, customs, real estate, and more. Instead of charging a flat rate worldwide, AWS charges based on the financial factors specific to the location.\n\nService availability. Some services may not be available in some Regions. The AWS documentation provides a table containing the Regions and the available services within each one.\nData compliance. Enterprise companies often need to comply with regulations that require customer data to be stored in a specific geographic territory. If applicable, you should choose a Region that meets your compliance requirements."
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#availability-zones",
    "href": "posts/others/2020-01-01-AWS.html#availability-zones",
    "title": "ThomasHSimm",
    "section": "Availability Zones",
    "text": "Availability Zones\nInside every Region is a cluster of Availability Zones (AZ). An AZ consists of one or more data centers with redundant power, networking, and connectivity. These data centers operate in discrete facilities with undisclosed locations. They are connected using redundant high-speed and low-latency links.\n\nWhat is low latency link? Low latency describes a computer network that is optimized to process a very high volume of data messages with minimal delay (latency).\n\n\nWhat does redundant mean in routing? Failover or Redundant routing is a network arrangement with several links and paths between the person who places a call and the call recipient. Should at any time a path or link in the network, the other links will automatically route the incoming call to a predetermined number, device or location.\n\nAZs also have a code name. Since they’re located inside Regions, they can be addressed by appending a letter to the end of the Region code name. For example:\n\nus-east-1a: an AZ in us-east-1 (Northern Virginia Region)\nsa-east-1b: an AZ in sa-east-1 (São Paulo Region in South America)\n\nIf you see that a resource exists in us-east-1c, you know this resource is located in AZ c of the us-east-1 Region.\n\nScope AWS Services\nDepending on the AWS Service you use, your resources are either deployed at the AZ, Region, or Global level. Each service is different, so you need to understand how the scope of a service may affect your application architecture.\nWhen you operate a Region-scoped service, you only need to select the Region you want to use. If you are not asked to specify an individual AZ to deploy the service in, this is an indicator that the service operates on a Region-scope level. For region-scoped services, AWS automatically performs actions to increase data durability and availability.\nOn the other hand, some services ask you to specify an AZ. With these services, you are often responsible for increasing the data durability and high availability of these resources.\n\n\nMaintain Resiliency\n\nWhat is resiliency? The capability to recover when stressed by load (more requests for service), attacks (either accidental through a bug, or deliberate through intention), and failure of any component in the workload’s components.\n\nTo keep your application available, you need to maintain high availability and resiliency. A well-known best practice for cloud architecture is to use Region-scoped, managed services. These services come with availability and resiliency built in.\nWhen that is not possible, make sure the workload is replicated across multiple AZs. At a minimum, you should use two AZs. If one entire AZ fails, your application will have infrastructure up and running in at least a second AZ to take over the traffic.\n\nInteracting with AWS\n\n\nAPI call: Application programming interfaces (APIs) are a way for one program to interact with another. API calls are the medium by which they interact. An API call, or API request, is a message sent to a server asking an API to provide a service or information.\n\nEvery action you make in AWS is an API call that is authenticated and authorized. In AWS, you can make API calls to services and resources through the AWS Management Console, the AWS Command Line Interface (CLI), or the AWS Software Development Kits (SDKs).\n\n\nThe AWS Management Console\nOne way to manage cloud resources is through the web-based console, where you log in and click on the desired service. This can be the easiest way to create and manage resources when you first begin working with the cloud. Below is a screenshot that shows the landing page when you first log into the AWS Management Console.\n\nThe services are placed in categories, such as compute, database, storage and security, identity and compliance.\nOn the upper right corner is the Region selector. If you click it and change the Region, you will make requests to the services in the chosen Region. The URL changes, too. Changing the Region directs the browser to make requests to a whole different AWS Region, represented by a different subdomain.\n\n\nThe AWS Command Line Interface (CLI)\nConsider the scenario where you run tens of servers on AWS for your application’s backend. You want to run a report to collect data from all of these servers. You need to do this programmatically every day because the server details may change. Instead of manually logging into the AWS Management Console and copying/pasting information, you can schedule an AWS Command Line Interface (CLI) script with an API call to pull this data for you.\nThe AWS CLI is a unified tool to manage AWS services. With just one tool to download and configure, you control multiple AWS services from the command line and automate them with scripts. The AWS CLI is open-source, and there are installers available for Windows, Linux, and Mac OS.\nHere is an example of running an API call against a service using the AWS CLI: aws ec2 describe-instances\nYou get this response:\n{     \"Reservations\": [         {             \"Groups\": [],             \"Instances\": [                 {                     \"AmiLaunchIndex\": 0,\nand so on.\n\n\nAWS Software Development Kits (SDKs)\nAPI calls to AWS can also be performed by executing code with programming languages. You can do this by using AWS Software Development Kits (SDKs). SDKs are open-source and maintained by AWS for the most popular programming languages, such as C++, Go, Java, JavaScript, .NET, Node.js, PHP, Python, and Ruby.\nDevelopers commonly use AWS SDKs to integrate their application source code with AWS services. Let’s say the backend of the application runs in Python and every time it receives a cat photo, it uploads that photo to a storage service. This action can be achieved from within the source code by using the AWS SDK for Python.\nHere is an example of code you can implement to work with AWS resources using the Python AWS SDK.\nimport boto3 ec2 = boto3.client('ec2') response = ec2.describe_instances() print(response)\n\nBack to contents\n\n\n\nSecurity and the AWS Shared Responsibility Model\n\n\nWhen you begin working with the AWS Cloud, managing security and compliance is a shared responsibility between AWS and you. To depict this shared responsibility, AWS created the shared responsibility model. This distinction of responsibility is commonly referred to as security of the cloud, versus security in the cloud.\n\n\n\nWhat Is AWS Responsible For?\nAWS is responsible for security of the cloud. This means AWS is required to protect and secure the infrastructure that runs all the services offered in the AWS Cloud. AWS is responsible for:\n\nProtecting and securing AWS Regions, Availability Zones, and data centers, down to the physical security of the buildings\nManaging the hardware, software, and networking components that run AWS services, such as the physical server, host operating systems, virtualization layers, and AWS networking components\n\nThe level of responsibility AWS has depends on the service. AWS classifies services into three different categories. The following table provides information about each, as well as the AWS responsibility.\n\n\n\n\n\n\n\n\nCategory\nExamples of AWS Services in the Category\nAWS Responsibility\n\n\n\n\nInfrastructure services\nCompute services, such as Amazon Elastic Compute Cloud (Amazon EC2)\nAWS manages the underlying infrastructure and foundation services.\n\n\nContainer services\nServices that require less management from the customer, such as Amazon Relational Database Service (Amazon RDS)\nAWS manages the underlying infrastructure and foundation services, operating system, and application platform.\n\n\nAbstracted services\nServices that require very little management from the customer, such as Amazon Simple Storage Service (Amazon S3)\nAWS operates the infrastructure layer, operating system, and platforms, as well as server-side encryption and data protection.\n\n\n\nNote Container services refer to AWS abstracting application containers behind the scenes, not Docker container services. This enables AWS to move the responsibility of managing that platform away from customers.\n\n\nWhat Is the Customer Responsible For?\nYou’re responsible for security in the cloud. When using any AWS service, you’re responsible for properly configuring the service and your applications, as well as ensuring your data is secure.\nThe level of responsibility you have depends on the AWS service. Some services require you to perform all the necessary security configuration and management tasks, while other more abstracted services require you to only manage the data and control access to your resources. Using the three categories of AWS services, you can determine your level of responsibility for each AWS service you use.\n\n\n\n\n\n\n\n\nCategory\nAWS Responsibility\nCustomer Responsibility\n\n\n\n\nInfrastructure services\nAWS manages the infrastructure and foundation services.\nYou control the operating system and application platform, as well as encrypting, protecting, and managing customer data.\n\n\nContainer services\nAWS manages the infrastructure and foundation services, operating system, and application platform.\nYou are responsible for customer data, encrypting that data, and protecting it through network firewalls and backups.\n\n\nAbstracted services\nAWS operates the infrastructure layer, operating system, and platforms, as well as server-side encryption and data protection.\nYou are responsible for managing customer data and protecting it through client-side encryption.\n\n\n\nDue to the varying level of effort, it’s important to consider which AWS service you use and review the level of responsibility required to secure the service. It’s also important to review how the shared security model aligns with the security standards in your IT environment, as well as any applicable laws and regulations.\nIt’s important to note that you maintain complete control of your data and are responsible for managing the security related to your content. Here are some examples of your responsibilities in context.\n\nChoosing a Region for AWS resources in accordance with data sovereignty regulations\nImplementing data protection mechanisms, such as encryption and managing backups\nUsing access control to limit who has access to your data and AWS resources\n\n\nProtect the AWS Root User"
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#whats-the-big-deal-about-auth",
    "href": "posts/others/2020-01-01-AWS.html#whats-the-big-deal-about-auth",
    "title": "ThomasHSimm",
    "section": "What’s the Big Deal About Auth?",
    "text": "What’s the Big Deal About Auth?\nWhen you’re configuring access to any account, two terms come up frequently: - authentication - and authorization.\nThough these terms may seem basic, you need to understand them to properly configure access management on AWS. It’s important to keep this in mind as you progress through the course. Let’s define both terms.\n\nUnderstand Authentication\nWhen you create your AWS account, you use a combination of an email address and a password to verify your identity. If the user types in the correct email and password, the system assumes the user is allowed to enter and grants them access. This is the process of authentication.\nAuthentication ensures that the user is who they say they are. Usernames and passwords are the most common types of authentication, but you may also work with other forms, such as token-based authentication or biometric data like a fingerprint. Authentication simply answers the question, “Are you who you say you are?”\n\n\nUnderstand Authorization\nOnce you’re inside your AWS account, you might be curious about what actions you can take. This is where authorization comes in. Authorization is the process of giving users permission to access AWS resources and services. Authorization determines whether the user can perform an action—whether it be to read, edit, delete, or create resources.\nAuthorization answers the question, “What actions can you perform?”\n\n\nWhat Is the AWS Root User?\nWhen you first create an AWS account, you begin with a single sign-in identity that has complete access to all AWS services and resources in the account. This identity is called the AWS root user and is accessed by signing in with the email address and password that you used to create the account.\n\n\nUnderstand the AWS Root User Credentials\nThe AWS root user has two sets of credentials associated with it. One set of credentials is the email address and password used to create the account. This allows you to access the AWS Management Console. The second set of credentials is called access keys, which allow you to make programmatic requests from the AWS Command Line Interface (AWS CLI) or AWS API.\nAccess keys consist of two parts:\n\nAn access key ID, for example, A2lAl5EXAMPLE\nA secret access key, for example, wJalrFE/KbEKxE\n\nSimilar to a username and password combination, you need both the access key ID and secret access key to authenticate your requests via the AWS CLI or AWS API. Access keys should be managed with the same security as an email address and password.\n\n\nFollow Best Practices When Working with the AWS Root User\nKeep in mind that the root user has complete access to all AWS services and resources in your account, as well as your billing and personal information. Due to this, securely lock away the credentials associated with the root user and do not use the root user for everyday tasks.\nTo ensure the safety of the root user:\n\nChoose a strong password for the root user.\nNever share your root user password or access keys with anyone.\nDisable or delete the access keys associated with the root user.\nDo not use the root user for administrative tasks or everyday tasks.\n\nWhen is it OK to use the AWS root user?\nThere are some tasks where it makes sense to use the AWS root user. Check out the links at the end of this section to read about them.\n\n\nDelete Your Keys to Stay Safe\nIf you don’t already have an access key for your AWS account root user, don’t create one unless you absolutely need to. If you do have an access key for your AWS account root user and want to delete the keys:\n\nGo to the My Security Credentials page in the AWS Management Console and sign in with the root user’s email address and password.\nOpen the Access keys section.\nUnder Actions, click Delete.\nClick Yes.\n\n\n\nThe Case for Multi-Factor Authentication\nWhen you create an AWS account and first log in to that account, you use single-factor authentication. Single-factor authentication is the simplest and most common form of authentication. It only requires one authentication method. In this case, you use a username and password to authenticate as the AWS root user. Other forms of single-factor authentication include a security pin or a security token.\nHowever, sometimes a user’s password is easy to guess. For example, your coworker Bob’s password, IloveCats222, might be easy for someone who knows Bob personally to guess, because it’s a combination of information that is easy to remember and describes certain things about Bob (1. Bob loves cats, and 2. Bob’s birthday is February 22).\nIf a bad actor guessed or cracked Bob’s password through social engineering, bots, or scripts, Bob might lose control of his account. Unfortunately, this is a common scenario that users of any website often face.\nThis is why using MFA has become so important in preventing unwanted account access. MFA requires two or more authentication methods to verify an identity, pulling from three different categories of information.\n\nSomething you know, such as a username and password, or pin number\nSomething you have, such as a one-time passcode from a hardware device or mobile app\nSomething you are, such as fingerprint or face scanning technology\n\nUsing a combination of this information enables systems to provide a layered approach to account access. Even though the first method of authentication, Bob’s password, was cracked by a malicious user, it’s very unlikely that a second method of authentication, such as a fingerprint, would also be cracked.\nThis extra layer of security is needed when protecting your most sacred accounts, which is why it’s important to enable MFA on your AWS root user.\n\n\nUse MFA on AWS\nIf you enable MFA on your root user, you are required to present a piece of identifying information from both the something you know category and the something you have category. The first piece of identifying information the user enters is an email and password combination. The second piece of information is a temporary numeric code provided by an MFA device.\nEnabling MFA adds an additional layer of security because it requires users to use a supported MFA mechanism in addition to their regular sign-in credentials. It’s best practice to enable MFA on the root user.\n\nReview Supported MFA Devices\nAWS supports a variety of MFA mechanisms, such as virtual MFA devices, hardware devices, and Universal 2nd Factor (U2F) security keys. For instructions on how to set up each method, check out the Resources section.\n\nIntroduction to AWS Identity and Access Management\n\n\n\n\nWhat Is IAM?\nIAM is a web service that enables you to manage access to your AWS account and resources. It also provides a centralized view of who and what are allowed inside your AWS account (authentication), and who and what have permissions to use and work with your AWS resources (authorization).\nWith IAM, you can share access to an AWS account and resources without having to share your set of access keys or password. You can also provide granular access to those working in your account, so that people and services only have permissions to the resources they need. For example, to provide a user of your AWS account with read-only access to a particular AWS service, you can granularly select which actions and which resources in that service they can access.\n\nGet to Know the IAM Features\nTo help control access and manage identities within your AWS account, IAM offers many features to ensure security.\n\nIAM is global and not specific to any one Region. This means you can see and use your IAM configurations from any Region in the AWS Management Console.\nIAM is integrated with many AWS services by default.\nYou can establish password policies in IAM to specify complexity requirements and mandatory rotation periods for users.\nIAM supports MFA (multi factor auth).\nIAM supports identity federation, which allows users who already have passwords elsewhere—for example, in your corporate network or with an internet identity provider—to get temporary access to your AWS account.\nAny AWS customer can use IAM; the service is offered at no additional charge.\n\n\n\n\nWhat Is an IAM User?\nAn IAM user represents a person or service that interacts with AWS. You define the user within your AWS account. And any activity done by that user is billed to your account. Once you create a user, that user can sign in to gain access to the AWS resources inside your account.\nYou can also add more users to your account as needed. For example, for your cat photo application, you could create individual users in your AWS account that correspond to the people who are working on your application. Each person should have their own login credentials. Providing users with their own login credentials prevents sharing of credentials.\n\n\nIAM User Credentials\nAn IAM user consists of a name and a set of credentials. When creating a user, you can choose to provide the user:\n\nAccess to the AWS Management Console\nProgrammatic access to the AWS Command Line Interface (AWS CLI) and AWS Application Programming Interface (AWS API)\n\nTo access the AWS Management Console, provide the users with a user name and password. For programmatic access, AWS generates a set of access keys that can be used with the AWS CLI and AWS API. IAM user credentials are considered permanent, in that they stay with the user until there’s a forced rotation by admins.\nWhen you create an IAM user, you have the option to grant permissions directly at the user level.\nThis can seem like a good idea if you have only one or a few users. However, as the number of users helping you build your solutions on AWS increases, it becomes more complicated to keep up with permissions. For example, if you have 3,000 users in your AWS account, administering access becomes challenging, and it’s impossible to get a top-level view of who can perform what actions on which resources.\nIf only there were a way to group IAM users and attach permissions at the group level instead. Guess what: There is!\n\n\nWhat Is an IAM Group?\nAn IAM group is a collection of users. All users in the group inherit the permissions assigned to the group. This makes it easy to give permissions to multiple users at once. It’s a more convenient and scalable way of managing permissions for users in your AWS account. This is why using IAM groups is a best practice.\nIf you have an application that you’re trying to build and have multiple users in one account working on the application, you might decide to organize these users by job function. You might want IAM groups organized by developers, security, and admins. You would then place all of your IAM users in the respective group for their job function.\nThis provides a better view to see who has what permissions within your organization and an easier way to scale as new people join, leave, and change roles in your organization.\nConsider the following examples.\n\nA new developer joins your AWS account to help with your application. You simply create a new user and add them to the developer group, without having to think about which permissions they need.\nA developer changes jobs and becomes a security engineer. Instead of editing the user’s permissions directly, you can instead remove them from the old group and add them to the new group that already has the correct level of access.\n\nKeep in mind the following features of groups.\n\nGroups can have many users.\nUsers can belong to many groups.\nGroups cannot belong to groups.\n\nThe root user can perform all actions on all resources inside an AWS account by default. This is in contrast to creating new IAM users, new groups, or new roles. New IAM identities can perform no actions inside your AWS account by default until you explicitly grant them permission.\nThe way you grant permissions in IAM is by using IAM policies.\n\n\nWhat Is an IAM Policy?\nTo manage access and provide permissions to AWS services and resources, you create IAM policies and attach them to IAM users, groups, and roles. Whenever a user or role makes a request, AWS evaluates the policies associated with them.\nFor example, if you have a developer inside the developers group who makes a request to an AWS service, AWS evaluates any policies attached to the developers group and any policies attached to the developer user to determine if the request should be allowed or denied.\n\nIAM Policy Examples\nMost policies are stored in AWS as JSON documents with several policy elements. Take a look at the following example of what providing admin access through an IAM identity-based policy looks like.\n{ \"Version\": \"2012-10-17\",     \"Statement\": [{         \"Effect\": \"Allow\",         \"Action\": \"*\",         \"Resource\": \"*\"      }] }\nIn this policy, there are four major JSON elements: - Version, - Effect, - Action, - Resource.\nThe Version element defines the version of the policy language. It specifies the language syntax rules that are needed by AWS to process a policy. To use all the available policy features, include “Version”: “2012-10-17” before the “Statement” element in all your policies.\nThe Effect element specifies whether the statement will allow or deny access. In this policy, the Effect is “Allow”, which means you’re providing access to a particular resource.\nThe Action element describes the type of action that should be allowed or denied. In the above policy, the action is “*“. This is called a wildcard, and it is used to symbolize every action inside your AWS account.\nThe Resource element specifies the object or objects that the policy statement covers. In the policy example above, the resource is also the wildcard “*“. This represents all resources inside your AWS console.\nPutting all this information together, you have a policy that allows you to perform all actions on all resources inside your AWS account. This is what we refer to as an administrator policy.\nLet’s look at another example of a more granular IAM policy.\n{ \"Version\": \"2012-10-17\",     \"Statement\": [{         \"Effect\": \"Allow\",         \"Action\": [             \"iam: ChangePassword\",             \"iam: GetUser\"             ]         \"Resource\": \"arn:aws:iam::123456789012:user/${aws:username}\"     }] }\nAfter looking at the JSON, you can see that this policy allows the IAM user to change their own IAM password (iam:ChangePassword) and get information about their own user (iam:GetUser).\nIt only permits them to access their own credentials because the resource restricts access with the variable substitution ${aws:username}.\n\n\nUnderstand Policy Structure\nWhen creating a policy, it is required to have each of the following elements inside a policy statement.\n\n\n\n\n\n\n\n\n\nElement\nDescription\nRequired\nExample\n\n\n\n\nEffect\nSpecifies whether the statement results in an allow or an explicit deny\n✔\n“Effect”: “Deny”\n\n\nAction\nDescribes the specific actions that will be allowed or denied\n✔\n“Action”: “iam:CreateUser”\n\n\nResource\nSpecifies the object or objects that the statement covers\n✔\n“Resource”: “arn:aws:iam::account-ID-without-hyphens:user/Bob”\n\n\n\n\nSummary Role Based Access in AWS\n\nThroughout these last few lessons, there have been sprinklings of IAM best practices. It’s helpful to have a brief summary of some of the most important IAM best practices you need to be familiar with before building out solutions on AWS.\n\nLock Down the AWS Root User\n\n\nThe root user is an all-powerful and all-knowing identity within your AWS account. If a malicious user were to gain control of root-user credentials, they would be able to access every resource within your account, including personal and billing information. To lock down the root user:\n\n\nDon’t share the credentials associated with the root user.\nConsider deleting the root user access keys.\nEnable MFA on the root account.\n\n\nFollow the Principle of Least Privilege\n\n\nLeast privilege is a standard security principle that advises you to grant only the necessary permissions to do a particular job and nothing more. To implement least privilege for access control, start with the minimum set of permissions in an IAM policy and then grant additional permissions as necessary for a user, group, or role.\n\n\nUse IAM Appropriately\n\n\nIAM is used to secure access to your AWS account and resources. It simply provides a way to create and manage users, groups, and roles to access resources within a single AWS account. IAM is not used for website authentication and authorization, such as providing users of a website with sign-in and sign-up functionality. IAM also does not support security controls for protecting operating systems and networks.\n\n\nUse IAM Roles When Possible\n\n\nMaintaining roles is easier than maintaining users. When you assume a role, IAM dynamically provides temporary credentials that expire after a defined period of time, between 15 minutes and 36 hours. Users, on the other hand, have long-term credentials in the form of user name and password combinations or a set of access keys.\n\nUser access keys only expire when you or the admin of your account rotates these keys. User login credentials expire if you have applied a password policy to your account that forces users to rotate their passwords.\n\nConsider Using an Identity Provider\n\n\nIf you decide to make your cat photo application into a business and begin to have more than a handful of people working on it, consider managing employee identity information through an identity provider (IdP). Using an IdP, whether it be an AWS service such as AWS Single Sign-On (SSO) or a third-party identity provider, provides you a single source of truth for all identities in your organization.\n\nYou no longer have to create separate IAM users in AWS. You can instead use IAM roles to provide permissions to identities that are federated from your IdP.\nFor example, you have an employee, Martha, that has access to multiple AWS accounts. Instead of creating and managing multiple IAM users named Martha in each of those AWS accounts, you can manage Martha in your company’s IdP. If Martha moves within the company or leaves the company, Martha can be updated in the IdP, rather than in every AWS account you have.\n\nConsider AWS SSO\n\n\nIf you have an organization that spans many employees and multiple AWS accounts, you may want your employees to sign in with a single credential. AWS SSO is an IdP that lets your users sign in to a user portal with a single set of credentials. It then provides them access to all their assigned accounts and applications in one central location.\n\nAWS SSO is similar to IAM, in that it offers a directory where you can create users, organize them in groups, and set permissions across those groups, and grant access to AWS resources. However, AWS SSO has some advantages over IAM. For example, if you’re using a third-party IdP, you can sync your users and groups to AWS SSO.\nThis removes the burden of having to re-create users that already exist elsewhere, and it enables you to manage those users from your IdP. More importantly, AWS SSO separates the duties between your IdP and AWS, ensuring that your cloud access management is not inside or dependent on your IdP.\n\nBack to contents\n\n\n\nCompute as a Service on AWS"
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#understanding-servers",
    "href": "posts/others/2020-01-01-AWS.html#understanding-servers",
    "title": "ThomasHSimm",
    "section": "Understanding Servers",
    "text": "Understanding Servers\nThe first building block you need to host an application is a server. Servers often times can handle Hypertext Transfer Protocol (HTTP) requests and send responses to clients following the client-server model, though any API based communication also falls under this model. A client being a person or computer that sends a request, and a server handling the requests is a computer, or collection of computers, connected to the internet serving websites to internet users.\nThese servers power your application by providing CPU, memory, and networking capacity to process users’ requests and transform them into responses. For context, common HTTP servers include:\n\nWindows options, such as Internet Information Services (IIS).\nLinux options, such as Apache HTTP Web Server, Nginx, and Apache Tomcat.\n\nTo run an HTTP server on AWS, you need to find a service that provides compute power in the AWS Management Console. You can log into the console and view the complete list of AWS compute services."
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#choose-the-right-compute-option",
    "href": "posts/others/2020-01-01-AWS.html#choose-the-right-compute-option",
    "title": "ThomasHSimm",
    "section": "Choose the Right Compute Option",
    "text": "Choose the Right Compute Option\nIf you’re responsible for setting up servers on AWS to run your infrastructure, you have many compute options. You need to know which service to use for which use case. At a fundamental level, there are three types of compute options: - virtual machines, e.g. EC2 - container services, - serverless.\nIf you’re coming to AWS with prior infrastructure knowledge, a virtual machine can often be the easiest compute option in AWS to understand. This is because a virtual machine emulates a physical server and allows you to install an HTTP server to run your applications. To run these virtual machines, you install a hypervisor on a host machine. This hypervisor provisions the resources to create and run your virtual machines.\n\nHow does AWS hypervisor work? A hypervisor is a piece of system software that provides virtual machines (VMs), on which users can run their OS and applications. The hypervisor provides isolation between VMs, which run independent of each other, and allows different VMs to run their own OS\n\nIn AWS, these virtual machines are called Amazon Elastic Compute Cloud or Amazon EC2. Behind the scenes, AWS operates and manages the host machines and the hypervisor layer. AWS also installs the virtual machine operating system, called the guest operating system.\nSome AWS compute services use Amazon EC2 or use virtualization concepts under the hood, therefore it is best to understand this service first before moving on to container services and serverless compute.\n\nIntroduction to Amazon Elastic Compute Cloud\n\n\nWhat Is Amazon EC2?\nAmazon EC2 is a web service that provides secure, resizable compute capacity in the cloud. It allows you to provision virtual servers called EC2 instances. Although AWS uses the phrase “web service” to describe it, it doesn’t mean that you are limited to running just web servers on your EC2 instances.\nYou can create and manage these instances through the AWS Management Console, the AWS Command Line Interface (CLI), AWS Software Development Kits (SDKs), or through automation tools and infrastructure orchestration services.\nIn order to create an EC2 instance, you need to define:\n\nHardware specifications, like CPU, memory, network, and storage.\nLogical configurations, like networking location, firewall rules, authentication, and the operating system of your choice.\n\nWhen launching an EC2 instance, the first setting you configure is which operating system you want by selecting an Amazon Machine Image (AMI).\n\n\nWhat Is an Amazon Machine Image (AMI)?\nIn the traditional infrastructure world, the process of spinning up a server consists of installing an operating system from installation disks, installation drives, or installation wizards over the network. In the AWS Cloud, this operating system installation is no longer your responsibility, and is instead built into the AMI that you choose.\nNot only does an AMI let you configure which operating system you want, you can also select storage mappings, the architecture type (such as 32-bit, 64-bit, or 64-bit ARM), and additional software installed.\n\n\nWhat Is the Relationship Between AMIs and EC2 Instances?\nEC2 instances are live instantiations of what is defined in an AMI, much like a cake is a live instantiation of a cake recipe. If you are familiar with software development, you can also see this kind of relationship between a Class and an Object.\nA Class is something you model and define, while an object is something you interact with. In this case, the AMI is how you model and define your instance, while the EC2 instance is the entity you interact with, where you can install your web server, and serve your content to users.\nWhen you launch a new instance, AWS allocates a virtual machine that runs on a hypervisor. Then the AMI you selected is copied to the root device volume, which contains the image used to boot the volume. In the end, you get a server you can connect to and install packages and any additional software. In this case, you install a web server along with the properly configured source code of your employee directory app.\n\nOne advantage of using AMIs is that they are reusable.\nYou might choose a Linux-based AMI and configure the HTTP server, application packages, and any additional software you may need to run your application.\nIf you wanted to create a second EC2 instance with the same configurations, how can you easily do that? One option is to go through the entire instance creation and configuration process and try to match your settings to the first instance. However, this is time consuming and leaves room for human error.\nThe second, better option, is to create an AMI from your running instance and use this AMI to start a new instance. This way, your new instance will have all the same configurations as your current instance, because the configurations set in the AMIs are the same.\n\n\nWhere Can You Find AMIs?\nYou can select an AMI from the following categories.\n\nQuick Start AMIs that are premade by AWS and allow you to get started quickly.\nAWS Marketplace AMIs that provide popular open source and commercial software from third-party vendors.\nMy AMIs that are created from your EC2 instances.\nCommunity AMIs that are provided by the AWS user community.\nBuild your own custom image with EC2 Image Builder.\n\nEach AMI in the AWS Management Console has an AMI ID, which is prefixed by “ami-”, followed by a random hash of numbers and letters. These IDs are unique to each AWS region."
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#amazon-ec2-instance-lifecycle",
    "href": "posts/others/2020-01-01-AWS.html#amazon-ec2-instance-lifecycle",
    "title": "ThomasHSimm",
    "section": "Amazon EC2 Instance Lifecycle",
    "text": "Amazon EC2 Instance Lifecycle\nNow that you know how to select an operating system for your EC2 instance, it’s time to choose other configurations to create your EC2 instance, such as the instance type, network, and storage.\nFor an application like the employee directory application, you need instances with enough capacity to run web servers and process incoming customer requests. Your instance sizing will depend on both the demands of your application and the anticipated size of your user base.\nForecasting server capacity for an on-premises application requires difficult decisions involving significant up-front capital spending, while changes to the allocation of your cloud-based services can be made with a simple API call.\nBecause of AWS’s pay-as-you-go model, you can match your infrastructure capacity to your application’s demand, instead of the other way around.\n\nWhat Makes Up an EC2 Instance?\nEC2 instances are a combination of virtual processors (vCPUs), memory, network, and in some cases, instance storage and graphics processing units (GPUs). When you create an EC2 instance, you need to choose how much you need of each of these components.\nAWS offers a variety of instances that differ based on performance. Some instances provide you with more capacity and others provide less. To get an overview of the capacity details for a particular instance, you should look at the instance type.\nInstance types consist of a prefix identifying the type of workloads they’re optimized for, followed by a size. For example, the instance type c5.large can be broken down into the following elements.\n\nc5 determines the instance family and generation number. Here, the instance belongs to the fifth generation of instances in an instance family that’s optimized for generic computation.\nlarge, which determines the amount of instance capacity.\n\n\n\nWhat Are Instance Families?\n\n\n\n\n\n\n\n\nInstance Family\nDescription\nUse Cases\n\n\n\n\nGeneral purpose\nProvides a balance of compute, memory, and networking resources, and can be used for a variety of workloads.\nScale-out workloads such as web servers, containerized microservices, caching fleets, distributed data stores, and development environments.\n\n\nCompute optimized\nIdeal for compute-bound applications that benefit from high-performance processors.\nHigh-performance web servers, scientific modeling, batch processing, distributed analytics, high-performance computing (HPC), machine/deep learning, ad serving, highly scalable multiplayer gaming.\n\n\nMemory optimized\nDesigned to deliver fast performance for workloads that process large data sets in memory.\nMemory-intensive applications such as high-performance databases, distributed web-scale in-memory caches, mid-size in-memory databases, real-time big-data analytics, and other enterprise applications.\n\n\nAccelerated computing\nUse hardware accelerators or co-processors to perform functions such as floating-point number calculations, graphics processing, or data pattern matching more efficiently than is possible with conventional CPUs.\n3D visualizations, graphics-intensive remote workstations, 3D rendering, application streaming, video encoding, and other server-side graphics workloads.\n\n\nStorage optimized\nDesigned for workloads that require high, sequential read and write access to large data sets on local storage. They are optimized to deliver tens of thousands of low-latency random I/O operations per second (IOPS) to applications that replicate their data across different instances.\nNoSQL databases, such as Cassandra, MongoDB, and Redis, in-memory databases, scale-out transactional databases, data warehousing, Elasticsearch, and analytics.\n\n\n\n\nWhere Does Your EC2 Instance Live?\nBy default, your EC2 instances are placed in a network called the default Amazon Virtual Private Cloud (VPC). This network was created so that you can easily get started with Amazon EC2 without having to learn how to create and configure a VPC.\nAny resource you put inside the default VPC will be public and accessible by the internet, so you shouldn’t place any customer data or private information inside of it.\nOnce you get more comfortable with networking on AWS, you should change this default setting to choose your own custom VPCs and restrict access with additional routing and connectivity mechanisms.\n\n\nArchitect for High Availability\nInside this network, your instance resides in an Availability Zone of your choice. AWS services that are scoped at the Availability Zone level must be architected with high availability in mind.\nWhile EC2 instances are typically reliable, two is better than one, and three is better than two. Specifying the instance size gives you an advantage when designing your architecture because you can use more smaller instances instead of a few larger ones.\nIf your frontend only has a single instance and that instance fails, your application goes down. On the other hand, if your workload is distributed across 10 instances and one fails, you lose only 10 percent of your fleet and your application availability is hardly affected.\nWhen architecting any application for high availability, consider using at least two EC2 instances in two separate Availability Zones.\n\n\nExplore the EC2 Instance Lifecycle\nAn EC2 instance transitions between different states from the moment you create it all the way through to its termination.\n\n\nWhen you launch an instance, it enters the pending state (1). When the instance is pending, billing has not started. At this stage, the instance is preparing to enter the running state. Pending is where AWS performs all actions needed to set up an instance, such as copying the AMI content to the root device and allocating the necessary networking components.\nWhen your instance is running (2), it’s ready to use. This is also the stage where billing begins. As soon as an instance is running, you are then able to take other actions on the instance, such as reboot, terminate, stop, and stop-hibernate.\nWhen you reboot an instance (3), it’s different than performing a stop action and then a start action. Rebooting an instance is equivalent to rebooting an operating system. The instance remains on the same host computer and maintains its public and private IP address, and any data on its instance store.\nIt typically takes a few minutes for the reboot to complete. When you stop and start an instance (4), your instance may be placed on a new underlying physical server. Therefore, you lose any data on the instance store that were on the previous host computer. When you stop an instance, the instance gets a new public IP address but maintains the same private IP address.\nWhen you terminate an instance (5), the instance store are erased, and you lose both the public IP address and private IP address of the machine. Termination of an instance means you can no longer access the machine.\n\n\n\nWhat Is the Difference Between Stop and Stop-Hibernate?\nWhen you stop your instance, it enters the stopping state, and then the stopped state. AWS does not charge usage or data transfer fees for your instance after you stop it, but storage for any Amazon EBS volumes is still charged. While your instance is in the stopped state, you can modify some attributes, like the instance type. When you stop your instance, the data stored in memory (RAM) is lost.\nWhen you stop-hibernate your instance, AWS signals the operating system to perform hibernation (suspend-to-disk), which saves the contents from the instance memory (RAM) to the Amazon EBS root volume.\nConsider a scenario where you build a standard three tier application, where you have web servers, application servers and database servers. Turns out, the application you built becomes extremely popular. To relieve some stress on the database that supports your application, you want to implement a custom backend layer that caches database information in memory (RAM). You decide to run this custom backend caching solution on Amazon EC2.\nIn this scenario, the stop-hibernate feature would be instrumental in persisting storage. It would prevent you from having to manually create scripts to save this RAM data before shutting down the server.\n\n\n\nWhat Makes Up the Pricing?\nTo understand EC2 pricing, let’s decouple the instance price from other services attached to it, such as storage and networking costs. In this unit we refer to the instance cost as the cost associated with the instance in terms of specifications and not the total blended cost of running an instance.\nOnce an instance is launched in your AWS account, the billing usually accrues on a per-second basis. For simplicity of calculation, prices are stated per-hour. For example, if you have an instance running for 5 minutes and 38 seconds during a given month, you only pay for 338 seconds of utilization at the end of the month.\nOne exception to this pricing convention may be third-party AMIs purchased from the AWS Marketplace, which may have a minimum billing of 1 hour. For more details, check out the resources section of this unit.\n\nWhat Are the EC2 Pricing Options?\nOne of the ways to reduce costs with Amazon EC2 is to choose the right pricing option for the way your applications run. There are three main purchasing options for EC2 instances: - on-demand, - reserved, and - spot instances.\n\n\nPay As You Go with On-Demand Instances\nWith On-Demand instances, you pay for compute capacity with no long-term commitments. Billing begins whenever the instance is running, and billing stops when the instance is in a stopped or terminated state. The price per second for a running On-Demand instance is fixed.\nFor applications that require servers to be running all the time, you are less likely to benefit from the On-Demand pricing model, simply because there is no situation where you will need to turn servers off. For example, you might want the web server hosting the frontend of your corporate directory application to be running 24/7 so that users can access the website at any time. Even if there are no users connected to your website, you don’t want to shut down the servers supporting the site in case of potential user activity.\nIn the case when servers cannot be stopped, consider using a Reserved Instance to save on costs.\n\n\nReserve Capacity with Reserved Instances (RIs)\nRIs provide you with a significant discount compared to On-Demand instance pricing. RIs provide a discounted hourly rate and an optional capacity reservation for EC2 instances. You can choose between three payment options: - All Upfront, - Partial Upfront, or - No Upfront.\nYou can select for each of these options either a - 1-year or - 3-year term\nDepending on which option you choose, you are discounted differently.\n\nAll Upfront offers a higher discount than Partial Upfront instances.\nPartial Upfront instances offer a higher discount than No Upfront.\nNo Upfront offers a higher discount than On-Demand.\n\nOn-Demand and No Upfront are similar since both do not require any upfront payment. However, there is a major difference. When you choose an On-Demand instance, you stop paying for the instance when you stop or terminate the instance. When you stop an RI, you still pay for it because you committed to a 1-year or 3-year term.\nReserved Instances are associated with an instance type and an Availability Zone depending on how you reserve it. The discount applied by a Reserved Instance purchase is not directly associated with a specific instance ID, but with an instance type.\n\n\nSave on Costs with Spot Instances\nAnother way of paying for EC2 instances is by using Spot Instances. Amazon EC2 Spot Instances allow you to take advantage of unused EC2 capacity in the AWS Cloud. They are available at up to a 90% discount compared to On-Demand prices.\nWith Spot Instances, you set a limit on how much you would like to pay for the instance hour. This is compared against the current Spot price that AWS determines. If the amount you pay is more than the current Spot price and there is capacity, then you will receive an instance. While they are very promising from the billing perspective, there are some architectural considerations you will need to consider in order to use them effectively.\nOne consideration is that your spot instance may be interrupted. For example, if AWS determines that capacity is no longer available for a particular spot instance or if the Spot price exceeds how much you are willing to pay, AWS will give you a 2-minute warning before it interrupts your instance. That means any application or workload that runs on a Spot instance must be able to be interrupted.\nBecause of this unique consideration, inherently fault-tolerant workloads are typically good candidates to use with Spot instances.\nThese include - big data, - containerized workloads, - continuous integration/continuous delivery (CI/CD), - web servers, - high-performance computing (HPC), - image and media rendering, - or other test and development workloads."
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#steps-to-create-an-ec2-instance",
    "href": "posts/others/2020-01-01-AWS.html#steps-to-create-an-ec2-instance",
    "title": "ThomasHSimm",
    "section": "Steps to create an EC2 instance",
    "text": "Steps to create an EC2 instance\n\nClick on Launch instance\nChoose Amazon Machine Image, click select\nChoose instance type (e.g. t2 micro) and click on config details\nEnter values in Network and Subnet field\nClick checkbox to protect against accidental terminaton\nChoose Share-Run a shared hardware instance tenancy.\nSelect storage type\nReview and Launch\nConfigure security group.\nLaunch\nSelect/create key pair\nLaunch Instances\nView Instance.\n\n\nContainer Services on AWS\n\nAWS offers a broad spectrum of compute offerings that give you the flexibility to choose the right tool for the right job. The three main categories of compute are - virtual machines, - containers, and - serverless.\nThere is no one-size-fits-all service because it depends on your needs.\nThe key is to understand what each option has to offer in order to build a more appropriate cloud architecture for your use case."
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#what-is-a-container",
    "href": "posts/others/2020-01-01-AWS.html#what-is-a-container",
    "title": "ThomasHSimm",
    "section": "What Is a Container?",
    "text": "What Is a Container?\n\nWhat is the container in AWS? Containers provide a standard way to package your application’s code, configurations, and dependencies into a single object. Containers share an operating system installed on the server and run as resource-isolated processes, ensuring quick, reliable, and consistent deployments, regardless of environment\n\nContainers can host a variety of different workloads, including - web applications, - lift and shift migrations, - distributed applications, and - streamlining of development, test, and production environments.\nWith the evolution of the open source software community, containers evolved. Today, containers are used as a solution to problems of traditional compute, including the issue of getting software to run reliably when it moves from one compute environment to another.\nA container is a standardized unit that packages up your code and all of its dependencies. This package is designed to run reliably on any platform, because the container creates its own independent environment. This makes it easy to carry workloads from one place to another, such as from development to production or from on-premises to the cloud."
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#what-is-docker",
    "href": "posts/others/2020-01-01-AWS.html#what-is-docker",
    "title": "ThomasHSimm",
    "section": "What Is Docker?",
    "text": "What Is Docker?\nWhen you hear the word container, you may associate it with Docker. Docker is a popular container runtime that simplifies the management of the entire operating system stack needed for container isolation, including networking and storage. Docker makes it easy to create, package, deploy, and run containers.\n\nWhat Is the Difference Between Containers and VMs?\nContainers share the same operating system and kernel as the host they exist on, whereas virtual machines contain their operating system. Since each virtual machine has to maintain a copy of an operating system, there’s a degree of wasted space.\nA container is more lightweight. They spin up quicker, almost instantly. This difference in startup time becomes instrumental when designing applications that need to scale quickly during input/output (I/O) bursts.\nWhile containers can provide speed, virtual machines offer you the full strength of an operating system and offer more resources, like package installation, a dedicated kernel, and more.\n\n\n\nOrchestrate Containers\nIn AWS, containers run on EC2 instances. For example, you may have a large instance and run a few containers on that instance.\nWhile running one instance is easy to manage, it lacks high availability and scalability. Most companies and organizations run many containers on many EC2 instances across several Availability Zones.\nIf you’re trying to manage your compute at a large scale, you need to know:\n\nHow to place your containers on your instances.\nWhat happens if your container fails.\nWhat happens if your instance fails.\nHow to monitor deployments of your containers.\n\nThis coordination is handled by a container orchestration service. AWS offers two container orchestration services: Amazon Elastic Container Service (ECS) and Amazon Elastic Kubernetes Service (EKS).\n\n\nManage Containers with Amazon Elastic Container Service (Amazon ECS)\n\nAmazon ECS is an end-to-end container orchestration service that allows you to quickly spin up new containers and manage them across a cluster of EC2 instances.\nTo run and manage your containers, you need to install the Amazon ECS Container Agent on your EC2 instances. This agent is open source and responsible for communicating back to the Amazon ECS service about cluster management details. You can run this agent on both Linux and Windows AMIs. An instance with the container agent installed is often called a container instance.\n\nOnce the Amazon ECS container instances are up and running, you can perform actions that include, but are not limited to, - launching and stopping containers, - getting cluster state, - scaling in and out, - scheduling the placement of containers across your cluster, - assigning permissions, and - meeting availability requirements.\nTo prepare your application to run on Amazon ECS, you create a task definition. The task definition is a text file, in JSON format, that describes one or more containers. A task definition is similar to a blueprint that describes the resources you need to run that container, such as CPU, memory, ports, images, storage, and networking information.\nHere is a simple task definition that you can use for your corporate director application. In this example, the runs on the Nginx web server.\n{     \"family\": \"webserver\",     \"containerDefinitions\": [ {         \"name\": \"web\",         \"image\": \"nginx\",         \"memory\": \"100\",         \"cpu\": \"99\"     } ],     \"requiresCompatibilities\": [ \"FARGATE\" ],     \"networkMode\": \"awsvpc\",     \"memory\": \"512\",     \"cpu\": \"256\" }\n\n\nUse Kubernetes with Amazon Elastic Kubernetes Service (Amazon EKS)\nKubernetes is a portable, extensible, open source platform for managing containerized workloads and services. By bringing software development and operations together by design, Kubernetes created a rapidly growing ecosystem that is very popular and well established in the market.\nIf you already use Kubernetes, you can use Amazon EKS to orchestrate these workloads in the AWS Cloud.\nAmazon EKS is conceptually similar to Amazon ECS, but there are some differences.\n\nAn EC2 instance with the ECS Agent installed and configured is called a container instance.\n\nIn Amazon EKS, it is called a worker node.\n\nAn ECS Container is called a task.\n\nIn the Amazon EKS ecosystem, it is called a pod.\n\nWhile Amazon ECS runs on AWS native technology,\n\nAmazon EKS runs on top of Kubernetes.\n\n\nIf you have containers running on Kubernetes and want an advanced orchestration solution that can provide simplicity, high availability, and fine-grained control over your infrastructure, Amazon EKS is the tool for you.\n\nServerless and AWS Lambda\n\n\n\nRemove the Undifferentiated Heavy Lifting\nIf you run your code on Amazon EC2, AWS is responsible for the physical hardware and you are responsible for the logical controls, such as guest operating system, security and patching, networking, security, and scaling.\nIf you run your code in containers on Amazon ECS and Amazon EKS, AWS is responsible for more of the container management, such as deploying containers across EC2 instances and managing the container cluster. However, when running ECS and EKS on EC2, you are still responsible for maintaining the underlying EC2 instances.\nIf you want to deploy your workloads and applications without having to manage any EC2 instances, you can do that on AWS with serverless compute.\n\n\nGo Serverless\n\nWhat is serverless in AWS? A serverless architecture is a way to build and run applications and services without having to manage infrastructure. Your application still runs on servers, but all the server management is done by AWS\n\nEvery definition of serverless mentions four aspects.\n\nNo servers to provision or manage.\nScales with usage.\nYou never pay for idle resources.\nAvailability and fault tolerance are built-in.\n\nWith serverless, spend time on the things that differentiate your application, rather than spending time on ensuring availability, scaling, and managing servers.\nAWS has several serverless compute options, including AWS Fargate and AWS Lambda.\n\n\nExplore Serverless Containers with AWS Fargate\nAmazon ECS and Amazon EKS enable you to run your containers in two modes.\n\nAmazon EC2 mode\nAWS Fargate mode\n\nAWS Fargate is a purpose-built serverless compute engine for containers. Fargate scales and manages the infrastructure, allowing developers to work on what they do best: application development.\nIt achieves this by allocating the right amount of compute, eliminating the need to choose and handle EC2 Instances and cluster capacity and scaling. Fargate supports both Amazon ECS and Amazon EKS architecture and provides workload isolation and improved security by design.\nAWS Fargate abstracts the EC2 instance so you’re not required to manage it. However, with AWS Fargate, you can use all the same ECS primitives, APIs, and AWS integrations. It natively integrates with AWS Identity and Access Management (IAM) and Amazon Virtual Private Cloud (VPC). Having native integration with Amazon VPC allows you to launch Fargate containers inside your network and control connectivity to your applications.\n\n\nRun Your Code on AWS Lambda\nIf you want to deploy your workloads and applications without having to manage any EC2 instances or containers, you can use AWS Lambda.\nAWS Lambda lets you run code without provisioning or managing servers or containers. You can run code for virtually any type of application or backend service, including data processing, real-time stream processing, machine learning, WebSockets, IoT backends, mobile backends, and web apps, like your corporate directory app!\nAWS Lambda requires zero administration from the user. You upload your source code and Lambda takes care of everything required to run and scale your code with high availability. There are no servers to manage, bringing you continuous scaling with subsecond metering and consistent performance.\n\n\nHow Lambda Works\nThere are three primary components of a Lambda function: - the trigger, - code, and - configuration.\nThe code is source code, that describes what the Lambda function should run. This code can be authored in three ways.\n\nYou create the code from scratch.\nYou use a blueprint that AWS provides.\nYou use same code from the AWS Serverless Application Repository, a resource that contains sample applications, such as “hello world” code, Amazon Alexa Skill sample code, image resizing code, video encoding, and more.\n\nWhen you create your Lambda function, you specify the runtime you want your code to run in. There are built-in runtimes such as Python, Node.js, Ruby, Go, Java, .NET Core, or you can implement your Lambda functions to run on a custom runtime.\nThe configuration of a Lambda function consists of information that describes how the function should run. In the configuration, you specify network placement, environment variables, memory, invocation type, permission sets, and other configurations. To dive deeper into these configurations, check out the resources section of this unit.\nTriggers describe when the Lambda function should run. A trigger integrates your Lambda function with other AWS services, enabling you to run your Lambda function in response to certain API calls that occur in your AWS account. This makes you quicker to respond to events in your console without having to perform manual actions.\nAll you need is the what, how, and when of a Lambda function to have functional compute capacity that runs only when you need it to.\nAmazon’s CTO, Werner Vogels, says, “No server is easier to manage than no server.” This quote summarizes the convenience you can have when running serverless solutions, like AWS Fargate and AWS Lambda. In the next unit, you apply all the information you’ve learned about Amazon EC2, Amazon ECS and Amazon EKS, and AWS Fargate and learn the use cases for each service.\n\n\nAWS Lambda function handler\nThe AWS Lambda function handler is the method in your function code that processes events. When your function is invoked, Lambda runs the handler method. When the handler exits or returns a response, it becomes available to handle another event.\nYou can use the following general syntax when creating a function handler in Python:\ndef handler_name(event, context):      ...     return some_value\n\nNaming\nThe Lambda function handler name specified at the time you create a Lambda function is derived from the following: - the name of the file in which the Lambda handler function is located - the name of the Python handler function\nA function handler can be any name; however, the default on the Lambda console is lambda_function.lambda_handler. This name reflects the function name as lambda_handler, and the file where the handler code is stored in lambda_function.py.\nIf you choose a different name for your function handler on the Lambda console, you must update the name on the Runtime settings pane.\n\n\nBilling Granularity\nAWS Lambda lets you run code without provisioning or managing servers, and you pay only for what you use. You are charged for the number of times your code is triggered (requests) and for the time your code executes, rounded up to the nearest 1ms (duration).\nAWS rounds up duration to the nearest millisecond with no minimum execution time.\nWith this pricing, it can be very cost effective to run functions whose execution time is very low, such as functions with durations under 100ms or low latency APIs.\nRead more here: https://aws.amazon.com/blogs/aws/new-for-aws-lambda-1ms-billing-granularity-adds-cost-savings/\n\n\nSource Code\nYou can find a tutorial on creating the AWS Lambda function as well as the code used in the AWS Lambda demo here: https://aws.amazon.com/blogs/compute/resize-images-on-the-fly-with-amazon-s3-aws-lambda-and-amazon-api-gateway/\n\nBack to contents\n\n\n\nNetworking on AWS"
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#what-is-networking",
    "href": "posts/others/2020-01-01-AWS.html#what-is-networking",
    "title": "ThomasHSimm",
    "section": "What Is Networking?",
    "text": "What Is Networking?\nNetworking is how you connect computers around the world and allow them to communicate with one another. In this trail, you’ve already seen a few examples of networking. One is the AWS global infrastructure. AWS has created a network of resources using data centers, Availability Zones, and Regions.\n\nKnow the Networking Basics\nThink about sending a letter. When sending a letter, there are three pieces of information you need.\n\nThe payload or letter inside the envelope.\nThe address of the sender in the From section.\nThe address of the recipient in the To section.\n\nLet’s go further. Each address must contain information such as:\n\nName of sender and recipient\nStreet\nCity\nState or province\nZip, area, or postal code\nCountry\n\nYou need all parts of an address to ensure that your letter gets to its destination. Without the correct address, postal workers are not able to properly deliver the message. In the digital world, computers handle the delivery of messages in a similar way. This is called routing."
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#what-are-ip-addresses",
    "href": "posts/others/2020-01-01-AWS.html#what-are-ip-addresses",
    "title": "ThomasHSimm",
    "section": "What Are IP Addresses?",
    "text": "What Are IP Addresses?\nIn order to properly route your messages to a location, you need an address. Just like each home has a mail address, each computer has an IP address. However, instead of using the combination of street, city, state, zip code, and country, the IP address uses a combination of bits, 0s and 1s.\nHere is an example of a 32-bit address in binary format:\n11000000 10101000 00000001 00011110\nIt’s called 32-bit because you have 32 digits. Feel free to count!\n\nWhat Is IPv4 Notation?\nTypically, you don’t see an IP address in this binary format. Instead, it’s converted into decimal format and noted as an Ipv4 address.\nIn the diagram below, the 32 bits are grouped into groups of 8 bits, also called octets. Each of these groups is converted into decimal format separated by a period.\n11000000 10101000 00000001 00011110\n192.168.1.30\nIn the end, this is what is called an Ipv4 address. This is important to know when trying to communicate to a single computer. But remember, you’re working with a network. This is where CIDR Notation comes in.\n\n\nUse CIDR Notation\n192.168.1.30 is a single IP address. If you wanted to express IP addresses between the range of 192.168.1.0 and 192.168.1.255, how can you do that?\nOne way is by using Classless Inter-Domain Routing (CIDR) notation. CIDR notation is a compressed way of specifying a range of IP addresses. Specifying a range determines how many IP addresses are available to you.\nCIDR notation looks like this:\n192.168.1.0/24\n\n192 is fixed\n168 is fixed\n1 is fixed\n0 is flexible\n\nIt begins with a starting IP address and is separated by a forward slash (the “/” character) followed by a number. The number at the end specifies how many of the bits of the IP address are fixed. In this example, the first 24 bits of the IP address are fixed. The rest are flexible.\n32 total bits subtracted by 24 fixed bits leaves 8 flexible bits. Each of these flexible bits can be either 0 or 1, because they are binary. That means you have two choices for each of the 8 bits, providing 256 IP addresses in that IP range.\nThe higher the number after the /, the smaller the number of IP addresses in your network. For example, a range of 192.168.1.0/24 is smaller than 192.168.1.0/16.\nWhen working with networks in the AWS Cloud, you choose your network size by using CIDR notation. In AWS, the smallest IP range you can have is /28, which provides you 16 IP addresses. The largest IP range you can have is a /16, which provides you with 65,536 IP addresses.\nStanford: Introduction to Computer Networking\nExternal Site: Ionos: CIDR: What is classless inter-domain routing?"
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#introduction-to-amazon-vpc",
    "href": "posts/others/2020-01-01-AWS.html#introduction-to-amazon-vpc",
    "title": "ThomasHSimm",
    "section": "Introduction to Amazon VPC",
    "text": "Introduction to Amazon VPC\nA VPC is an isolated network you create in the AWS cloud, similar to a traditional network in a data center. When you create a VPC, you need to choose three main things.\n\nThe name of your VPC.\nA Region for your VPC to live in. Each VPC spans multiple Availability Zones within the Region you choose.\nA IP range for your VPC in CIDR notation. This determines the size of your network. Each VPC can have up to four /16 IP ranges.\n\nUsing this information, AWS will provision a network and IP addresses for that network.\n\n\nCreate a Subnet\nAfter you create your VPC, you need to create subnets inside of this network.\nThink of subnets as smaller networks inside your base network—or virtual area networks (VLANs) in a traditional, on-premises network. In an on-premises network, the typical use case for subnets is to isolate or optimize network traffic. In AWS, subnets are used for high availability and providing different connectivity options for your resources.\nWhen you create a subnet, you need to choose three settings.\n\nThe VPC you want your subnet to live in, in this case VPC (10.0.0.0/16).\nThe Availability Zone you want your subnet to live in, in this case AZ1.\nA CIDR block for your subnet, which must be a subset of the VPC CIDR block, in this case 10.0.0.0/24.\n\nWhen you launch an EC2 instance, you launch it inside a subnet, which will be located inside the Availability Zone you choose.\n\n\n\nHigh Availability with A VPC\nWhen you create your subnets, keep high availability in mind. In order to maintain redundancy and fault tolerance, create at least two subnets configured in two different Availability Zones.\nAs you learned earlier in the trail, it’s important to consider that “everything fails all the time.” In this case, if one of these AZs fail, you still have your resources in another AZ available as backup.\n\n\n\nReserved IPs\nFor AWS to configure your VPC appropriately, AWS reserves five IP addresses in each subnet. These IP addresses are used for routing, Domain Name System (DNS), and network management.\nFor example, consider a VPC with the IP range 10.0.0.0/22. The VPC includes 1,024 total IP addresses. This is divided into four equal-sized subnets, each with a /24 IP range with 256 IP addresses. Out of each of those IP ranges, there are only 251 IP addresses that can be used because AWS reserves five.\n\nSince AWS reserves these five IP addresses, it can impact how you design your network. A common starting place for those who are new to the cloud is to create a VPC with a IP range of /16 and create subnets with a IP range of /24. This provides a large amount of IP addresses to work with at both the VPC and subnet level.\n\n\nGateways\n\nInternet Gateway\nTo enable internet connectivity for your VPC, you need to create an internet gateway. Think of this gateway as similar to a modem. Just as a modem connects your computer to the internet, the internet gateway connects your VPC to the internet. Unlike your modem at home, which sometimes goes down or offline, an internet gateway is highly available and scalable. After you create an internet gateway, you then need to attach it to your VPC.\n\n\nVirtual Private Gateway\nA virtual private gateway allows you to connect your AWS VPC to another private network. Once you create and attach a VGW to a VPC, the gateway acts as anchor on the AWS side of the connection. On the other side of the connection, you’ll need to connect a customer gateway to the other private network. A customer gateway device is a physical device or software application on your side of the connection. Once you have both gateways, you can then establish an encrypted VPN connection between the two sides.\n\nAmazon VPC Routing and Security"
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#the-main-route-table",
    "href": "posts/others/2020-01-01-AWS.html#the-main-route-table",
    "title": "ThomasHSimm",
    "section": "The Main Route Table",
    "text": "The Main Route Table\nWhen you create a VPC, AWS creates a route table called the main route table. A route table contains a set of rules, called routes, that are used to determine where network traffic is directed. AWS assumes that when you create a new VPC with subnets, you want traffic to flow between them. Therefore, the default configuration of the main route table is to allow traffic between all subnets in the local network. Below is an example of a main route table:\n\n\n\nDestination\nTarget\nStatus\nPropagated\n\n\n\n\n10.2.0.0/16\nlocal\nactive\nno\n\n\n\nThere are two main parts to this route table.\n\nThe destination, which is a range of IP addresses where you want your traffic to go. In the example of sending a letter, you need a destination to route the letter to the appropriate place. The same is true for routing traffic. In this case, the destination is the IP range of our VPC network.\nThe target, which is the connection through which to send the traffic. In this case, the traffic is routed through the local VPC network.\n\n\nCustom Route Tables\nWhile the main route table controls the routing for your VPC, you may want to be more granular about how you route your traffic for specific subnets. For example, your application may consist of a frontend and a database. You can create separate subnets for these resources and provide different routes for each of them.\nIf you associate a custom route table with a subnet, the subnet will use it instead of the main route table. By default, each custom route table you create will have the local route already inside it, allowing communication to flow between all resources and subnets inside the VPC.\n\n\n\nSecure Your Subnets with Network ACLs\nThink of a network ACL as a firewall at the subnet level. A network ACL enables you to control what kind of traffic is allowed to enter or leave your subnet. You can configure this by setting up rules that define what you want to filter. Here’s an example.\n\nInbound\n\n\n\nRule #\nType\nProtocolPort Range\nSource\nAllow/Deny\n\n\n\n\n100\nAll IPv4 traffic\nAll\n0.0.0.0/0\nALLOW\n\n\n*\nAll IPv4 traffic\nAll\n0.0.0.0/0\nDENY\n\n\n\n\n\nOutbound\n\n\n\nRule #\nType\nProtocolPort Range\nSource\nAllow/Deny\n\n\n\n\n100\nAll IPv4 traffic\nAll\n0.0.0.0/0\nALLOW\n\n\n*\nAll IPv4 traffic\nAll\n0.0.0.0/0\nDENY\n\n\n\nThe default network ACL, shown in the table above, allows all traffic in and out of your subnet. To allow data to flow freely to your subnet, this is a good starting place.\nHowever, you may want to restrict data at the subnet level. For example, if you have a web application, you might restrict your network to allow HTTPS traffic and remote desktop protocol (RDP) traffic to your web servers.\n\n\nInbound\n\n\n\n\n\n\n\n\n\n\n\nRule #\nSource IP\nProtocol\nPort\nAllow/Deny\nComments\n\n\n\n\n100\nAll IPv4 traffic\nTCP\n443\nALLOW\nAllows inbound HTTPS traffic from anywhere\n\n\n130\n192.0.2.0/24\nTCP\n3389\nALLOW\nAllows inbound RDP traffic to the web servers from your home network’s public IP address range (over the internet gateway)\n\n\n*\nAll IPv4 traffic\nAll\nAll\nDENY\nDenies all inbound traffic not already handled by a preceding rule (not modifiable)\n\n\n\n\n\nOutbound\n\n\n\n\n\n\n\n\n\n\n\nRule #\nDestination IP\nProtocol\nPort\nAllow/Deny\nComments\n\n\n\n\n120\n0.0.0.0/0\nTCP\n1025-65535\nALLOW\nAllows outbound responses to clients on the internet (serving people visiting the web servers in the subnet)\n\n\n*\n0.0.0.0/0\nAll\nAll\nDENY\nDenies all outbound traffic not already handled by a preceding rule (not modifiable)\n\n\n\nNotice that in the network ACL example above, you allow inbound 443 and outbound range 1025-65535. That’s because HTTP uses port 443 to initiate a connection and will respond to an ephemeral port. Network ACL’s are considered stateless, so you need to include both the inbound and outbound ports used for the protocol. If you don’t include the outbound range, your server would respond but the traffic would never leave the subnet.\nSince network ACLs are configured by default to allow incoming and outgoing traffic, you don’t need to change their initial settings unless you need additional security layers.\n\n\n\nSecure Your EC2 Instances with Security Groups\nThe next layer of security is for your EC2 Instances. Here, you can create a firewall called a security group. The default configuration of a security group blocks all inbound traffic and allows all outbound traffic.\n\nYou might be wondering: “Wouldn’t this block all EC2 instances from receiving the response of any customer requests?” Well, security groups are stateful, meaning they will remember if a connection is originally initiated by the EC2 instance or from the outside and temporarily allow traffic to respond without having to modify the inbound rules.\nIf you want your EC2 instance to accept traffic from the internet, you’ll need to open up inbound ports. If you have a web server, you may need to accept HTTP and HTTPS requests to allow that type of traffic in through your security group. You can create an inbound rule that will allow port 80 (HTTP) and port 443 (HTTPS) as shown below.\n\n\n\nType\nProtocol\nPort Range\nSource\n\n\n\n\nHTTP (80)\nTCP (6)\n80\n0.0.0.0/0\n\n\nHTTP (80)\nTCP (6)\n80\n::/0\n\n\nHTTP (443)\nTCP (6)\n443\n0.0.0.0/0\n\n\nHTTP (443)\nTCP (6)\n443\n::/0\n\n\n\nYou learned in a previous unit that subnets can be used to segregate traffic between computers in your network. Security groups can be used to do the same thing. A common design pattern is organizing your resources into different groups and creating security groups for each to control network communication between them.\n\nThis example allows you to define three tiers and isolate each tier with the security group rules you define. In this case, you only allow internet traffic to the web tier over HTTPS, Web Tier to Application Tier over HTTP, and Application tier to Database tier over MySQL. This is different from traditional on-premises environments, in which you isolate groups of resources via VLAN configuration. In AWS, security groups allow you to achieve the same isolation without tying it to your network.\n\nBack to contents\n\n\n\nStorage Types on AWS\n\n\nAWS storage services are grouped into three different categories: - block storage, e.g. EBS - file storage, e.g. EFS - object storage, e.g. S3"
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#file-storage",
    "href": "posts/others/2020-01-01-AWS.html#file-storage",
    "title": "ThomasHSimm",
    "section": "File Storage",
    "text": "File Storage\nYou may be familiar with file storage if you’ve interacted with file storage systems like Windows File Explorer or Finder on MacOS. You place your files in a tree-like hierarchy that consists of folders and subfolders. For example, if you have hundreds of cat photos on your laptop, you may want to create a folder called Cat photos, and place those images inside that folder to organize them. Since you know these images will be used in an application, you may want to place the cat photos folder inside another folder called Application files.\nEach file has metadata such as file name, file size, and the date the file was created. The file also has a path, for example, computer/Application_files/Cat_photos/cats-03.png. When you need to retrieve a file, your system can use the path to find it in the file hierarchy.\nFile storage is ideal when you require centralized access to files that need to be easily shared and managed by multiple host computers. Typically, this storage is mounted onto multiple hosts and requires file locking and integration with existing file system communication protocols.\nCommon use cases for file storage include:\n\nLarge content repositories\nDevelopment environments\nUser home directories"
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#block-storage",
    "href": "posts/others/2020-01-01-AWS.html#block-storage",
    "title": "ThomasHSimm",
    "section": "Block Storage",
    "text": "Block Storage\nWhile file storage treats files as a singular unit, block storage splits files into fixed-size chunks of data called blocks that have their own addresses. Since each block is addressable, blocks can be retrieved efficiently.\nWhen data is requested, these addresses are used by the storage system to organize the blocks in the correct order to form a complete file to present back to the requestor. Outside of the address, there is no additional metadata associated with each block. So, when you want to change a character in a file, you just change the block, or the piece of the file, that contains the character. This ease of access is why block storage solutions are fast and use less bandwidth.\n\nSince block storage is optimized for low-latency operations, it is a typical storage choice for high-performance enterprise workloads, such as databases or enterprise resource planning (ERP) systems, that require low-latency storage."
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#object-storage",
    "href": "posts/others/2020-01-01-AWS.html#object-storage",
    "title": "ThomasHSimm",
    "section": "Object Storage",
    "text": "Object Storage\nObjects, much like files, are also treated as a single unit of data when stored. However, unlike file storage, these objects are stored in a flat structure instead of a hierarchy. Each object is a file with a unique identifier. This identifier, along with any additional metadata, is bundled with the data and stored.\nChanging just one character in an object is more difficult than with block storage. When you want to change one character in a file, the entire file must be updated.\n\nWith object storage, you can store almost any type of data, and there is no limit to the number of objects stored, making it easy to scale. Object storage is generally useful when storing large data sets, unstructured files like media assets, and static assets, such as photos.\n\nRelate Back to Traditional Storage Systems\nIf you’ve worked with storage on-premises, you may already be familiar with block, file, and object storage. Consider the following technologies and how they relate to systems you may have seen before.\n\nBlock storage in the cloud is analogous to direct-attached storage (DAS) or a storage area network (SAN).\nFile storage systems are often supported with a network attached storage (NAS) server.\n\nAdding more storage in a traditional data center environment is a more rigid process, as you need to purchase, install, and configure these storage solutions. With cloud computing, the process is more flexible. You can create, delete, and modify storage solutions all within a matter of minutes.\n\nAmazon EC2 Instance Storage and Amazon Elastic Block Store"
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#amazon-ec2-instance-store",
    "href": "posts/others/2020-01-01-AWS.html#amazon-ec2-instance-store",
    "title": "ThomasHSimm",
    "section": "Amazon EC2 Instance Store",
    "text": "Amazon EC2 Instance Store\nAmazon EC2 Instance Store provides temporary block-level storage for your instance. This storage is located on disks that are physically attached to the host computer. This ties the lifecycle of your data to the lifecycle of your EC2 instance. If you delete your instance, the instance store is deleted as well. Due to this, instance store is considered ephemeral storage (volatile temporary storage attached to your instances which is only present during the running lifetime of the instance).\nInstance store is ideal if you are hosting applications that replicate data to other EC2 instances, such as Hadoop clusters. For these cluster-based workloads, having the speed of locally attached volumes and the resiliency of replicated data helps you achieve data distribution at high performance. It’s also ideal for temporary storage of information that changes frequently, such as buffers, caches, scratch data, and other temporary content."
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#amazon-elastic-block-storage-amazon-ebs",
    "href": "posts/others/2020-01-01-AWS.html#amazon-elastic-block-storage-amazon-ebs",
    "title": "ThomasHSimm",
    "section": "Amazon Elastic Block Storage (Amazon EBS)",
    "text": "Amazon Elastic Block Storage (Amazon EBS)\nAs the name implies, Amazon EBS is a block-level storage device that you can attach to an Amazon EC2 instance. These storage devices are called Amazon EBS volumes. EBS volumes are essentially drives of a user-configured size attached to an EC2 instance, similar to how you might attach an external drive to your laptop.\nEBS volumes act similarly to external drives in more than one way.\n\nMost Amazon EBS volumes can only be connected with one computer at a time. Most EBS volumes have a one-to-one relationship with EC2 instances, so they cannot be shared by or attached to multiple instances at one time. Note: Recently, AWS announced the Amazon EBS multi-attach feature that enables volumes to be attached to multiple EC2 instances at one time. This feature is not available for all instance types and all instances must be in the same Availability Zone.\nDetach and re-attach. You can detach an EBS volume from one EC2 instance and attach it to another EC2 instance in the same Availability Zone, to access the data on it.\nThe external drive is separate from the computer. That means, if an accident happens and the computer goes down, you still have your data on your external drive. The same is true for EBS volumes.\nYou’re limited to the size of the external drive, since it has a fixed limit to how scalable it can be. For example, you may have a 2 TB external drive and that means you can only have 2 TB of content on there. This relates to EBS as well, since volumes also have a max limitation of how much content you can store on the volume.\n\n\nSteps to create EBS\n\nOpen Amazon Ec2 console\nChoose Volumes from navigation pane\nCreate Volume\nChoose Volume type\nEnter volume size in Gb\nEnter min/max input/output operations per second (IOPS)\nEnter Throughput (MiB/s)\nChoose Availability Zone (has to be same as instance)\nSnapshot ID (keep as default)\nCreate volume\n\n\n\nScale Amazon EBS Volumes\nYou can scale Amazon EBS volumes in two ways.\n\nIncrease the volume size, as long as it doesn’t increase above the maximum size limit. For EBS volumes, the maximum amount of storage you can have is 16 TB. That means if you provision a 5 TB EBS volume, you can choose to increase the size of your volume until you get to 16 TB.\nAttach multiple volumes to a single Amazon EC2 instance. EC2 has a one-to-many relationship with EBS volumes. You can add these additional volumes during or after EC2 instance creation to provide more storage capacity for your hosts.\n\n\n\nAmazon EBS Use Cases\nAmazon EBS is useful when you need to retrieve data quickly and have data persist long-term. Volumes are commonly used in the following scenarios.\n\nOperating systems: Boot/root volumes to store an operating system. The root device for an instance launched from an Amazon Machine Image (AMI) is typically an Amazon EBS volume. These are commonly referred to as EBS-backed AMIs.\nDatabases: A storage layer for databases running on Amazon EC2 that rely on transactional reads and writes.\nEnterprise applications: Amazon EBS provides reliable block storage to run business-critical applications.\nThroughput-intensive applications: Applications that perform long, continuous reads and writes.\n\n\n\nAmazon EBS Volume Types\nThere are two main categories of Amazon EBS volumes: solid-state drives (SSDs) and hard-disk drives (HDDs). SSDs provide strong performance for random input/output (I/O), while HDDs provide strong performance for sequential I/O. AWS offers two types of each.\nThe following chart can help you decide which EBS volume is the right option for your workload.\n\nEBS Provisioned IOPS SSD\n\nDescription\n\nHighest performance SSD designed for latency-sensitive transactional workloads\n\nUse Cases\n\nI/O-intensive NoSQL and relational databases\n\nVolume Size\n\n4 GB-16 TB\n\nMax IOPS/Volume\n\n64,000\n\nMax Throughput/Volume\n\n1,000 MB/s\n\n\n\n\nEBS General Purpose SSD\n\nDescription\n\nGeneral purpose SSD that balances price and performance for a wide variety of transactional workloads\n\nUse Cases\n\nBoot volumes, low-latency interactive apps, development, and test\n\nVolume Size\n\n1 GB-16 TB\n\nMax IOPS/Volume\n\n16,000\n\nMax Throughput/Volume\n\n250 MB/s\n\n\n\n\nThroughput Optimized HDD\n\nDescription\n\nLow-cost HDD designed for frequently accessed, throughput intensive workloads\n\nUse Cases\n\nBig data, data warehouses, log processing\n\nVolume Size\n\n500 GB-16 TB\n\nMax IOPS/Volume\n\n500\n\nMax Throughput/Volume\n\n500 MB/s\n\n\n\n\nCold HDD\n\nDescription\n\nLowest cost HDD designed for less frequently accessed workloads\n\nUse Cases\n\nColder data requiring fewer scans per day\n\nVolume Size\n\n500 GB-16 TB\n\nMax IOPS/Volume\n\n250\n\nMax Throughput/Volume\n\n250 MB/s\n\n\n\n\n\nBenefits of Using Amazon EBS\nHere are the following benefits of using Amazon EBS (in case you need a quick cheat sheet).\n\nHigh availability: When you create an EBS volume, it is automatically replicated within its Availability Zone to prevent data loss from single points of failure.\nData persistence: The storage persists even when your instance doesn’t.\nData encryption: All EBS volumes support encryption.\nFlexibility: EBS volumes support on-the-fly changes. You can modify volume type, volume size, and input/output operations per second (IOPS) capacity without stopping your instance.\nBackups: Amazon EBS provides you the ability to create backups of any EBS volume.\n\n\n\nEBS Snapshots\nErrors happen. One of those errors is not backing up data, and then, inevitably losing that data. To prevent this from happening to you, you should back up your data—even in AWS.\nSince your EBS volumes consist of the data from your Amazon EC2 instance, you’ll want to take backups of these volumes, called snapshots.\nEBS snapshots are incremental backups that only save the blocks on the volume that have changed after your most recent snapshot. For example, if you have 10 GB of data on a volume, and only 2 GB of data have been modified since your last snapshot, only the 2 GB that have been changed are written to Amazon Simple Storage Service (Amazon S3).\nWhen you take a snapshot of any of your EBS volumes, these backups are stored redundantly in multiple Availability Zones using Amazon S3. This aspect of storing the backup in Amazon S3 will be handled by AWS, so you won’t need to interact with Amazon S3 to work with your EBS snapshots. You simply manage them in the EBS console (which is part of the EC2 console).\nEBS snapshots can be used to create multiple new volumes, whether they’re in the same Availability Zone or a different one. When you create a new volume from a snapshot, it’s an exact copy of the original volume at the time the snapshot was taken.\n\nObject Storage with Amazon S3"
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#what-is-amazon-s3",
    "href": "posts/others/2020-01-01-AWS.html#what-is-amazon-s3",
    "title": "ThomasHSimm",
    "section": "What Is Amazon S3?",
    "text": "What Is Amazon S3?\nUnlike Amazon EBS, Amazon S3 is a standalone storage solution that isn’t tied to compute. It enables you to retrieve your data from anywhere on the web. If you’ve ever used an online storage service to back up the data from your local machine, then you most likely have used a service similar to Amazon S3. The big difference between those online storage services and Amazon S3 is the storage type.\nAmazon S3 is an object storage service. Object storage stores data in a flat structure, using unique identifiers to look up objects when requested. An object is simply a file combined with metadata and that you can store as many of these objects as you’d like. All of these characteristics of object storage are also characteristics of Amazon S3.\n\nUnderstand Amazon S3 Concepts\nIn Amazon S3, you have to store your objects in containers called buckets. You can’t upload any object, not even a single photo, to S3 without creating a bucket first. When you create a bucket, you choose, at the very minimum, two things: the bucket name and the AWS Region you want the bucket to reside in.\nThe first part is choosing the Region you want the bucket to reside in. Typically, this will be a Region that you’ve used for other resources, such as your compute. When you choose a Region for your bucket, all objects you put inside that bucket are redundantly stored across multiple devices, across multiple Availability Zones. This level of redundancy is designed to provide Amazon S3 customers with 99.999999999% durability and 99.99% availability for objects over a given year.\nThe second part is choosing a bucket name which must be unique across all AWS accounts. AWS stops you from choosing a bucket name that has already been chosen by someone else in another AWS account. Once you choose a name, that name is yours and cannot be claimed by anyone else unless you delete that bucket, which then releases the name for others to use.\n\nAWS uses this name as part of the object identifier. In S3, each object is identified using a URL, which looks like this:\nAfter the http://, you see the bucket name. In this example, the bucket is named doc. Then, the identifier uses the service name, s3 and specifies the service provider amazonaws. After that, you have an implied folder inside the bucket called 2006-03-01 and the object inside the folder that is named AmazonS3.html. The object name is often referred to as the key name.\nNote, you can have folders inside of buckets to help you organize objects. However, remember that there’s no actual file hierarchy that supports this on the back end. It is instead a flat structure where all files and folders live at the same level. Using buckets and folders implies a hierarchy, which makes it easy to understand for the human eye.\n\n\nS3 Use Cases\nAmazon S3 is one of the most widely used storage services, with far more use cases than could fit on one screen. The following list summarizes some of the most common ways you can use Amazon S3.\n\nBackup and storage: S3 is a natural place to back up files because it is highly redundant. As mentioned in the last unit, AWS stores your EBS snapshots in S3 to take advantage of its high availability.\nMedia hosting: Because you can store unlimited objects, and each individual object can be up to 5 TBs, S3 is an ideal location to host video, photo, or music uploads.\nSoftware delivery: You can use S3 to host your software applications that customers can download.\nData lakes: S3 is an optimal foundation for a data lake because of its virtually unlimited scalability. You can increase storage from gigabytes to petabytes of content, paying only for what you use.\nStatic websites: You can configure your bucket to host a static website of HTML, CSS, and client-side scripts.\nStatic content: Because of the limitless scaling, the support for large files, and the fact that you access any object over the web at any time, S3 is the perfect place to store static content.\n\n\n\nChoose the Right Connectivity Option for Your Resources\nEverything in Amazon S3 is private by default. This means that all S3 resources, such as buckets, folders, and objects can only be viewed by the user or AWS account that created that resource. Amazon S3 resources are all private and protected to begin with.\nIf you decide that you want everyone on the internet to see your photos, you can choose to make your buckets, folders, and objects public. Keep in mind that a public resource means that everyone on the internet can see it. Most of the time, you don’t want your permissions to be all or nothing. Typically, you want to be more granular about the way you provide access to your resources.\n\nTo be more specific about who can do what with your S3 resources, Amazon S3 provides two main access management features: IAM policies and S3 bucket policies.\n\n\nUnderstand IAM Policies\nPreviously, you learned about creating and using IAM policies, and now you get to apply this to Amazon S3. When IAM policies are attached to IAM users, groups, and roles, the policies define which actions they can perform. IAM policies are not tied to any one AWS service and can be used to define access to nearly any AWS action.\nYou should use IAM policies for private buckets when:\n\nYou have many buckets with different permission requirements. Instead of defining many different S3 bucket policies, you can use IAM policies instead.\nYou want all policies to be in a centralized location. Using IAM policies allows you to manage all policy information in one location.\n\n\n\nUnderstand S3 Bucket Policies\nS3 bucket policies are similar to IAM policies, in that they are both defined using the same policy language in a JSON format. The difference is IAM policies are attached to users, groups, and roles, whereas S3 bucket policies are only attached to buckets. S3 bucket policies specify what actions are allowed or denied on the bucket.\nFor example, if you have a bucket called employeebucket, you can attach an S3 bucket policy to it that allows another AWS account to put objects in that bucket.\nOr if you wanted to allow anonymous viewers to read the objects in employeebucket, then you can apply a policy to that bucket that allows anyone to read objects in the bucket using \"Effect\":Allow on the \"Action:[\"s3:GetObject\"]\".\nHere’s an example of what that S3 bucket policy might look like. {     \"Version\":\"2012-10-17\",         \"Statement\":[{             \"Sid\":\"PublicRead\",             \"Effect\":\"Allow\",             \"Principal\": \"*\",             \"Action\":[\"s3:GetObject\"],             \"Resource\":[\"arn:aws:s3:::employeebucket/*\"]         }] }\nS3 Bucket policies can only be placed on buckets, and cannot be used for folders or objects. However, the policy that is placed on the bucket applies to every object in that bucket.\nYou should use S3 bucket policies when:\n\nYou need a simple way to do cross-account access to S3, without using IAM roles.\nYour IAM policies bump up against the defined size limit. S3 bucket policies have a larger size limit."
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#encrypt-s3",
    "href": "posts/others/2020-01-01-AWS.html#encrypt-s3",
    "title": "ThomasHSimm",
    "section": "Encrypt S3",
    "text": "Encrypt S3\nAmazon S3 reinforces encryption in transit (as it travels to and from Amazon S3) and at rest. To protect data at rest, you can use:\n\nServer-side encryption: This allows Amazon S3 to encrypt your object before saving it on disks in its data centers and then decrypt it when you download the objects.\nClient-side encryption: Encrypt your data client-side and upload the encrypted data to Amazon S3. In this case, you manage the encryption process, the encryption keys, and all related tools.\n\nTo encrypt in transit, you can use client-side encryption or Secure Sockets Layer (SSL).\n\nUse Versioning to Preserve Objects\nAs you know, Amazon S3 identifies objects in part by using the object name. For example, when you upload an employee photo to S3, you may name the object employee.jpg and store it in a folder called employees. If you don’t use Amazon S3 versioning, anytime you upload an object called employee.jpg to the employees folder, it overwrites the original file. This can be an issue for several reasons.\n\nemployee.jpg is a common name for an employee photo object. You or someone else who has access to that bucket might not have intended to overwrite it, and now that you have, you no longer have access to the original file.\nYou may want to preserve different versions of employee.jpg. Without versioning, if you wanted to create a new version of employee.jpg, you would need to upload the object and choose a different name for it. Having several objects all with slight differences in naming variations may cause confusion and clutter in your bucket.\n\nSo, what do you do? You use S3 versioning!\nVersioning enables you to keep multiple versions of a single object in the same bucket. This allows you to preserve old versions of an object without having to use different naming constructs, in case you need to recover from accidental deletions, accidental overwrites, or even application failures. Let’s see how this works.\n\nIf you enable versioning for a bucket, Amazon S3 automatically generates a unique version ID for the object being stored. In one bucket, for example, you can have two objects with the same key, but different version IDs, such as employeephoto.gif (version 111111) and employeephoto.gif (version 121212).\nVersioning-enabled buckets let you recover objects from accidental deletion or overwrite.\n\nDeleting an object does not remove the object permanently. Instead, Amazon S3 puts a marker on the object that shows you tried to delete it. If you want to restore the object, you can remove this marker and it reinstates the object.\nIf you overwrite an object, it results in a new object version in the bucket. You still have access to previous versions of the object.\n\n\n\nUnderstand Versioning States\nBuckets can be in one of three states.\n\nUnversioned (the default): No new or existing objects in the bucket have a version.\nVersioning-enabled: This enables versioning for all objects in the bucket.\nVersioning-suspended: This suspends versioning for new objects. All new objects in the bucket will not have a version. However, all existing objects keep their object versions.\n\nThe versioning state applies to all of the objects in that bucket. Keep in mind that storage costs are incurred for all objects in your bucket and all versions of those objects. To reduce your S3 bill, you may want to delete previous versions of your objects that are no longer in use."
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#what-are-amazon-s3-storage-classes",
    "href": "posts/others/2020-01-01-AWS.html#what-are-amazon-s3-storage-classes",
    "title": "ThomasHSimm",
    "section": "What Are Amazon S3 Storage Classes?",
    "text": "What Are Amazon S3 Storage Classes?\nWhen you upload an object to Amazon S3 and you don’t specify the storage class, you’re uploading it to the default storage class—often referred to as standard storage. When you learned about Amazon S3 in previous units, you were learning about the standard storage class without even knowing it!\nS3 storage classes let you change your storage tier as your data characteristics change. For example, if you are now accessing your old photos infrequently, you may want to change the storage class those photos are stored in to save on costs.\nThere are six S3 storage classes.\n\nAmazon S3 Standard: This is considered general purpose storage for cloud applications, dynamic websites, content distribution, mobile and gaming applications, and big data analytics.\nAmazon S3 Intelligent-Tiering: This tier is useful if your data has unknown or changing access patterns. S3 Intelligent-Tiering stores objects in two tiers, a frequent access tier and an infrequent access tier. Amazon S3 monitors access patterns of your data, and automatically moves your data to the most cost-effective storage tier based on frequency of access.\nAmazon S3 Standard-Infrequent Access (S3 Standard-IA): S3 Standard-IA is for data that is accessed less frequently, but requires rapid access when needed. S3 Standard-IA offers the high durability, high throughput, and low latency of S3 Standard, with a low per-GB storage price and per-GB retrieval fee. This storage tier is ideal if you want to store long-term backups, disaster recovery files, and so on.\nAmazon S3 One Zone-Infrequent Access (S3 One Zone-IA): Unlike other S3 storage classes which store data in a minimum of three Availability Zones (AZs), S3 One Zone-IA stores data in a single AZ and costs 20% less than S3 Standard-IA. S3 One Zone-IA is ideal for customers who want a lower-cost option for infrequently accessed data but do not require the availability and resilience of S3 Standard or S3 Standard-IA. It’s a good choice for storing secondary backup copies of on-premises data or easily re-creatable data.\nAmazon S3 Glacier: S3 Glacier is a secure, durable, and low-cost storage class for data archiving. You can reliably store any amount of data at costs that are competitive with or cheaper than on-premises solutions. To keep costs low yet suitable for varying needs, S3 Glacier provides three retrieval options that range from a few minutes to hours.\nAmazon S3 Glacier Deep Archive: S3 Glacier Deep Archive is Amazon S3’s lowest-cost storage class and supports long-term retention and digital preservation for data that may be accessed once or twice in a year. It is designed for customers—particularly those in highly regulated industries, such as the Financial Services, Healthcare, and Public Sectors—that retain data sets for 7 to 10 years or longer to meet regulatory compliance requirements.\n\n\nAutomate Tier Transitions with Object Lifecycle Management\nIf you keep manually changing your objects, such as your employee photos, from storage tier to storage tier, you may want to look into automating this process using a lifecycle policy. When you define a lifecycle policy configuration for an object or group of objects, you can choose to automate two actions: transition and expiration actions.\n\nTransition actions are used to define when you should transition your objects to another storage class.\nExpiration actions define when objects expire and should be permanently deleted.\n\nFor example, you might choose to transition objects to S3 Standard-IA storage class 30 days after you created them, or archive objects to the S3 Glacier storage class one year after creating them.\n\nThe following use cases are good candidates for lifecycle management.\n\nPeriodic logs: If you upload periodic logs to a bucket, your application might need them for a week or a month. After that, you might want to delete them.\nData that changes in access frequency: Some documents are frequently accessed for a limited period of time. After that, they are infrequently accessed. At some point, you might not need real-time access to them, but your organization or regulations might require you to archive them for a specific period. After that, you can delete them.\n\n\nChoose the Right Storage Service\n\nHere’s a recap of all the storage services mentioned so far. By the end of this reading, you should be able to better answer the question “Which storage service should I use?” for some of the more common scenarios."
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#amazon-ec2-instance-store-1",
    "href": "posts/others/2020-01-01-AWS.html#amazon-ec2-instance-store-1",
    "title": "ThomasHSimm",
    "section": "Amazon EC2 Instance Store",
    "text": "Amazon EC2 Instance Store\nInstance store is ephemeral block storage. This is preconfigured storage that exists on the same physical server that hosts the EC2 instance and cannot be detached from Amazon EC2. You can think of it as a built-in drive for your EC2 instance.\nInstance store is generally well-suited for temporary storage of information that is constantly changing, such as buffers, caches, and scratch data. It is not meant for data that is persistent or long-lasting.\nIf you need persistent long-term block storage that can be detached from Amazon EC2 and provide you more management flexibility, such as increasing volume size or creating snapshots, then you should use Amazon EBS."
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#amazon-ebs",
    "href": "posts/others/2020-01-01-AWS.html#amazon-ebs",
    "title": "ThomasHSimm",
    "section": "Amazon EBS",
    "text": "Amazon EBS\nAmazon EBS is meant for data that changes frequently and needs to persist through instance stops, terminations, or hardware failures. Amazon EBS has two different types of volumes: SSD-backed volumes and HDD-backed volumes.\nSSD-backed volumes have the following characteristics.\n\nPerformance depends on IOPS (input/output operations per second).\nIdeal for transactional workloads such as databases and boot volumes.\n\nHDD-backed volumes have the following characteristics:\n\nPerformance depends on MB/s.\nIdeal for throughput-intensive workloads, such as big data, data warehouses, log processing, and sequential data I/O.\n\nHere are a few important features of Amazon EBS that you need to know when comparing it to other services.\n\nIt is block storage.\nYou pay for what you provision (you have to provision storage in advance).\nEBS volumes are replicated across multiple servers in a single Availability Zone.\nMost EBS volumes can only be attached to a single EC2 instance at a time."
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#amazon-s3",
    "href": "posts/others/2020-01-01-AWS.html#amazon-s3",
    "title": "ThomasHSimm",
    "section": "Amazon S3",
    "text": "Amazon S3\nIf your data doesn’t change that often, Amazon S3 might be a more cost-effective and scalable storage solution. S3 is ideal for storing static web content and media, backups and archiving, data for analytics, and can even be used to host entire static websites with custom domain names.\nHere are a few important features of Amazon S3 to know about when comparing it to other services.\n\nIt is object storage.\nYou pay for what you use (you don’t have to provision storage in advance).\nAmazon S3 replicates your objects across multiple Availability Zones in a Region.\nAmazon S3 is not storage attached to compute."
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#amazon-elastic-file-system-amazon-efs-and-amazon-fsx",
    "href": "posts/others/2020-01-01-AWS.html#amazon-elastic-file-system-amazon-efs-and-amazon-fsx",
    "title": "ThomasHSimm",
    "section": "Amazon Elastic File System (Amazon EFS) and Amazon FSx",
    "text": "Amazon Elastic File System (Amazon EFS) and Amazon FSx\nIn this module, you’ve already learned about Amazon S3 and Amazon EBS. You learned that S3 uses a flat namespace and isn’t meant to serve as a standalone file system. You also learned most EBS volumes can only be attached to one EC2 instance at a time. So, if you need file storage on AWS, which service should you use?\nFor file storage that can mount on to multiple EC2 instances, you can use Amazon Elastic File System (Amazon EFS) or Amazon FSx. Use the following table for more information about each of these services.\n\n\n\n\n\n\n\n\nService\nCharacteristic\nMore Information\n\n\n\n\nAmazon Elastic File System (EFS)\nFully managed NFS file system.\nEFS FAQs\n\n\nAmazon FSx for Windows File Server\nFully managed file server built on Windows Server that supports the SMB protocol.\nFSx for Windows File Server FAQs\n\n\nAmazon FSx for Lustre\nFully managed Lustre file system that integrates with S3.\nFSx for Lustre FAQs\n\n\n\nHere are a few important features of Amazon EFS and FSx to know about when comparing them to other services.\n\nIt is file storage.\nYou pay for what you use (you don’t have to provision storage in advance).\nAmazon EFS and Amazon FSx can be mounted onto multiple EC2 instances.\n\n\n\nExplore Databases on AWS"
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#understanding-the-history-behind-enterprise-databases",
    "href": "posts/others/2020-01-01-AWS.html#understanding-the-history-behind-enterprise-databases",
    "title": "ThomasHSimm",
    "section": "Understanding the History Behind Enterprise Databases",
    "text": "Understanding the History Behind Enterprise Databases\nChoosing a database used to be a straightforward decision. There were only a few options to choose from. In the past, you likely considered a few vendors and then inevitably chose one for all of your applications.\nBusinesses often selected the database technology they were going to use, even before they fully understood their use case. Since the 1970s, the database most commonly selected by businesses was a relational database.\n\nWhat Is a Relational Database?\nA relational database organizes data into tables. Data in one table can be linked to data in other tables to create relationships—hence, the relational part of the name.\nA table stores data in rows and columns. A row, often called a record, contains all information about a specific entry. Columns describe attributes of that entry. Here’s an example of three tables in a relational database.\n\nThis shows a table for books, a table for sales, and a table for authors. In the books table, each row includes the book ISBN, the title, the author, and the format. Each of these attributes is stored in its own column. The books table has something in common with the other two tables: the author attribute. That common column creates a relationship between the tables.\nThe tables, rows, columns, and relationships between them is referred to as a logical schema. With relational databases, a schema is fixed. Once the database is operational, it becomes difficult to change the schema. This requires most of the data modeling to be done upfront before the database is active.\n\n\nWhat Is a Relational Database Management System?\nA relational database management system (RDBMS) lets you create, update, and administer a relational database. Here are some common examples of relational database management systems:\n\nMySQL\nPostgresQL\nOracle\nSQL server\nAmazon Aurora\n\nYou communicate with most RDBMS by using Structured Query Language (SQL) queries. Here’s an example: SELECT * FROM table_name.\nThis query selects all of the data from a particular table. However, the real power of SQL queries is in creating more complex queries that let you pull data from several tables to piece together patterns and answers to business problems. For example, querying the sales table and the book table together to see sales in relation to an author’s books. This is made possible by a join, which we talk about next.\n\n\nThe Benefits of Using a Relational Database\nThere are many benefits to using a relational database. A few of them are listed here.\n\nJoins: You can join tables, enabling you to better understand relationships between your data.\nReduced redundancy: You can store data in one table and reference it from other tables instead of saving the same data in different places.\nFamiliarity: Relational databases have been a popular choice since the 1970s. Due to this popularity, technical professionals often have familiarity and experience with this type of database.\nAccuracy: Relational databases ensure that your data is persisted with high integrity and adheres to the ACID (atomicity, consistency, isolation, durability) principle.\n\n\n\nUse Cases for Relational Databases\nMuch of the world runs on relational databases. In fact, they’re at the core of many mission-critical applications, some of which you may use in your day to day life. Here are some common use cases for relational databases.\nApplications that have a solid schema that doesn’t change often, such as:\n\nLift and shift applications that lifts an app from on-premises and shifts it to the cloud, with little or no modifications.\n\nApplications that need persistent storage that follows the ACID principle, such as:\n\nEnterprise Resource Planning (ERP) applications\nCustomer Relationship Management (CRM) applications\nCommerce and financial applications\n\n\n\nChoose Between Unmanaged and Managed Databases\nIf you want to run a relational database on AWS, you first need to select how you want to run it: the unmanaged way or the managed way.\nThe paradigm of managed versus unmanaged services is similar to the Shared Responsibility Model. The Shared Responsibility Model distinguishes between AWS’s and the customer’s security responsibility over a service. Similarly, managed versus unmanaged can be understood as a tradeoff between convenience and control.\n\n\nOn-Premises Database\nLet’s say you operate a relational database on-premises (in your own data center). In this scenario, you are responsible for all aspects of operation, including the security and electricity of the data center, the management of the host machine, the management of the database on that host, as well as optimizing queries and managing customer data. You are responsible for absolutely everything, which means you have control over absolutely everything.\n\n\nUnmanaged Database\n\nNow, let’s say you want to shift some of this work to AWS by running your relational database on Amazon EC2. If you host a database on Amazon EC2, AWS takes care of implementing and maintaining the physical infrastructure and hardware and installing the operating system of the EC2 instance. However, you’re still responsible for managing the EC2 instance, managing the database on that host, optimizing queries, and managing customer data.\nThis is what is often referred to as the unmanaged database option on AWS. AWS is responsible for and has control over the hardware and underlying infrastructure, and you are responsible and have control over management of the host and database.\n\n\nManaged Database\n\nIf you want to shift even more of the work to AWS, you can use a managed database service. These services provide the setup of both the EC2 instance and the database, and they provide systems for high availability, scalability, patching, and backups. However, you’re still responsible for database tuning, query optimization, and of course, ensuring that your customer data is secure. This provides you ultimate convenience, but you have the least amount of control compared to the two previous options."
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#what-is-amazon-rds",
    "href": "posts/others/2020-01-01-AWS.html#what-is-amazon-rds",
    "title": "ThomasHSimm",
    "section": "What Is Amazon RDS?",
    "text": "What Is Amazon RDS?\nAmazon RDS enables you to create and manage relational databases in the cloud without the operational burden of traditional database management. For example, if you sell healthcare equipment and your goal is to be the number-one seller in the Pacific Northwest, building out a database doesn’t directly help you achieve that goal though having a database is necessary to achieve the goal.\nAmazon RDS helps you offload some of this unrelated work of creating and managing a database. You can focus on the tasks that differentiate your application, instead of infrastructure-related tasks such as provisioning, patching, scaling, and restoring.\nAmazon RDS supports most of the popular relational database management systems, ranging from commercial options, open source options, and even an AWS-specific option. Here are the supported Amazon RDS engines.\n\nCommercial: Oracle, SQL Server\nOpen Source: MySQL, PostgreSQL, MariaDB\nCloud Native: Amazon Aurora\n\nNote: The cloud native option, Amazon Aurora, is a MySQL and PostgreSQL-compatible database built for the cloud. It is more durable, more available, and provides faster performance than the Amazon RDS version of MySQL and PostgreSQL.\n\nUnderstand DB Instances\nJust like the databases that you would build and manage yourself, Amazon RDS is built off of compute and storage. The compute portion is called the DB (database) instance, which runs the database engine. Depending on the engine of the DB instance you choose, the engine will have different supported features and configurations. A DB instance can contain multiple databases with the same engine, and each database can contain multiple tables.\nUnderneath the DB instance is an EC2 instance. However, this instance is managed through the Amazon RDS console instead of the Amazon EC2 console. When you create your DB instance, you choose the instance type and size. Amazon RDS supports three instance families.\n\nStandard, which include general-purpose instances\nMemory Optimized, which are optimized for memory-intensive applications\nBurstable Performance, which provides a baseline performance level, with the ability to burst to full CPU usage.\n\nThe DB instance you choose affects how much processing power and memory it has. Not all of the options are available to you, depending on the engine that you choose.\nMuch like a regular EC2 instance, the DB instance uses Amazon Elastic Block Store (EBS) volumes as its storage layer. You can choose between three types of EBS volume storage.\n\nGeneral purpose (SSD)\nProvisioned IOPS (SSD)\nMagnetic storage (not recommended)\n\n\n\nWork with Amazon RDS in an Amazon Virtual Private Cloud\nWhen you create a DB instance, you select the Amazon Virtual Private Cloud (VPC) that your databases will live in. Then, you select the subnets that you want the DB instances to be placed in. This is referred to as a DB subnet group. To create a DB subnet group, you specify:\n\nThe Availability Zones (AZs) that include the subnets you want to add\nThe subnets in that AZ where your DB instance are placed\n\nThe subnets you add should be private so they don’t have a route to the internet gateway. This ensures your DB instance, and the cat data inside of it, can only be reached by the app backend.\nAccess to the DB instance can be further restricted by using network access control lists (ACLs) and security groups. With these firewalls, you can control, at a granular level, what type of traffic you want to allow into your database. Using these controls provide layers of security for your infrastructure. It reinforces that only the backend instances have access to the database.\n\n\nUse AWS Identity and Access Management (IAM) Policies to Secure Amazon RDS\nNetwork ACLs and security groups allow you to dictate the flow of traffic. If you want to restrict what actions and resources your employees can access, you can use IAM policies."
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#back-up-your-data",
    "href": "posts/others/2020-01-01-AWS.html#back-up-your-data",
    "title": "ThomasHSimm",
    "section": "Back Up Your Data",
    "text": "Back Up Your Data\nYou don’t want to lose any of that precious cat information. To take regular backups of your RDS instance, you can use:\n\nAutomatic backups\nManual snapshots\n\n\nAutomatic Backups\nAutomated backups are turned on by default. This backs up your entire DB instance (not just individual databases on the instance), and your transaction logs. When you create your DB instance, you set a backup window that is the period of time that automatic backups occur. Typically, you want to set these windows during a time when your database experiences little activity because it can cause increased latency and downtime.\nYou can retain your automated backups between 0 and 35 days. You might ask yourself, “Why set automated backups for 0 days?” The 0 days setting actually disables automatic backups from happening. Keep in mind that if you set it to 0, it will also delete all existing automated backups. This is not ideal, as the benefit of having automated backups is having the ability to do point-in-time recovery.\nIf you restore data from an automated backup, you have the ability to do point-in-time recovery. Point-in-time recovery creates a new DB instance using data restored from a specific point in time. This restoration method provides more granularity by restoring the full backup and rolling back transactions up to the specified time range.\n\n\nManual Snapshots\nIf you want to keep your automated backups longer than 35 days, use manual snapshots. Manual snapshots are similar to taking EBS snapshots, except you manage them in the RDS console. These are backups that you can initiate at any time, that exist until you delete them.\nFor example, to meet a compliance requirement that mandates you to keep database backups for a year, you would need to use manual snapshots to ensure those backups are retained for that period of time.\nIf you restore data from a manual snapshot, it creates a new DB instance using the data from the snapshot.\n\n\nWhich Backup Option Should I Use?\nThe answer, almost always, is both. Automated backups are beneficial for the point-in-time recovery. Manual snapshots allow you to retain backups for longer than 35 days.\n\n\nGet Redundancy with Amazon RDS Multi-AZ\nWhen you enable Amazon RDS Multi-AZ, Amazon RDS creates a redundant copy of your database in another AZ. You end up with two copies of your database: a primary copy in a subnet in one AZ and a standby copy in a subnet in a second AZ.\nThe primary copy of your database provides access to your data so that applications can query and display that information.\nThe data in the primary copy is synchronously replicated to the standby copy. The standby copy is not considered an active database, and does not get queried by applications.\nTo improve availability, Amazon RDS Multi-AZ ensures that you have two copies of your database running and that one of them is in the primary role. If there’s an availability issue, such as the primary database losing connectivity, Amazon RDS triggers an automatic failover.\nWhen you create a DB instance, a domain name system (DNS) name is provided. AWS uses that DNS name to failover to the standby database. In an automatic failover, the standby database is promoted to the primary role and queries are redirected to the new primary database.\nTo ensure that you don’t lose Multi-AZ configuration, a new standby database is created by either:\n\nDemoting the previous primary to standby if it’s still up and running\nOr standing up a new standby DB instance\n\nThe reason you can select multiple subnets for an Amazon RDS database is because of the Multi-AZ configuration. You’ll want to ensure that you have used subnets in different AZs for your primary and standby copies."
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#what-is-amazon-dynamodb",
    "href": "posts/others/2020-01-01-AWS.html#what-is-amazon-dynamodb",
    "title": "ThomasHSimm",
    "section": "What Is Amazon DynamoDB?",
    "text": "What Is Amazon DynamoDB?\nAmazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. DynamoDB lets you offload the administrative burdens of operating and scaling a distributed database so that you don’t have to worry about hardware provisioning, setup and configuration, replication, software patching, or cluster scaling.\nWith DynamoDB, you can create database tables that can store and retrieve any amount of data and serve any level of request traffic. You can scale up or scale down your tables’ throughput capacity without downtime or performance degradation. You can use the AWS Management Console to monitor resource utilization and performance metrics.\nDynamoDB automatically spreads the data and traffic for your tables over a sufficient number of servers to handle your throughput and storage requirements, while maintaining consistent and fast performance. All of your data is stored on solid-state disks (SSDs) and is automatically replicated across multiple Availability Zones in an AWS Region, providing built-in high availability and data durability."
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#core-components-of-amazon-dynamodb",
    "href": "posts/others/2020-01-01-AWS.html#core-components-of-amazon-dynamodb",
    "title": "ThomasHSimm",
    "section": "Core Components of Amazon DynamoDB",
    "text": "Core Components of Amazon DynamoDB\nIn DynamoDB, tables, items, and attributes are the core components that you work with. A table is a collection of items, and each item is a collection of attributes. DynamoDB uses primary keys to uniquely identify each item in a table and secondary indexes to provide more querying flexibility.\nThe following are the basic DynamoDB components:\nTables – Similar to other database systems, DynamoDB stores data in tables. A table is a collection of data. For example, see the example table called People that you could use to store personal contact information about friends, family, or anyone else of interest. You could also have a Cars table to store information about vehicles that people drive.\nItems – Each table contains zero or more items. An item is a group of attributes that is uniquely identifiable among all of the other items. In a People table, each item represents a person. For a Cars table, each item represents one vehicle. Items in DynamoDB are similar in many ways to rows, records, or tuples in other database systems. In DynamoDB, there is no limit to the number of items you can store in a table.\nAttributes – Each item is composed of one or more attributes. An attribute is a fundamental data element, something that does not need to be broken down any further. For example, an item in a People table contains attributes called PersonID, LastName, FirstName, and so on. For a Department table, an item might have attributes such as DepartmentID, Name, Manager, and so on. Attributes in DynamoDB are similar in many ways to fields or columns in other database systems.\n\nSecurity with Amazon DynamoDB\nDynamoDB also offers encryption at rest, which eliminates the operational burden and complexity involved in protecting sensitive data. For more information, see DynamoDB Encryption at Rest."
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#aws-database-services",
    "href": "posts/others/2020-01-01-AWS.html#aws-database-services",
    "title": "ThomasHSimm",
    "section": "AWS Database Services",
    "text": "AWS Database Services\nAWS has a variety of different database options for different use cases. Use the table below to get a quick look at the AWS database portfolio.\n\n\n\n\n\n\n\n\nDatabase Type\nUse Cases\nAWS Service\n\n\n\n\nRelational\nTraditional applications, ERP, CRM, e-commerce\nAmazon RDS, Amazon Aurora, Amazon Redshift\n\n\nKey-value\nHigh-traffic web apps, e-commerce systems, gaming applications\nAmazon DynamoDB\n\n\nIn-memory\nCaching, session management, gaming leaderboards, geospatial applications\nAmazon ElastiCache for Memcached, Amazon ElastiCache for Redis\n\n\nDocument\nContent management, catalogs, user profiles\nAmazon DocumentDB (with MongoDB compatibility)\n\n\nWide column\nHigh-scale industrial apps for equipment maintenance, fleet management, and route optimization\nAmazon Keyspaces (for Apache Cassandra)\n\n\nGraph\nFraud detection, social networking, recommendation engines\nAmazon Neptune\n\n\nTime series\nIoT applications, DevOps, industrial telemetry\nAmazon Timestream\n\n\nLedger\nSystems of record, supply chain, registrations, banking transactions\nAmazon QLDB"
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#breaking-up-applications-and-databases",
    "href": "posts/others/2020-01-01-AWS.html#breaking-up-applications-and-databases",
    "title": "ThomasHSimm",
    "section": "Breaking Up Applications and Databases",
    "text": "Breaking Up Applications and Databases\nAs the industry changes, applications and databases change too. Today, with larger applications, you no longer see just one database supporting it. Instead, these applications are being broken into smaller services, each with their own purpose-built database supporting it.\nThis shift removes the idea of a one-size-fits-all database and replaces it with a complementary database strategy. You can give each database the appropriate functionality, performance, and scale that the workload requires.\n\n\nMonitoring on AWS\n\n\nWhen operating a website like the Employee Directory Application on AWS you may have questions like:\n\nHow many people are visiting my site day to day?\nHow can I track the number of visitors over time?\nHow will I know if the website is having performance or availability issues?\nWhat happens if my Amazon Elastic Compute Cloud (EC2) instance runs out of capacity?\nWill I be alerted if my website goes down?\n\nYou need a way to collect and analyze data about the operational health and usage of your resources. The act of collecting, analyzing, and using data to make decisions or answer questions about your IT resources and systems is called monitoring.\nMonitoring enables you to have a near real-time pulse on your system and answer the questions listed above. You can use the data you collect to watch for operational issues caused by events like over-utilization of resources, application flaws, resource misconfiguration, or security-related events.\nThink of the data collected through monitoring as outputs of the system, or metrics."
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#use-metrics-to-solve-problems",
    "href": "posts/others/2020-01-01-AWS.html#use-metrics-to-solve-problems",
    "title": "ThomasHSimm",
    "section": "Use Metrics to Solve Problems",
    "text": "Use Metrics to Solve Problems\nThe resources that host your solutions on AWS all create various forms of data that you might be interested in collecting. You can think of each individual data point that is created by a resource as a metric. Metrics that are collected and analyzed over time become statistics, like the example of average CPU utilization over time.\nConsider this: One way to evaluate the health of an Amazon EC2 instance is through CPU utilization. Generally speaking, if an EC2 instance has a high CPU utilization, it can mean a flood of requests. Or it can reflect a process that has encountered an error and is consuming too much of the CPU. When analyzing CPU utilization, take a process that exceeds a specific threshold for an unusual length of time. Use that abnormal event as a cue to either manually or automatically resolve the issue through actions like scaling the instance.\nThis is one example of a metric. Other examples of metrics EC2 instances have are - network utilization, - disk performance, - memory utilization, and - the logs created by the applications running on top of EC2."
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#know-the-different-types-of-metrics",
    "href": "posts/others/2020-01-01-AWS.html#know-the-different-types-of-metrics",
    "title": "ThomasHSimm",
    "section": "Know the Different Types of Metrics",
    "text": "Know the Different Types of Metrics\nDifferent resources in AWS create different types of metrics. An Amazon Simple Storage Service (S3) bucket would not have CPU utilization like an EC2 instance does. Instead, S3 creates metrics related to - the objects stored in a bucket like the overall size - the number of objects in a bucket - requests made to the bucket such as reading or writing objects.\nAmazon Relational Database Service (RDS) creates metrics such as - database connections, - CPU utilization of an instance, - or disk space consumption.\nThis is not a complete list for any of the services mentioned, but you can see how different resources create different metrics.\nYou could be interested in a wide variety of metrics depending on the types of resources you are using, the goals you have, or the types of questions you want answered."
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#understand-the-benefits-of-monitoring",
    "href": "posts/others/2020-01-01-AWS.html#understand-the-benefits-of-monitoring",
    "title": "ThomasHSimm",
    "section": "Understand the Benefits of Monitoring",
    "text": "Understand the Benefits of Monitoring\nMonitoring gives you visibility into your resources, but the question now is, “Why is that important?” The following are some of the benefits of monitoring.\nRespond to operational issues proactively before your end users are aware of them. It’s a bad practice to wait for end users to let you know your application is experiencing an outage. Through monitoring, you can keep tabs on metrics like error response rate or request latency, over time, that help signal that an outage is going to occur. This enables you to automatically or manually perform actions to prevent the outage from happening—fixing the problem before your end users are aware of it.\nImprove the performance and reliability of your resources. Monitoring the different resources that comprise your application provides you with a full picture of how your solution behaves as a system. Monitoring, if done well, can illuminate bottlenecks and inefficient architectures. This enables you to drive performance and reliability improvement processes.\nRecognize security threats and events. When you monitor resources, events, and systems over time, you create what is called a baseline. A baseline defines what activity is normal. Using a baseline, you can spot anomalies like unusual traffic spikes or unusual IP addresses accessing your resources. When an anomaly occurs, an alert can be sent out or an action can be taken to investigate the event.\nMake data-driven decisions for your business. Monitoring is not only to keep an eye on IT operational health. It also helps drive business decisions. For example, let’s say you launched a new feature for your cat photo app, and want to know whether it’s being used. You can collect application-level metrics and view the number of users who use the new feature. With your findings, you decide whether to invest more time into improving the new feature.\nCreate more cost-effective solutions. Through monitoring, you can view resources that are being underutilized and rightsize your resources to your usage. This helps you optimize cost and make sure you aren’t spending more money than necessary."
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#enable-visibility",
    "href": "posts/others/2020-01-01-AWS.html#enable-visibility",
    "title": "ThomasHSimm",
    "section": "Enable Visibility",
    "text": "Enable Visibility\nAWS resources create data you can monitor through metrics, logs, network traffic, events, and more. This data is coming from components that are distributed in nature, which can lead to difficulty in collecting the data you need if you don’t have a centralized place to review it all. AWS has already done that for you with a service called Amazon CloudWatch.\nAmazon CloudWatch is a monitoring and observability service that collects data like those mentioned in this module. CloudWatch provides actionable insights into your applications, and enables you to respond to system-wide performance changes, optimize resource utilization, and get a unified view of operational health. This unified view is important.\nYou can use CloudWatch to:\n\nDetect anomalous behavior in your environments.\nSet alarms to alert you when something’s not right.\nVisualize logs and metrics with the AWS Management Console.\nTake automated actions like scaling.\nTroubleshoot issues.\nDiscover insights to keep your applications healthy."
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#how-cloudwatch-works",
    "href": "posts/others/2020-01-01-AWS.html#how-cloudwatch-works",
    "title": "ThomasHSimm",
    "section": "How CloudWatch Works",
    "text": "How CloudWatch Works\nThe great thing about CloudWatch is that all you need to get started is an AWS account. It is a managed service, which enables you to focus on monitoring, without managing any underlying infrastructure.\nThe employee directory app is built with various AWS services working together as building blocks. It would be difficult to monitor all of these different services independently, so CloudWatch acts as one centralized place where metrics are gathered and analyzed. You already learned how EC2 instances post CPU utilization as a metric to CloudWatch. Different AWS resources post different metrics that you can monitor. You can view a list of services that send metrics to CloudWatch in the resources section of this unit.\nMany AWS services send metrics automatically for free to CloudWatch at a rate of one data point per metric per 5-minute interval, without you needing to do anything to turn on that data collection. This by itself gives you visibility into your systems without you needing to spend any extra money to do so. This is known as basic monitoring. For many applications, basic monitoring does the job.\nFor applications running on EC2 instances, you can get more granularity by posting metrics every minute instead of every 5 minutes using a feature like detailed monitoring. Detailed monitoring has an extra fee associated."
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#break-down-metrics",
    "href": "posts/others/2020-01-01-AWS.html#break-down-metrics",
    "title": "ThomasHSimm",
    "section": "Break Down Metrics",
    "text": "Break Down Metrics\nEach metric in CloudWatch has a timestamp and is organized into containers called namespaces. Metrics in different namespaces are isolated from each other—you can think of them as belonging to different categories.\nAWS services that send data to CloudWatch attach dimensions to each metric. A dimension is a name/value pair that is part of the metric’s identity. You can use dimensions to filter the results that CloudWatch returns. For example, you can get statistics for a specific EC2 instance by specifying the InstanceId dimension when you search."
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#set-up-custom-metrics",
    "href": "posts/others/2020-01-01-AWS.html#set-up-custom-metrics",
    "title": "ThomasHSimm",
    "section": "Set Up Custom Metrics",
    "text": "Set Up Custom Metrics\nLet’s say for your application you wanted to record the number of page views your website gets. How would you record this metric to CloudWatch? It’s an application-level metric, meaning that it’s not something the EC2 instance would post to CloudWatch by default. This is where custom metrics come in. Custom metrics allows you to publish your own metrics to CloudWatch.\nIf you want to gain more granular visibility, you can use high-resolution custom metrics, which enable you to collect custom metrics down to a 1-second resolution. This means you can send one data point per second per custom metric.\nOther examples of custom metrics are:\n\nWeb page load times\nRequest error rates\nNumber of processes or threads on your instance\nAmount of work performed by your application\n\nNote: You can get started with custom metrics by programmatically sending the metric to CloudWatch using the PutMetricData API."
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#understand-the-cloudwatch-dashboards",
    "href": "posts/others/2020-01-01-AWS.html#understand-the-cloudwatch-dashboards",
    "title": "ThomasHSimm",
    "section": "Understand the CloudWatch Dashboards",
    "text": "Understand the CloudWatch Dashboards\nOnce you’ve provisioned your AWS resources and they are sending metrics to CloudWatch, you can then visualize and review that data using the CloudWatch console with dashboards. Dashboards are customizable home pages that you use for data visualization for one or more metrics through the use of widgets, such as a graph or text.\nYou can build many custom dashboards, each one focusing on a distinct view of your environment. You can even pull data from different Regions into a single dashboard in order to create a global view of your architecture.\nCloudWatch aggregates statistics according to the period of time that you specify when creating your graph or requesting your metrics. You can also choose whether your metric widgets display live data. Live data is data published within the last minute that has not been fully aggregated.\nYou are not bound to using CloudWatch exclusively for all your visualization needs. You can use external or custom tools to ingest and analyze CloudWatch metrics using the GetMetricData API.\nAs far as security goes, you can control who has access to view or manage your CloudWatch dashboards through AWS Identity and Access Management (IAM) policies that get associated with IAM users, IAM groups, or IAM roles.\n\nGet to Know CloudWatch Logs\nCloudWatch can also be the centralized place for logs to be stored and analyzed, using CloudWatch Logs. CloudWatch Logs can monitor, store, and access your log files from applications running on Amazon EC2 instances, AWS Lambda functions, and other sources.\nCloudWatch Logs allows you to query and filter your log data. For example, let’s say you’re looking into an application logic error for your application, and you know that when this error occurs it will log the stack trace. Since you know it logs the error, you query your logs in CloudWatch Logs to find the stack trace. You also set up metric filters on logs, which turn log data into numerical CloudWatch metrics that you graph and use on your dashboards.\nSome services are set up to send log data to CloudWatch Logs with minimal effort, like AWS Lambda. With AWS Lambda, all you need to do is give the Lambda function the correct IAM permissions to post logs to CloudWatch Logs. Other services require more configuration. For example, if you want to send your application logs from an EC2 instance into CloudWatch Logs, you need to first install and configure the CloudWatch Logs agent on the EC2 instance.\nThe CloudWatch Logs agent enables Amazon EC2 instances to automatically send log data to CloudWatch Logs. The agent includes the following components.\n\nA plug-in to the AWS Command Line Interface (CLI) that pushes log data to CloudWatch Logs.\nA script that initiates the process to push data to CloudWatch Logs.\nA cron job that ensures the daemon is always running.\n\nAfter the agent is installed and configured, you can then view your application logs in CloudWatch Logs.\n\n\nLearn the CloudWatch Logs Terminology\nLog data sent to CloudWatch Logs can come from different sources, so it’s important you understand how they’re organized and the terminology used to describe your logs.\nLog event: A log event is a record of activity recorded by the application or resource being monitored, and it has a timestamp and an event message.\nLog stream: Log events are then grouped into log streams, which are sequences of log events that all belong to the same resource being monitored. For example, logs for an EC2 instance are grouped together into a log stream that you can then filter or query for insights.\nLog groups: Log streams are then organized into log groups. A log group is composed of log streams that all share the same retention and permissions settings. For example, if you have multiple EC2 instances hosting your application and you are sending application log data to CloudWatch Logs, you can group the log streams from each instance into one log group. This helps keep your logs organized."
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#configure-a-cloudwatch-alarm",
    "href": "posts/others/2020-01-01-AWS.html#configure-a-cloudwatch-alarm",
    "title": "ThomasHSimm",
    "section": "Configure a CloudWatch Alarm",
    "text": "Configure a CloudWatch Alarm\nYou can create CloudWatch alarms to automatically initiate actions based on sustained state changes of your metrics. You configure when alarms are triggered and the action that is performed.\nYou first need to decide what metric you want to set up an alarm for, then you define the threshold at which you want the alarm to trigger. Next, you define the specified time period of which the metric should cross the threshold for the alarm to be triggered.\nFor example, if you wanted to set up an alarm for an EC2 instance to trigger when the CPU utilization goes over a threshold of 80%, you also need to specify the time period the CPU utilization is over the threshold. You don’t want to trigger an alarm based on short temporary spikes in the CPU. You only want to trigger an alarm if the CPU is elevated for a sustained amount of time, for example if it is over 80% for 5 minutes or longer, when there is a potential resource issue.\nKeeping all that in mind, to set up an alarm you need to choose the metric, the threshold, and the time period.\nAn alarm has three possible states.\n\nOK: The metric is within the defined threshold. Everything appears to be operating like normal.\nALARM: The metric is outside of the defined threshold. This could be an operational issue.\nINSUFFICIENT_DATA: The alarm has just started, the metric is not available, or not enough data is available for the metric to determine the alarm state.\n\nAn alarm can be triggered when it transitions from one state to another. Once an alarm is triggered, it can initiate an action. Actions can be an Amazon EC2 action, an Auto Scaling action, or a notification sent to Amazon Simple Notification Service (SNS).\n\nUse CloudWatch Alarms to Prevent and Troubleshoot Issues\nCloudWatch Logs uses metric filters to turn the log data into metrics that you can graph or set an alarm on. For the employee directory application, let’s say you set up a metric filter for 500-error response codes.\nThen, you define an alarm for that metric that will go into the ALARM state if 500-error responses go over a certain amount for a sustained time period. Let’s say if it’s more than five 500-error responses per hour, the alarm should enter the ALARM state. Next, you define an action that you want to take place when the alarm is triggered.\nIn this case, it makes sense to send an email or text alert to you so you can start troubleshooting the website, hopefully fixing it before it becomes a bigger issue. Once the alarm is set up, you feel comfortable knowing that if the error happens again, you’ll be notified promptly.\nYou can set up different alarms for different reasons to help you prevent or troubleshoot operational issues. In the scenario just described, the alarm triggered an SNS notification that went to a person who looked into the issue manually. Another option is to have alarms trigger actions that automatically remediate technical issues.\nFor example, you can set up an alarm to trigger an EC2 instance to reboot, or scale services up or down. You can even set up an alarm to trigger an SNS notification, which then triggers an AWS Lambda function. The Lambda function then calls any AWS API to manage your resources, and troubleshoot operational issues. By using AWS services together like this, you respond to events more quickly."
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#what-is-availability",
    "href": "posts/others/2020-01-01-AWS.html#what-is-availability",
    "title": "ThomasHSimm",
    "section": "What Is Availability?",
    "text": "What Is Availability?\nThe availability of a system is typically expressed as a percentage of uptime in a given year or as a number of nines. Below, you can see a list of the percentages of availability based on the downtime per year, as well as its notation in nines.\n\n\n\nAvailability (%)\nDowntime (per year)\n\n\n\n\n90% (“one nine”)\n36.53 days\n\n\n99% (“two nines”)\n3.65 days\n\n\n99.9% (“three nines”)\n8.77 hours\n\n\n99.95% (“three and a half nines”)\n4.38 hours\n\n\n99.99% (“four nines”)\n52.60 minutes\n\n\n99.995% (“four and a half nines”)\n26.30 minutes\n\n\n99.999% (“five nines”)\n5.26 minutes\n\n\n\nTo increase availability, you need redundancy. This typically means more infrastructure: - more data centers, - more servers, - more databases, and - more replication of data.\nYou can imagine that adding more of this infrastructure means a higher cost. Customers want the application to always be available, but you need to draw a line where adding redundancy is no longer viable in terms of revenue."
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#improve-application-availability",
    "href": "posts/others/2020-01-01-AWS.html#improve-application-availability",
    "title": "ThomasHSimm",
    "section": "Improve Application Availability",
    "text": "Improve Application Availability\nIn the current application, there is only one EC2 instance used to host the application, the photos are served from Amazon Simple Storage Service (S3) and the structured data is stored in Amazon DynamoDB. That single EC2 instance is a single point of failure for the application.\nEven if the database and S3 are highly available, customers have no way to connect if the single instance becomes unavailable. One way to solve this single point of failure issue is by adding one more server."
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#use-a-second-availability-zone",
    "href": "posts/others/2020-01-01-AWS.html#use-a-second-availability-zone",
    "title": "ThomasHSimm",
    "section": "Use a Second Availability Zone",
    "text": "Use a Second Availability Zone\nThe physical location of that server is important. On top of having software issues at the operating system or application level, there can be a hardware issue. It could be in the physical server, the rack, the data center or even the Availability Zone hosting the virtual machine. An easy way to fix the physical location issue is by deploying a second EC2 instance in a different Availability Zone.\nThat would also solve issues with the operating system and the application. However, having more than one instance brings new challenges."
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#manage-replication-redirection-and-high-availability",
    "href": "posts/others/2020-01-01-AWS.html#manage-replication-redirection-and-high-availability",
    "title": "ThomasHSimm",
    "section": "Manage Replication, Redirection, and High Availability",
    "text": "Manage Replication, Redirection, and High Availability\n\nCreate a Process for Replication\nThe first challenge is that you need to create a process to replicate the configuration files, software patches, and application itself across instances. The best method is to automate where you can.\n\n\nAddress Customer Redirection\nThe second challenge is how to let the clients, the computers sending requests to your server, know about the different servers. There are different tools that can be used here. The most common is using a Domain Name System (DNS) where the client uses one record which points to the IP address of all available servers. However, the time it takes to update that list of IP addresses and for the clients to become aware of such change, sometimes called propagation, is typically the reason why this method isn’t always used.\nAnother option is to use a load balancer which takes care of health checks and distributing the load across each server. Being between the client and the server, the load balancer avoids propagation time issues. We discuss load balancers later.\n\n\nUnderstand the Types of High Availability\nThe last challenge to address when having more than one server is the type of availability you need—either be an active-passive or an active-active system.\n\nActive-Passive: With an active-passive system, only one of the two instances is available at a time. One advantage of this method is that for stateful applications where data about the client’s session is stored on the server, there won’t be any issues as the customers are always sent to the same server where their session is stored.\nActive-Active: A disadvantage of active-passive and where an active-active system shines is scalability. By having both servers available, the second server can take some load for the application, thus allowing the entire system to take more load. However, if the application is stateful, there would be an issue if the customer’s session isn’t available on both servers. Stateless applications work better for active-active systems."
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#whats-a-load-balancer",
    "href": "posts/others/2020-01-01-AWS.html#whats-a-load-balancer",
    "title": "ThomasHSimm",
    "section": "What’s a Load Balancer?",
    "text": "What’s a Load Balancer?\nLoad balancing refers to the process of distributing tasks across a set of resources. In the case of the corporate directory application, the resources are EC2 instances that host the application, and the tasks are the different requests being sent. It’s time to distribute the requests across all the servers hosting the application using a load balancer.\nTo do this, you first need to enable the load balancer to take all of the traffic and redirect it to the backend servers based on an algorithm. The most popular algorithm is round-robin, which sends the traffic to each server one after the other.\nA typical request for the application would start from the browser of the client. It’s sent to a load balancer. Then, it’s sent to one of the EC2 instances that hosts the application. The return traffic would go back through the load balancer and back to the client browser. Thus, the load balancer is directly in the path of the traffic.\nAlthough it is possible to install your own software load balancing solution on EC2 instances, AWS provides a service for that called Elastic Load Balancing (ELB)."
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#features-of-elb",
    "href": "posts/others/2020-01-01-AWS.html#features-of-elb",
    "title": "ThomasHSimm",
    "section": "Features of ELB",
    "text": "Features of ELB\nThe ELB service provides a major advantage over using your own solution to do load balancing, in that you don’t need to manage or operate it. It can distribute incoming application traffic across EC2 instances as well as containers, IP addresses, and AWS Lambda functions.\n\nThe fact that ELB can load balance to IP addresses means that it can work in a hybrid mode as well, where it also load balances to on-premises servers.\nELB is highly available. The only option you have to ensure is that the load balancer is deployed across multiple Availability Zones.\nIn terms of scalability, ELB automatically scales to meet the demand of the incoming traffic. It handles the incoming traffic and sends it to your backend application."
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#health-checks",
    "href": "posts/others/2020-01-01-AWS.html#health-checks",
    "title": "ThomasHSimm",
    "section": "Health Checks",
    "text": "Health Checks\nTaking the time to define an appropriate health check is critical. Only verifying that the port of an application is open doesn’t mean that the application is working. It also doesn’t mean that simply making a call to the home page of an application is the right way either.\nFor example, the employee directory application depends on a database, and S3. The health check should validate all of those elements. One way to do that would be to create a monitoring webpage like “/monitor” that will make a call to the database to ensure it can connect and get data, and make a call to S3. Then, you point the health check on the load balancer to the “/monitor” page.\n\nAfter determining the availability of a new EC2 instance, the load balancer starts sending traffic to it. If ELB determines that an EC2 instance is no longer working, it stops sending traffic to it and lets EC2 Auto Scaling know. EC2 Auto Scaling’s responsibility is to remove it from the group and replace it with a new EC2 instance. Traffic only sends to the new instance if it passes the health check.\nIn the case of a scale down action that EC2 Auto Scaling needs to take due to a scaling policy, it lets ELB know that EC2 instances will be terminated. ELB can prevent EC2 Auto Scaling from terminating the EC2 instance until all connections to that instance end, while preventing any new connections. That feature is called connection draining."
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#elb-components",
    "href": "posts/others/2020-01-01-AWS.html#elb-components",
    "title": "ThomasHSimm",
    "section": "ELB Components",
    "text": "ELB Components\nThe ELB service is made up of three main components.\n\n\nListeners: The client connects to the listener. This is often referred to as client-side. To define a listener, a port must be provided as well as the protocol, depending on the load balancer type. There can be many listeners for a single load balancer.\nTarget groups: The backend servers, or server-side, is defined in one or more target groups. This is where you define the type of backend you want to direct traffic to, such as EC2 Instances, AWS Lambda functions, or IP addresses. Also, a health check needs to be defined for each target group.\nRules: To associate a target group to a listener, a rule must be used. Rules are made up of a condition that can be the source IP address of the client and a condition to decide which target group to send the traffic to.\n\n\nApplication Load Balancer (ALB)\nHere are some primary features of Application Load Balancer (ALB).\nALB routes traffic based on request data. It makes routing decisions based on the HTTP protocol like the URL path (/upload) and host, HTTP headers and method, as well as the source IP address of the client. This enables granular routing to the target groups.\nSend responses directly to the client. ALB has the ability to reply directly to the client with a fixed response like a custom HTML page. It also has the ability to send a redirect to the client which is useful when you need to redirect to a specific website or to redirect the request from HTTP to HTTPS, removing that work from your backend servers.\nALB supports TLS offloading. Speaking of HTTPS and saving work from backend servers, ALB understands HTTPS traffic. To be able to pass HTTPS traffic through ALB, an SSL certificate is provided by either importing a certificate via Identity and Access Management (IAM) or AWS Certificate Manager (ACM) services, or by creating one for free using ACM. This ensures the traffic between the client and ALB is encrypted.\nAuthenticate users. On the topic of security, ALB has the ability to authenticate the users before they are allowed to pass through the load balancer. ALB uses the OpenID Connect protocol and integrates with other AWS services to support more protocols like SAML, LDAP, Microsoft AD, and more.\nSecure traffic. To prevent traffic from reaching the load balancer, you configure a security group to specify the supported IP address ranges.\nALB uses the round-robin routing algorithm. ALB ensures each server receives the same number of requests in general. This type of routing works for most applications.\nALB uses the least outstanding request routing algorithm. If the requests to the backend vary in complexity where one request may need a lot more CPU time than another, then the least outstanding request algorithm is more appropriate. It’s also the right routing algorithm to use if the targets vary in processing capabilities. An outstanding request is when a request is sent to the backend server and a response hasn’t been received yet.\nFor example, if the EC2 instances in a target group aren’t the same size, one server’s CPU utilization will be higher than the other if the same number of requests are sent to each server using the round-robin routing algorithm. That same server will have more outstanding requests as well. Using the least outstanding request routing algorithm would ensure an equal usage across targets.\nALB has sticky sessions. In the case where requests need to be sent to the same backend server because the application is stateful, then use the sticky session feature. This feature uses an HTTP cookie to remember across connections which server to send the traffic to.\nFinally, ALB is specifically for HTTP and HTTPS traffic. If your application uses a different protocol, then consider the Network Load Balancer (NLB)."
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#network-load-balancer",
    "href": "posts/others/2020-01-01-AWS.html#network-load-balancer",
    "title": "ThomasHSimm",
    "section": "Network Load Balancer",
    "text": "Network Load Balancer\nHere are some primary features of Network Load Balancer (NLB).\nNetwork Load Balancer supports TCP, UDP, and TLS protocols. HTTPS uses TCP and TLS as protocol. However, NLB operates at the connection layer, so it doesn’t understand what a HTTPS request is. That means all features discussed above that are required to understand the HTTP and HTTPS protocol, like routing rules based on that protocol, authentication, and least outstanding request routing algorithm, are not available with NLB.\nNLB uses a flow hash routing algorithm. The algorithm is based on: The protocol\n\nThe source IP address and source port\nThe destination IP address and destination port\nThe TCP sequence number\n\nIf all of these parameters are the same, then the packets are sent to the exact same target. If any of them are different in the next packets, then the request may be sent to a different target.\nNLB has sticky sessions. Different from ALB, these sessions are based on the source IP address of the client instead of a cookie.\nNLB supports TLS offloading. NLB understands the TLS protocol. It can also offload TLS from the backend servers similar to how ALB works.\nNLB handles millions of requests per second. While ALB can also support this number of requests, it needs to scale to reach that number. This takes time. NLB can instantly handle this amount of requests.\nNLB supports static and elastic IP addresses. There are some situations where the application client needs to send requests directly to the load balancer IP address instead of using DNS. For example, this is useful if your application can’t use DNS or if the connecting clients require firewall rules based on IP addresses. In this case, NLB is the right type of load balancer to use.\nNLB preserves source IP address. NLB preserves the source IP address of the client when sending the traffic to the backend. With ALB, if you look at the source IP address of the requests, you will find the IP address of the load balancer. While with NLB, you would see the real IP address of the client, which is required by the backend application in some cases.\n\nSelect Between ELB Types\nSelecting between the ELB service types is done by determining which feature is required for your application. Below you can find a list of the major features that you learned in this unit and the previous.\n\n\n\n\n\n\n\n\nFeature\nApplication Load Balancer\nNetwork Load Balancer\n\n\n\n\nProtocols\nHTTP, HTTPS\n\n\n\nConnection draining (deregistration delay)\n✔\n\n\n\nIP addresses as targets\n✔\n✔\n\n\nStatic IP and Elastic IP address\n\n✔\n\n\nPreserve Source IP address\n\n✔\n\n\nRouting based on Source IP address, path, host, HTTP headers, HTTP method, and query string\n✔\n\n\n\nRedirects\n✔\n\n\n\nFixed response\n✔\n\n\n\nUser authentication\n✔"
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#vertical-scaling",
    "href": "posts/others/2020-01-01-AWS.html#vertical-scaling",
    "title": "ThomasHSimm",
    "section": "Vertical Scaling",
    "text": "Vertical Scaling\nIf there are too many requests sent to a single active-passive system, the active server will become unavailable and hopefully failover to the passive server. But this doesn’t solve anything.\nWith active-passive, you need vertical scaling. This means increasing the size of the server. With EC2 instances, you select either a larger type or a different instance type. This can only be done while the instance is in a stopped state.\nIn this scenario, the following steps occur:\n\nStop the passive instance. This doesn’t impact the application since it’s not taking any traffic.\nChange the instance size or type, then start the instance again.\nShift the traffic to the passive instance, turning it active.\nThe last step is to stop, change the size, and start the previous active instance as both instances should match.\n\nWhen the amount of requests reduces, the same operation needs to be done. Even though there aren’t that many steps involved, it’s actually a lot of manual work to do. Another disadvantage is that a server can only scale vertically up to a certain limit.\nOnce that limit is reached, the only option is to create another active-passive system and split the requests and functionalities across them. This could require massive application rewriting.\nThis is where the active-active system can help. When there are too many requests, this system can be scaled horizontally by adding more servers."
  },
  {
    "objectID": "posts/others/2020-01-01-AWS.html#horizontal-scaling",
    "href": "posts/others/2020-01-01-AWS.html#horizontal-scaling",
    "title": "ThomasHSimm",
    "section": "Horizontal Scaling",
    "text": "Horizontal Scaling\nAs mentioned above, for the application to work in an active-active system, it’s already created as stateless, not storing any client session on the server. This means that having two servers or having four wouldn’t require any application changes. It would only be a matter of creating more instances when required and shutting them down when the traffic decreases.\nThe Amazon EC2 Auto Scaling service can take care of that task by automatically creating and removing EC2 instances based on metrics from Amazon CloudWatch.\nYou can see that there are many more advantages to using an active-active system in comparison with an active-passive. Modifying your application to become stateless enables scalability.\n\nIntegrate ELB with EC2 Auto Scaling\nThe ELB service integrates seamlessly with EC2 Auto Scaling. As soon as a new EC2 instance is added to or removed from the EC2 Auto Scaling group, ELB is notified. However, before it can send traffic to a new EC2 instance, it needs to validate that the application running on that EC2 instance is available.\nThis validation is done via the health checks feature of ELB. Monitoring is an important part of load balancers, as it should route traffic to only healthy EC2 instances. That’s why ELB supports two types of health checks.\n\nEstablishing a connection to a backend EC2 instance using TCP, and marking the instance as available if that connection is successful.\nMaking an HTTP or HTTPS request to a webpage that you specify, and validating that an HTTP response code is returned.\n\n\n\nDifferentiate Between Traditional Scaling and Auto Scaling\nWith a traditional approach to scaling, you buy and provision enough servers to handle traffic at its peak. However, this means that at night time, there is more capacity than traffic. This also means you’re wasting money. Turning off those servers at night or at times where the traffic is lower only saves on electricity.\nThe cloud works differently, with a pay-as-you-go model. It’s important to turn off the unused services, especially EC2 instances that you pay for On-Demand. One could manually add and remove servers at a predicted time. But with unusual spikes in traffic, this solution leads to a waste of resources with over-provisioning or with a loss of customers due to under-provisioning.\nThe need here is for a tool that automatically adds and removes EC2 instances according to conditions you define—that’s exactly what the EC2 Auto Scaling service does.\n\n\nUse Amazon EC2 Auto Scaling\nThe EC2 Auto Scaling service works to add or remove capacity to keep a steady and predictable performance at the lowest possible cost. By adjusting the capacity to exactly what your application uses, you only pay for what your application needs. And even with applications that have steady usage, EC2 Auto Scaling can help with fleet management. If there is an issue with an EC2 instance, EC2 Auto Scaling can automatically replace that instance. This means that EC2 Auto Scaling helps both to scale your infrastructure and ensure high availability.\n\n\nConfigure EC2 Auto Scaling Components\nThere are three main components to EC2 Auto Scaling.\n\nLaunch template or configuration: What resource should be automatically scaled?\nEC2 Auto Scaling Group: Where should the resources be deployed?\nScaling policies: When should the resources be added or removed?\n\n\n\nLearn About Launch Templates\nThere are multiple parameters required to create EC2 instances: Amazon Machine Image (AMI) ID, instance type, security group, additional Amazon Elastic Block Store (EBS) volumes, and more. All this information is also required by EC2 Auto Scaling to create the EC2 instance on your behalf when there is a need to scale. This information is stored in a launch template.\nYou can use a launch template to manually launch an EC2 instance. You can also use it with EC2 Auto Scaling. It also supports versioning, which allows for quickly rolling back if there was an issue or to specify a default version of your launch template. This way, while iterating on a new version, other users can continue launching EC2 instances using the default version until you make the necessary changes.\n\nYou can create a launch template one of three ways.\n\nThe fastest way to create a template is to use an existing EC2 instance. All the settings are already defined.\nAnother option is to create one from an already existing template or a previous version of a launch template.\nThe last option is to create a template from scratch. The following options will need to be defined: AMI ID, instance type, key pair, security group, storage, and resource tags.\n\nNote: Another way to define what Amazon EC2 Auto Scaling needs to scale is by using a launch configuration. It’s similar to the launch template, but it doesn’t allow for versioning using a previously created launch configuration as a template. Nor does it allow for creating one from an already existing EC2 instance. For these reasons and to ensure that you’re getting the latest features from Amazon EC2, use a launch template instead of launch configuration.\n\n\nGet to Know EC2 Auto Scaling Groups\nThe next component that EC2 Auto Scaling needs is an EC2 Auto Scaling Group (ASG). An ASG enables you to define where EC2 Auto Scaling deploys your resources. This is where you specify the Amazon Virtual Private Cloud (VPC) and subnets the EC2 instance should be launched in.\nEC2 Auto Scaling takes care of creating the EC2 instances across the subnets, so it’s important to select at least two subnets that are across different Availability Zones.\nASGs also allow you to specify the type of purchase for the EC2 instances. You can use On-Demand only, Spot only, or a combination of the two, which allows you to take advantage of Spot instances with minimal administrative overhead. To specify how many instances EC2 Auto Scaling should launch, there are three capacity settings to configure for the group size.\n\nMinimum: The minimum number of instances running in your ASG even if the threshold for lowering the amount of instances is reached.\nMaximum: The maximum number of instances running in your ASG even if the threshold for adding new instances is reached.\nDesired capacity: The amount of instances that should be in your ASG. This number can only be within or equal to the minimum or maximum. EC2 Auto Scaling automatically adds or removes instances to match the desired capacity number.\n\n\nWhen EC2 Auto Scaling removes EC2 instances because the traffic is minimal, it keeps removing EC2 instances until it reaches a minimum capacity. Depending on your application, using a minimum of two is a good idea to ensure high availability, but you know how many EC2 instances at a bare minimum your application requires at all times. When reaching that limit, even if EC2 Auto Scaling is instructed to remove an instance, it does not, to ensure the minimum is kept.\nOn the other hand, when the traffic keeps growing, EC2 Auto Scaling keeps adding EC2 instances. This means the cost for your application will also keep growing. That’s why it’s important to set a maximum amount to make sure it doesn’t go above your budget.\nThe desired capacity is the amount of EC2 instances that EC2 Auto Scaling creates at the time the group is created. If that number decreases, then EC2 Auto Scaling removes the oldest instance by default. If that number increases, then EC2 Auto Scaling creates new instances using the launch template.\n\n\nEnsure Availability with EC2 Auto Scaling\n\nUsing different numbers for minimum, maximum, and desired capacity is used for dynamically adjusting the capacity. However, if you prefer to use EC2 Auto Scaling for fleet management, you can configure the three settings to the same number, for example four. EC2 Auto Scaling will ensure that if an EC2 instance becomes unhealthy, it replaces it to always ensure that four EC2 instances are available. This ensures high availability for your applications.\n\n\nEnable Automation with Scaling Policies\nBy default, an ASG will be kept to its initial desired capacity. Although it’s possible to manually change the desired capacity, you can also use scaling policies.\nIn the AWS Monitoring module, you learned about Amazon CloudWatch metrics and alarms. You use metrics to keep information about different attributes of your EC2 instance like the CPU percentage. You use alarms to specify an action when a threshold is reached. Metrics and alarms are what scaling policies use to know when to act. For example, you set up an alarm that says when the CPU utilization is above 70% across the entire fleet of EC2 instances, trigger a scaling policy to add an EC2 instance.\nThere are three types of scaling policies: simple, step, and target tracking scaling.\n\n\nSimple Scaling Policy\nA simple scaling policy allows you to do exactly what’s described above. You use a CloudWatch alarm and specify what to do when it is triggered. This can be a number of EC2 instances to add or remove, or a specific number to set the desired capacity to. You can specify a percentage of the group instead of using an amount of EC2 instances, which makes the group grow or shrink more quickly.\nOnce this scaling policy is triggered, it waits a cooldown period before taking any other action. This is important as it takes time for the EC2 instances to start and the CloudWatch alarm may still be triggered while the EC2 instance is booting. For example, you could decide to add an EC2 instance if the CPU utilization across all instances is above 65%. You don’t want to add more instances until that new EC2 instance is accepting traffic.\nHowever, what if the CPU utilization was now above 85% across the ASG? Only adding one instance may not be the right move here. Instead, you may want to add another step in your scaling policy. Unfortunately, a simple scaling policy can’t help with that.\n\n\nStep Scaling Policy\nThis is where a step scaling policy helps. Step scaling policies respond to additional alarms even while a scaling activity or health check replacement is in progress. Similar to the example above, you decide to add two more instances in case the CPU utilization is at 85%, and four more instances when it’s at 95%.\nDeciding when to add and remove instances based on CloudWatch alarms may seem like a difficult task. This is why the third type of scaling policy exists: target tracking. Target Tracking Scaling Policy\nIf your application scales based on average CPU utilization, average network utilization (in or out), or based on request count, then this scaling policy type is the one to use. All you need to provide is the target value to track and it automatically creates the required CloudWatch alarms."
  },
  {
    "objectID": "posts/others/2021-10-04-OS.html#name-of-operating-system",
    "href": "posts/others/2021-10-04-OS.html#name-of-operating-system",
    "title": "ThomasHSimm",
    "section": "name of operating system",
    "text": "name of operating system\n\nprint('os.name',',',os.name,'\\n')\n\nos.name , nt"
  },
  {
    "objectID": "posts/others/2021-10-04-OS.html#get-current-dir",
    "href": "posts/others/2021-10-04-OS.html#get-current-dir",
    "title": "ThomasHSimm",
    "section": "get current dir",
    "text": "get current dir\n\nprint('os.getcwd()',',',os.getcwd(),'\\n')\n\nos.getcwd() , C:\\Users\\44781\\pyproj\\_misc"
  },
  {
    "objectID": "posts/others/2021-10-04-OS.html#list-files-in-dir",
    "href": "posts/others/2021-10-04-OS.html#list-files-in-dir",
    "title": "ThomasHSimm",
    "section": "list files in dir",
    "text": "list files in dir\n\nprint('os.listdir()',',',os.listdir(),'\\n')\n\nos.listdir() , ['.ipynb_checkpoints', 'adapic.jpg', 'awarhol.jpg', 'awarhol.webp', 'dog.jpg', 'draw.png', 'ebsd2.tif', 'ebsdmap.png', 'ebsdmap.tif', 'fastAI_C1_notes.ipynb', 'image_1000.jpg', 'image_5000(1).jpg', 'IndeedExtract-Copy1.ipynb', 'IndeedExtract-Copy2.ipynb', 'No_61_Mark_Rothko-thumbnail_webp-9999x9999.webp', 'os.ipynb', 'output', 'PF_500C R.png', 'PythonBook.ipynb', 're.ipynb', 'styleTransfer.ipynb', 'test2', 'Untitled.ipynb', 'vangogh.jpg', 'vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5', 'water.webp']"
  },
  {
    "objectID": "posts/others/2021-10-04-OS.html#make-a-directory",
    "href": "posts/others/2021-10-04-OS.html#make-a-directory",
    "title": "ThomasHSimm",
    "section": "make a directory",
    "text": "make a directory\n\nthis_dir=os.getcwd()\ndirectory='test'\n#this adds either / or \\ depending on os\npath = os.path.join(this_dir, directory)\n\ntry:\n    os.mkdir(path)\nexcept:\n    pass\n\n#or \ntry:\n    os.mkdir('test2')\nexcept:\n    pass"
  },
  {
    "objectID": "posts/others/2021-10-04-OS.html#list-files-in-dir-1",
    "href": "posts/others/2021-10-04-OS.html#list-files-in-dir-1",
    "title": "ThomasHSimm",
    "section": "list files in dir",
    "text": "list files in dir\n\nprint('os.listdir()',',',os.listdir(),'\\n')\n\nos.listdir() , ['.ipynb_checkpoints', 'adapic.jpg', 'awarhol.jpg', 'awarhol.webp', 'dog.jpg', 'draw.png', 'ebsd2.tif', 'ebsdmap.png', 'ebsdmap.tif', 'fastAI_C1_notes.ipynb', 'image_1000.jpg', 'image_5000(1).jpg', 'IndeedExtract-Copy1.ipynb', 'IndeedExtract-Copy2.ipynb', 'No_61_Mark_Rothko-thumbnail_webp-9999x9999.webp', 'os.ipynb', 'output', 'PF_500C R.png', 'PythonBook.ipynb', 're.ipynb', 'styleTransfer.ipynb', 'test', 'test2', 'Untitled.ipynb', 'vangogh.jpg', 'vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5', 'water.webp']"
  },
  {
    "objectID": "posts/others/2021-10-04-OS.html#check-filedir-exists",
    "href": "posts/others/2021-10-04-OS.html#check-filedir-exists",
    "title": "ThomasHSimm",
    "section": "Check file/dir exists",
    "text": "Check file/dir exists\n\nprint(os.path.exists(\"test2\"),os.path.exists(\"test\"))\n\nTrue True"
  },
  {
    "objectID": "posts/others/2021-10-04-OS.html#change-directory",
    "href": "posts/others/2021-10-04-OS.html#change-directory",
    "title": "ThomasHSimm",
    "section": "change directory",
    "text": "change directory\n\nos.chdir(directory)\nprint('os.getcwd()',',',os.getcwd(),'\\n')\n\nos.getcwd() , C:\\Users\\44781\\pyproj\\_misc\\test"
  },
  {
    "objectID": "posts/others/2021-10-04-OS.html#create-and-rename-a-file",
    "href": "posts/others/2021-10-04-OS.html#create-and-rename-a-file",
    "title": "ThomasHSimm",
    "section": "Create and Rename a file",
    "text": "Create and Rename a file\n\n# create a file 'a' is append\nopen('Old.txt','a').close()\n\n# check file exists \nprint('old=',os.path.exists(\"Old.txt\"),'. new=',os.path.exists(\"New.txt\"))\n\n# rename a file\nfd = \"Old.txt\"\nos.rename(fd,'New.txt')\n\n# check file exists \nprint('old=',os.path.exists(\"Old.txt\"),'. new=',os.path.exists(\"New.txt\"))\n\nold= True . new= False\nold= False . new= True"
  },
  {
    "objectID": "posts/others/2021-10-04-OS.html#copy-a-file-",
    "href": "posts/others/2021-10-04-OS.html#copy-a-file-",
    "title": "ThomasHSimm",
    "section": "Copy a file-",
    "text": "Copy a file-\n\ncan be done in os but easier in shutil\nhttps://stackabuse.com/how-to-copy-a-file-in-python/\n\nimport shutil\nshutil.copyfile('New.txt', 'Old.txt')\n\n# check file exists \nprint('old=',os.path.exists(\"Old.txt\"),'. new=',os.path.exists(\"New.txt\"))\n\nold= True . new= True"
  },
  {
    "objectID": "posts/others/2021-10-04-OS.html#remove-a-file",
    "href": "posts/others/2021-10-04-OS.html#remove-a-file",
    "title": "ThomasHSimm",
    "section": "Remove a file",
    "text": "Remove a file\n\n# remove a file\nos.remove(\"New.txt\")\n# os.remove(\"Old.txt\")\n\n# check file exists \nprint('old=',os.path.exists(\"Old.txt\"),'. new=',os.path.exists(\"New.txt\"))\n\nold= True . new= False"
  },
  {
    "objectID": "posts/others/2021-10-04-OS.html#go-back-up-in-directory",
    "href": "posts/others/2021-10-04-OS.html#go-back-up-in-directory",
    "title": "ThomasHSimm",
    "section": "Go back up in directory",
    "text": "Go back up in directory\n\n\nos.path.dirname(os.path.dirname(   ))\n\n'C:\\\\Users\\\\44781\\\\pyproj'\n\n\n\n#remove directory\nos.rmdir(path)\n#list files in dir\nprint('os.listdir()',',',os.listdir(),'\\n')\n\nos.listdir() , ['.ipynb_checkpoints', 'adapic.jpg', 'awarhol.jpg', 'awarhol.webp', 'dog.jpg', 'draw.png', 'ebsd2.tif', 'ebsdmap.png', 'ebsdmap.tif', 'fastAI_C1_notes.ipynb', 'image_1000.jpg', 'image_5000(1).jpg', 'IndeedExtract-Copy1.ipynb', 'IndeedExtract-Copy2.ipynb', 'No_61_Mark_Rothko-thumbnail_webp-9999x9999.webp', 'os.ipynb', 'output', 'PF_500C R.png', 'PythonBook.ipynb', 're.ipynb', 'styleTransfer.ipynb', 'test2', 'Untitled.ipynb', 'vangogh.jpg', 'vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5', 'water.webp']"
  },
  {
    "objectID": "posts/others/2021-10-04-OS.html#create-new-files",
    "href": "posts/others/2021-10-04-OS.html#create-new-files",
    "title": "ThomasHSimm",
    "section": "Create new files",
    "text": "Create new files\n\n#create new file and edit\nnano file.txt\n#create file\ntouch file.txt\n\n\n!cd\n\nC:\\Users\\44781\\pyproj\\_misc\\test"
  },
  {
    "objectID": "posts/others/2021-10-04-OS.html#shell-commands",
    "href": "posts/others/2021-10-04-OS.html#shell-commands",
    "title": "ThomasHSimm",
    "section": "Shell Commands",
    "text": "Shell Commands\n\necho\n\necho HELLO print HELLO to screen\necho $string1 print variable string1\n\n\n\nmaths operations\n\necho $(( 10 + 5 )) add two numbers\n\n\n\ncat\n\ncat [file] command allows us to create single or multiple files, view the contents of a file, concatenate files, and redirect output in terminal or other files.\n\n\n\ngrep\nGrep command, which stands for “global regular expression print”, processes text line-by-line and prints any lines that match a specified pattern.\n\ngrep [pattern] [file-directory/location]\ne.g. grep \"jane\" list.txt find the occurances of “jane” in list.txt\n\nHere, [file-directory] is the path to the directory/folder where you want to perform a search operation. The grep command is also used to search text and match a string or pattern within a file.\n\n\ncut\n\ncut [options] [file] The cut command extracts a given number of characters or columns from a file. A delimiter is a character or set of characters that separate text strings.\nFor delimiter separated fields, the - d option is used. The -f option specifies the field, a set of fields, or a range of fields to be extracted. cut -d [delimiter] -f [field number]\n\n\n\ncat\n\ncat > [file]\nEach stream uses redirection commands. A single greater than sign (>) or a double greater than sign (>>) can be used to redirect standard output. If the target file doesn’t exist, a new file with the same name will be created.\ncat >> [file]\nCommands with a double greater than sign (>>) do not overwrite the existing file content, but it will append to it.\n> can be used to create a file\ne.g. > test.txt\ncreates the file test.txt\necho \"I am appending text to this test file\" >> test.txt\nto append to the same file\n\n\n\ntest\ntests an assertion can be replaced with []\n\n\nnano\nEdit a file / create a file and edit it\nfor a shell script - #!/bin/bash\nAnd for a python script - #!/usr/bin/env python3\n\n\nchmod\n\nchmod +x findJane.sh\n\n\n\nfor statement\n\nfor i in 1 2 3; do echo $i; done\nthe key elements to note are ; do and done\n\nExamples\ncat list.txt\n001 jane /data/jane_profile_07272018.doc\n002 kwood /data/kwood_profile_04022017.doc\n003 pchow /data/pchow_profile_05152019.doc\n004 janez /data/janez_profile_11042019.doc\n005 jane /data/jane_pic_07282018.jpg\n006 kwood /data/kwood_pic_04032017.jpg\n007 pchow /data/pchow_pic_05162019.jpg\n008 jane /data/jane_contact_07292018.csv\n009 kwood /data/kwood_contact_04042017.csv\n010 pchow /data/pchow_contact_05172019.csv\ncat list.txt | cut -d ' ' -f 2\njane\nkwood\npchow\njanez\njane\nkwood\npchow\njane\nkwood\npchow\n\nhere | is a pipe to connect commands\neach line is cut by delimiter space ’ ’ then the 2nd term is outputted. IF we put 1 instead we get 001, 002 etc\nor f 1,3 would get 1st and 3rd parts after split by space\n\nif test -e ~/data/jane_profile_07272018.doc; then echo \"File exists\"; else echo \"File doesn't exist\"; fi\n\nthe use of e tests if a file exists\n\n\n# For statement print numbers 1 to 99 in steps of 1\nfor i in {1..99..1}\ndo \n  echo $i\ndone\n\n\n# For and If statements with math operation\nfor i in {1..99..1}\ndo \n  if [ $(( i % 2 )) == 1 ]\n  then\n    echo $i\n  fi\ndone\n\n\n#!/bin/bash\n> oldFiles.txt\n\nfiles=\"$(grep \" jane \" ../data/list.txt | cut -d \" \" -f 3)\"\n\nfor file in $files;\n  do\n   if test -e \"..\"$file;\n   then echo $file >> oldFiles.txt;\n   else echo \"no\" $file;\n   fi\n\ndone\n\nBash Scripting Resources\nCheck out the following links for more information:\n\nhttps://ryanstutorials.net/bash-scripting-tutorial/\nhttps://linuxconfig.org/bash-scripting-tutorial-for-beginners\nhttps://www.shellscript.sh"
  },
  {
    "objectID": "posts/others/2021-10-04-PythonBook.html#some-maths",
    "href": "posts/others/2021-10-04-PythonBook.html#some-maths",
    "title": "ThomasHSimm",
    "section": "Some maths",
    "text": "Some maths\n\nimport math\nprint(math.pi)\nprint(math.sqrt(34))\n\n3.141592653589793\n5.830951894845301\n\n\n\nimport random\nprint(random.random())\n\nprint(random.choice([1,2,3,4]))\nprint(random.choice([1,2,3,4]))\n\n0.7617615023592539\n4\n2"
  },
  {
    "objectID": "posts/others/2021-10-04-PythonBook.html#indexing",
    "href": "posts/others/2021-10-04-PythonBook.html#indexing",
    "title": "ThomasHSimm",
    "section": "Indexing",
    "text": "Indexing\n\nS='Spam'\n\nprint(S[1:3])\nprint(S[:-1])\nprint(S[1:])\nprint(S[:])\nprint(S*3)\n\npa\nSpa\npam\nSpam\nSpamSpamSpam"
  },
  {
    "objectID": "posts/others/2021-10-04-PythonBook.html#immutability",
    "href": "posts/others/2021-10-04-PythonBook.html#immutability",
    "title": "ThomasHSimm",
    "section": "Immutability",
    "text": "Immutability\n\n\n# strings are not immutable\nS[0]='p' ##error\n\nTypeError: 'str' object does not support item assignment\n\n\n\n# but we can create a new string\nS = 'z' +S[1:]\nprint(S)\n#or use replaces\n\nzpam"
  },
  {
    "objectID": "posts/others/2021-10-04-PythonBook.html#string-specific-methods",
    "href": "posts/others/2021-10-04-PythonBook.html#string-specific-methods",
    "title": "ThomasHSimm",
    "section": "String specific methods",
    "text": "String specific methods\nhttps://docs.python.org/3/library/stdtypes.html#string-methods\n\nSplit\nReturn a list of the words in the string, using sep as the delimiter string. If maxsplit is given, at most maxsplit splits are done (thus, the list will have at most maxsplit+1 elements). If maxsplit is not specified or -1, then there is no limit on the number of splits (all possible splits are made).\n\nstra='xsxhu csjoaij jsaijaio j dijoi'\nstra1=stra.split()\nprint(stra1)\nstra1=stra.split('j',maxsplit=2)\nprint(stra1)\n\n['xsxhu', 'csjoaij', 'jsaijaio', 'j', 'dijoi']\n['xsxhu cs', 'oai', ' jsaijaio j dijoi']\n\n\n\nline='aaa,bbb,cccc,dd d\\n'\n\n#split based on something\nprint(line.split(','))\n#creates a list\nprint(type(line.split(',')))\n\n# strip out whitespace on rhs\nprint(line.rstrip())\n\n\n\nStrip\nReturn a copy of the string with the leading and trailing characters removed.\n\n'   spacious   '.strip()\n\n'spacious'\n\n\n\n\nCheck nature of string\nstr.isalnum()\nThis method checks if all the characters of a string are alphanumeric (a-z, A-Z and 0-9).\n\nstra='abcD1'\nprint(stra,stra.isalnum())\nstra='abcD1#'\nprint(stra,stra.isalnum())\n\nabcD1 True\nabcD1# False\n\n\nstr.isalpha()\nThis method checks if all the characters of a string are alphabetical (a-z and A-Z).\n\nstra='abcD1'\nprint(stra,stra.isalpha())\nstra='abcD'\nprint(stra,stra.isalpha())\n\nabcD1 False\nabcD True\n\n\nstr.isdigit()\nThis method checks if all the characters of a string are digits (0-9)\nstra=‘abcD1’ print(stra,stra.isdigit()) stra=‘190’ print(stra,stra.isdigit())\nstr.islower()\nThis method checks if all the characters of a string are lowercase characters (a-z).\n\nstra='abcD1'\nprint(stra,stra.islower())\nstra='190'\nprint(stra,stra.islower())\n\nabcD1 False\n190 False\n\n\nstr.lower or str.upper change whether upper or lower case\n\nstra='abcD1'\nprint(stra,stra.lower(),stra.upper())\n\nabcD1 abcd1 ABCD1\n\n\ncaptialize the first character\n\nprint(stra.capitalize())\n\nAbcd1\n\n\n\n\nFind\nFind the position of a substring within a string\nstring.find(stringIN) scan left to right\nstring.rfind(stringIN) scan right to left\n\nstra='ABCDCDC'\nstraa='BCD'\nstra.find('CD'),stra.rfind('CD')\n\n(2, 4)\n\n\n\n\nReplace\nstring.replace(sub_string,string) replace parts of a string\n\nS='oke doke karaoke'\n\n#replace parts of a string\nprint(S)\nprint(S.replace('karaoke','noke'))\n\n\noke doke karaoke\noke doke noke"
  },
  {
    "objectID": "posts/others/2021-10-04-PythonBook.html#formatting",
    "href": "posts/others/2021-10-04-PythonBook.html#formatting",
    "title": "ThomasHSimm",
    "section": "Formatting",
    "text": "Formatting\n\n\nprint('%s, eggs, and %s' % ('spam', 'SPAM!'))\n\nprint('{0}, eggs, and {1}'.format('spam', 'SPAM!'))"
  },
  {
    "objectID": "posts/others/2021-10-04-PythonBook.html#help",
    "href": "posts/others/2021-10-04-PythonBook.html#help",
    "title": "ThomasHSimm",
    "section": "Help",
    "text": "Help\nPut into help( ) to get help on it\n\nhelp(S.format)"
  },
  {
    "objectID": "posts/others/2021-10-04-PythonBook.html#lists",
    "href": "posts/others/2021-10-04-PythonBook.html#lists",
    "title": "ThomasHSimm",
    "section": "Lists",
    "text": "Lists\nhttps://docs.python.org/3/tutorial/datastructures.html?highlight=tuple\nThe Python list object is the most general sequence provided by the language. Lists are positionally ordered collections of arbitrarily typed objects, and they have no fixed size. They are also mutable—unlike strings, lists can be modified in-place by assignment to offsets as well as a variety of list method calls\n\nNew List\nCreate a list with list(XX)\n\na='1 2 3 4 5 6 7'\nlista = list(a)\nprint(lista)\n\n['1', ' ', '2', ' ', '3', ' ', '4', ' ', '5', ' ', '6', ' ', '7']\n\n\n\n\nCopy\nlist.copy\n\nlista =a.split(' ')\nlista=lista[0:3]\nprint('Original a=\\n',lista)\nlistb=lista\nlistc=lista.copy()\nlistd=lista[:]\n\nlistb[1]='po'\n\nprint('list b, where b=a and b[1] modified, b=\\n',listb,\n    '\\nJust using equals b=a after mods, a=\\n',lista,\n      '\\n Using a copy c=a.copy(), c=\\n',listc,\n      '\\n Using d=a[:] to create a copy, d=\\n',listd)\n\nOriginal a=\n ['1', '2', '3']\nlist b, where b=a and b[1] modified, b=\n ['1', 'po', '3'] \nJust using equals b=a after mods, a=\n ['1', 'po', '3'] \n Using a copy c=a.copy(), c=\n ['1', '2', '3'] \n Using d=a[:] to create a copy, d=\n ['1', '2', '3']\n\n\n\n\nAppend\nlist.append(x), add x to end of a list\n\nprint(lista)\nlista.append('ok')\nprint(lista)\n\n['1', 'po', '3']\n['1', 'po', '3', 'ok']\n\n\n\n\nInsert\nlista.insert(i,x) insert x at position i\n\nlista.insert(2,'two')\nprint(lista)\n\n['1', 'po', 'two', '3', 'ok']\n\n\n\n\nRemove\nlista.remove(x) Remove the first item from the list whose value is equal to x.\n\nlista.remove('two')\nprint(lista)\n\n['1', 'po', '3', 'ok']\n\n\n\n\nPop\nlista.pop([i]) Remove the item at the given position in the list, and return it\nlist.popleft at left\n\nprint(lista.pop(1))\n\npo\n\n\n\n\nRemove\nlist.clear() Remove all items from the list. Equivalent to del a[:].\n\nprint(lista.clear())\nprint(lista)\n\nNone\n[]\n\n\n\n\nIndex\nlist.index(x,i) Return position of x within list starting at position i\n\nlistc.append('1')\nprint(listc)\nprint(listc.index('1'),2)\n\n['1', '2', '3', '1']\n0 2\n\n\n\n\nCount\nlista.count(x) Return the number of times x appears in the list.\n\nprint(listc.count('1'))\n\n2\n\n\n\n\nSort\nlista.sort(*, key=None, reverse=False) Sort the items of the list in place (the arguments can be used for sort customization, see sorted() (https://docs.python.org/3/library/functions.html#sorted) for their explanation).\n\nlistc.sort()\nprint(listc)\n\n['1', '1', '2', '3']\n\n\n\n\nReverse\nlista.reverse() Reverse the elements of the list in place.\n\n\nDel\ndel lista[0] remove an item from a list given its index instead of its value\n\nprint(listc)\ndel listc[2]\nprint(listc)\n\n['1', '1', '2', '3']\n['1', '1', '3']\n\n\n\n\n#list of different types\nL =[123, 'spam',1.23]\nprint(L)\n\n#access\nprint(L[2])\n\n#append\nL.append('NI')\nprint(L)\n\n#get rid of one pop!\nL.pop(0)\nprint(L)\n\nM=['aa','jeji','boio','popo','gsss','zulu','ccc']\n#sort\nM.sort()\nprint(M)\nM.reverse()\nprint(M)\n\n[123, 'spam', 1.23]\n1.23\n[123, 'spam', 1.23, 'NI']\n['spam', 1.23, 'NI']\n['aa', 'boio', 'ccc', 'gsss', 'jeji', 'popo', 'zulu']\n['zulu', 'popo', 'jeji', 'gsss', 'ccc', 'boio', 'aa']"
  },
  {
    "objectID": "posts/others/2021-10-04-PythonBook.html#comprehensions",
    "href": "posts/others/2021-10-04-PythonBook.html#comprehensions",
    "title": "ThomasHSimm",
    "section": "Comprehensions",
    "text": "Comprehensions\nList comprehensions provide a concise way to create lists. Common applications are to make new lists where each element is the result of some operations applied to each member of another sequence or iterable, or to create a subsequence of those elements that satisfy a certain condition.\n\nsquares=[]\nfor x in range(10):\n    squares.append(x**2)\nprint(squares)\n\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n\n\n\n# the comprehension version\n\nprint([x**2 for x in range(10)])\n\n[0, 1, 4, 9, 16, 25, 36, 49, 64, 81]\n\n\nA list comprehension consists of brackets containing an expression followed by a for clause, then zero or more for or if clauses. The result will be a new list resulting from evaluating the expression in the context of the for and if clauses which follow it. For example, this listcomp combines the elements of two lists if they are not equal:\n\n[(x,y) for x in [1,2,3] for y in [3, 1, 4] if x!=y]\n\n[(1, 3), (1, 4), (2, 3), (2, 1), (2, 4), (3, 1), (3, 4)]\n\n\n\nvec = [[1,2,3], [4,5,6], [7,8,9]]\n[num for elem in vec for num in elem]\n\n[1, 2, 3, 4, 5, 6, 7, 8, 9]\n\n\n\nmatrix=[[1,2,3,4],\n       [5,6,7,8],\n       [9,19,11,12]]\n[[col[i] for col in matrix] for i in range(4)]\n\n[[1, 5, 9], [2, 6, 19], [3, 7, 11], [4, 8, 12]]\n\n\n\nList to string\nConvert a list to a string with \"\".join(lista)\n\nstra = \"\".join(listd)\nprint(stra,'-',stra[1:])\n\n123 - 23"
  },
  {
    "objectID": "posts/others/2021-10-04-PythonBook.html#dictionaries",
    "href": "posts/others/2021-10-04-PythonBook.html#dictionaries",
    "title": "ThomasHSimm",
    "section": "Dictionaries",
    "text": "Dictionaries\nPython dictionaries are something completely different (Monty Python reference intended)—they are not sequences at all, but are instead known as mappings. Mappings are also collections of other objects, but they store objects by key instead of by relative position. In fact, mappings don’t maintain any reliable left-to-right order; they simply map keys to associated values. Dictionaries, the only mapping type in Python’s core objects set, are also mutable: they may be changed in-place and can grow and shrink on demand, like lists.\n\n#create dict\n\nD = {'food':'Spam','quality':4, 'color':'pink'}\nprint(D)\n\n#or create by key assignment \nD={}\nD['food']='Spam'\nD['quality']=4\nD['color']='pink'\nprint(D)\n\n#index it\nprint(D['food'])\n\n\n# Nesting\n#what if the info is more complex? Nest\n\nrec = {'name':{'first':'Bob','last':'Smith'},\n        'job':['dev','mgr'],\n        'age':40.5}\n\n#index them\nprint(rec['name'])\n\nprint(rec['name']['last'])\n\nprint(rec['job'][0])\n\n#or add more NB job is a list\nrec['job'].append('janitor')\n\nprint(rec)\n\n#keys are 1st bit\nprint(rec.keys())"
  },
  {
    "objectID": "posts/others/2021-10-04-PythonBook.html#tuples",
    "href": "posts/others/2021-10-04-PythonBook.html#tuples",
    "title": "ThomasHSimm",
    "section": "Tuples",
    "text": "Tuples\nroughly like a list that cannot be changed—tuples are sequences, like lists, but they are immutable, like strings. Syntactically, they are coded in parentheses instead of square brackets, and they support arbitrary types, arbitrary nesting, and the usual sequence operations:\n\nT=(1,2,3,4)\nprint(len(T))\n\n#concatenation\nprint(T+(5,6))\n\n#indexing\nprint(T[0])\n\n## or\nT = ('spam', 3.0, [11, 22, 33])\nprint(T)"
  },
  {
    "objectID": "posts/others/2021-10-04-PythonBook.html#print",
    "href": "posts/others/2021-10-04-PythonBook.html#print",
    "title": "ThomasHSimm",
    "section": "Print",
    "text": "Print\n\n# to so many decimal places\n\nx=30.5557889\n\nprint('{:.5f}'.format(x))\n\n30.55579\n\n\n\n# add additional characters to string\n\nwidth = 20\nprint('HackerRank'.ljust(width,'-'))#or rjust\n\nHackerRank----------\n\n\n\nwidth = 20\nprint('HackerRank'.center(width,'-'))\n\n-----HackerRank-----\n\n\nhttps://docs.python.org/3/library/stdtypes.html#string-methods\nConversion……………………… Meaning\nd………………………………. Signed integer decimal.\ni………………………………. Signed integer decimal.\no………………………………. Signed octal value.\nu………………………………. Obsolete type – it is identical to ‘d’.\nx………………………………. Signed hexadecimal (lowercase).\nX………………………………. Signed hexadecimal (uppercase).\ne ………………………………. Floating point exponential format (lowercase).\nE………………………………. Floating point exponential format (uppercase).\nf………………………………. Floating point decimal format.\nF………………………………. Floating point decimal format.\ng………………………………. Floating point format. Uses lowercase exponential format if exponent is less than -4 or not less than precision, decimal format otherwise.\nG ………………………………. Floating point format. Uses uppercase exponential format if exponent is less than -4 or not less than precision, decimal format otherwise.\nc………………………………. Single character (accepts integer or single character string).\nr………………………………. String (converts any Python object using repr()).\ns………………………………. String (converts any Python object using str()).\na………………………………. String (converts any Python object using ascii()).\n%………………………………. No argument is converted, results in a ‘%’ character in the result."
  },
  {
    "objectID": "posts/others/2021-10-10-ImageClassifier.html#the-model-part",
    "href": "posts/others/2021-10-10-ImageClassifier.html#the-model-part",
    "title": "ThomasHSimm",
    "section": "1) The model Part",
    "text": "1) The model Part\nThe model was run on https://colab.research.google.com with a GPU\nNecessary to have a GPU for time\nSome imports and installs\n\n!pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\n\n\nfrom fastbook import *\nfrom fastai.vision.widgets import *\n\nGets Azure search key to use Bing search API\n\nkey = os.environ.get('AZURE_SEARCH_KEY', 'keygoeshere')\n\nI’m going to do a classifier for holiday types\n\ndestas={'beach':{'beach','tropical','sea','beach holidays'},\n        'snow':{'ski','snowboard','snow','ski holidays'},\n        'countryside':{'lakes mountains','countryside','forest','fields'},\n        'city':{'city','cities','bars','buildings'},\n        'safari':{'safari','safari holidays','safari park','safari africa'}}\n\nCreates a folder containing images for each type\nNeed to add to dir with different searches not delete and add new stuff each time\nFor eaach holiday type- go through the search topics and add the results together before downloading images to the holiday type folder\nThis gives 600 pics per holiday type\n\npath = Path('Destinations')\ntry:\n    path.mkdir()\nexcept:\n    pass\n\n\nfor o in destas:\n    dest = (path/o)\n    dest.mkdir(exist_ok=True)\n    for ii,oo in enumerate(destas[o]): \n        # print(oo)\n        results = search_images_bing(key, f'{oo}')\n        if ii>0:\n            resultsALL = L(resultsALL,results).concat()\n        else:\n            resultsALL=results\n    print(o)\n    print(np.shape(resultsALL))\n    download_images(dest, urls=resultsALL.attrgot('contentUrl'))\n\nCreate a data block for fastai\n\nfns = get_image_files(path)\n\ndests = DataBlock(\n    blocks=(ImageBlock, CategoryBlock), \n    get_items=get_image_files, \n    splitter=RandomSplitter(valid_pct=0.2, seed=42),\n    get_y=parent_label,\n    item_tfms=Resize(128))\n\nget rid of failed images\n\nfailed = verify_images(fns)\nfailed.map(Path.unlink)\nfailed\n\nHave a look at the images\n\ndls = dests.dataloaders(path)\ndls.valid.show_batch(max_n=40, nrows=10)\n\n\ncreate a dls for the learner\n\nrandomresizedcrop\ndefault aug transforms\n\n\ndests = dests.new(\n    item_tfms=RandomResizedCrop(224, min_scale=0.5),\n    batch_tfms=aug_transforms())\ndls = dests.dataloaders(path)\n\nDo the learning over 4 epochs\n\nlearn = cnn_learner(dls, resnet18, metrics=error_rate)\nlearn.fine_tune(4)\n\n\nConfusion matrix\n\ninterp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix()\n\n\nSave the model (saves as export.pkl)\n\nlearn.export()"
  },
  {
    "objectID": "posts/others/2021-10-10-ImageClassifier.html#the-app-in-python",
    "href": "posts/others/2021-10-10-ImageClassifier.html#the-app-in-python",
    "title": "ThomasHSimm",
    "section": "2) The App in python",
    "text": "2) The App in python\nSome imports\n\n# Classifier App\n# THSimm\n\nfrom fastai.vision.all import *\nfrom fastai.vision.widgets import *\nimport urllib.request\n\nCreate parts of the widget\n\nlearn_inf = load_learner('export.pkl')\nbtn_upload = widgets.FileUpload()\n\nout_pl = widgets.Output()\nout_pl.clear_output()\n\nlbl_pred = widgets.Label()\n\nFunction occurs on click upload\n\ndef on_click_classify(change):\n    img = PILImage.create(btn_upload.data[-1])\n    out_pl.clear_output()\n    with out_pl: display(img.to_thumb(500,500))\n    pred,pred_idx,probs = learn_inf.predict(img)\n    lbl_pred.value = f'Prediction: {pred}; Probability: {probs[pred_idx]:.04f}'\n\n\nbtn_upload.observe(on_click_classify,names=['data'])\n\nWhat is displayed\n\n#hide_output\ntext ='Select your plane'\n\ndisplay(VBox([widgets.HTML(value = f\"<h1><font color='Black'>{text}</h1>\\\n                                     <ol text-align: center><font color='Black'>\\\n                                    <li>Beach</li>\\\n                                    <li>Snow</li>\\\n                                    <li>Countryside</li>\\\n                                    <li>City</li>\\\n                                    <li>Safari</li>\\\n                                    </ol>\"), \n      btn_upload,  out_pl, lbl_pred]) )"
  },
  {
    "objectID": "posts/others/2021-10-10-ImageClassifier.html#the-binder-part",
    "href": "posts/others/2021-10-10-ImageClassifier.html#the-binder-part",
    "title": "ThomasHSimm",
    "section": "3) The Binder Part",
    "text": "3) The Binder Part\n\nBinder\n\nmybinder.org is an online service for building and sharing reproducible and interactive computational environments from online repositories. Under the hood, it is a federation of BinderHub deployments that are maintained by the Binder community. It serves as both a public service and a demonstration of the BinderHub technology, though it is by no means the only BinderHub in existence. If you’re interested in deploying your own BinderHub for your own uses, please see the BinderHub documentation and don’t hesitate to reach out to the Binder community.\n\nhttps://mybinder.readthedocs.io/en/latest/introduction.html#preparing-a-repository-for-binder\nBasically allowing us to put code online\n\n\nVoila\nThe 2nd import part is Voila which allows us to hide the code and just display outputs\nhttps://voila.readthedocs.io/en/stable/using.html\n\nMethod:\n\n\nCreate repository on github that is public, containing the ipynb file and a requirements.txt file\n\n\nThen go to binder https://mybinder.org/\n\n\nFill in form as shown below\n\n\nIn requirements.txt:\nvoila fastai packaging ipywidgets"
  },
  {
    "objectID": "posts/others/2021-10-10-ImageClassifier.html#the-result",
    "href": "posts/others/2021-10-10-ImageClassifier.html#the-result",
    "title": "ThomasHSimm",
    "section": "The result",
    "text": "The result\nhttps://mybinder.org/v2/gh/ThomasHSimm/LocationsClassifier/HEAD?urlpath=%2Fvoila%2Frender%2FAppForDestClass.ipynb\nhttps://tinyurl.com/LocClassAppThomasHSimm"
  },
  {
    "objectID": "posts/others/2021-10-29-Imbedding-Python-Web.html#ipywidgets",
    "href": "posts/others/2021-10-29-Imbedding-Python-Web.html#ipywidgets",
    "title": "ThomasHSimm",
    "section": "Ipywidgets",
    "text": "Ipywidgets\nhttps://ipywidgets.readthedocs.io/en/latest/\n\nipywidgets, also known as jupyter-widgets or simply widgets, are interactive HTML widgets for Jupyter notebooks and the IPython kernel.\nNotebooks come alive when interactive widgets are used. Users gain control of their data and can visualize changes in the data.\nLearning becomes an immersive, fun experience. Researchers can easily see how changing inputs to a model impact the results.\n\nImport\n\nimport ipywidgets as widgets\n\nA slider\n\nwidgets.IntSlider()\n\n\n\n\nChange the details of the slider\n\nwidgets.IntSlider(\n    value=7,\n    min=0,\n    max=10,\n    step=1,\n    description='Test:',\n    disabled=False,\n    continuous_update=False,\n    orientation='horizontal',\n    readout=True,\n    readout_format='d'\n)\n\n\n\n\nGet the value\n\nx=widgets.IntSlider()\nprint('The value is {}'.format(str(x.value)))\nx\n\nThe value is 0\n\n\n\n\n\nA different way with interact\n\n#import the interact part\nfrom ipywidgets import interact\n\n#some function we want to call\ndef square(x):\n    return x * x *2\n\n#the new widget slider\ninteract(square, x=(0, 100, 10));\n\n\n\n\nA text box\n\nwidgets.Text(value='Hello World!', disabled=True)\n\n\n\n\nA toggle button\n\nwidgets.ToggleButton(\n    value=False,\n    description='Click me',\n    disabled=False,\n    button_style='', # 'success', 'info', 'warning', 'danger' or ''\n    tooltip='Description',\n    icon='check' # (FontAwesome names without the `fa-` prefix)\n)\n\n\n\n\nA dropdown menu\n\nwidgets.Dropdown(\n    options=['1', '2', '3'],\n    value='2',\n    description='Number:',\n    disabled=False,\n)\n\n\n\n\nA radiobutton with a label in a box\n\nwidgets.Box(\n    [\n        widgets.Label(value='Pizza topping with a very long label:'),\n        widgets.RadioButtons(\n            options=[\n                'pepperoni',\n                'pineapple',\n                'anchovies',\n                'and the long name that will fit fine and the long name that will fit fine and the long name that will fit fine '\n            ],\n            layout={'width': 'max-content'}\n        )\n    ]\n)\n\n\n\n\nSome html\n\nwidgets.HTMLMath(\n    value=r\"Some math and <i>HTML</i>: \\(x^2\\) and $$\\frac{x+1}{x-1}$$\",\n    placeholder='Some HTML',\n    description='Some HTML',\n)\n\n\n\n\nAccordion, with slider and textbox\n\naccordion = widgets.Accordion(children=[widgets.IntSlider(), widgets.Text()], titles=('Slider', 'Text'))\naccordion\n\n\n\n\nmultiple sliders\n\na = widgets.IntSlider(description='a')\nb = widgets.IntSlider(description='b')\nc = widgets.IntSlider(description='c')\ndef f(a, b, c):\n    print('{}*{}*{}={}'.format(a, b, c, a*b*c))\n\nout = widgets.interactive_output(f, {'a': a, 'b': b, 'c': c})\n\nwidgets.HBox([widgets.VBox([a, b, c]), out])"
  },
  {
    "objectID": "posts/others/2021-10-29-Imbedding-Python-Web.html#inserting-into-a-website",
    "href": "posts/others/2021-10-29-Imbedding-Python-Web.html#inserting-into-a-website",
    "title": "ThomasHSimm",
    "section": "Inserting into a website",
    "text": "Inserting into a website\nAfter installing nbinteract a html file of a python notebook can be created\n\nnbinteract Quick_ipyWidget.ipynb -s thomashsimm/TestWdgets/main"
  },
  {
    "objectID": "posts/others/2021-11-17-StyleTransfer.html#introduction",
    "href": "posts/others/2021-11-17-StyleTransfer.html#introduction",
    "title": "ThomasHSimm",
    "section": "Introduction",
    "text": "Introduction\nUse two images a content image and style image to create a new image of the content image in the style of the style image.\nSource used, Deep Learning Specialization Week 4 https://www.coursera.org/lecture/convolutional-neural-networks/what-is-neural-style-transfer-SA5H8\n\nyoutube: https://youtu.be/R39tWYYKNcI\n\nFrom original NST paper published by the Visual Geometry Group at University of Oxford in 2014"
  },
  {
    "objectID": "posts/others/2021-11-17-StyleTransfer.html#code",
    "href": "posts/others/2021-11-17-StyleTransfer.html#code",
    "title": "ThomasHSimm",
    "section": "Code",
    "text": "Code\nSome imports\n\nimport os\nimport sys\nimport scipy.io\nimport scipy.misc\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow\nfrom PIL import Image\nimport numpy as np\nimport tensorflow as tf\nfrom tensorflow.python.framework.ops import EagerTensor\nimport pprint\n%matplotlib inline\n\nfrom google.colab import drive\ndrive.mount('/content/drive')\n\nDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n\n\nSet the style image\n\nstyle_image = np.array(Image.open(\"/content/drive/MyDrive/Colab Notebooks/Tiles.jpg\"))\n\nimshow(style_image)\n\n\nAnd the content image\n\n\ncontent_image = np.array(Image.open(\"/content/drive/MyDrive/Colab Notebooks/boat.jpg\"))\n\nprint(np.shape(content_image))\nimshow(content_image)   \n\n\nResize the images\n\n#gonna reduce to a square image of size in pixels of\nimg_size = 1100\n\n# get image as array, then resize\ncontent_image = Image.fromarray(content_image)\ncontent_image =np.array(content_image.resize((img_size, img_size)))\n\n# create content image as tf tensor\ncontent_image = tf.constant(np.reshape(content_image, ((1,) + content_image.shape)))\n\n\n\n# same for style image\n\nstyle_image = Image.fromarray(style_image)\nstyle_image = np.array(style_image.resize((img_size, img_size)))\n\nstyle_image = tf.constant(np.reshape(style_image, ((1,) + style_image.shape)))\n\n\nLoad parameters from the VGG model. A pretrained model for image classification\nhttps://www.robots.ox.ac.uk/~vgg/research/very_deep/\nhttps://gist.github.com/ksimonyan/3785162f95cd2d5fee77#file-readme-md\n\ntf.random.set_seed(272)\npp = pprint.PrettyPrinter(indent=4)\n\n\nvgg = tf.keras.applications.VGG19(include_top=False,\n                                  input_shape=(img_size, img_size, 3),\n                                  weights='/content/drive/MyDrive/Colab Notebooks/vgg19_weights_tf_dim_ordering_tf_kernels_notop.h5')\n\nvgg.trainable = False\npp.pprint(vgg)\n\n<keras.engine.functional.Functional object at 0x7f84e154af90>\n\n\nNow choose layers to represent the style of the image and assign style costs: Lower number more basic features\n\nSTYLE_LAYERS = [\n    ('block1_conv1', .2),\n    ('block2_conv1', .2),\n    ('block3_conv1', .2),\n    ('block4_conv1', .2),\n    ('block5_conv1', .2)]\n\nCompute the “content cost” using TensorFlow.\n\ndef compute_content_cost(content_output, generated_output):\n    \"\"\"\n    Computes the content cost\n    \n    Arguments:\n    a_C -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image C \n    a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing content of the image G\n    \n    Returns: \n    J_content -- scalar that you compute using equation 1 above.\n    \"\"\"\n    a_C = content_output[-1]\n    a_G = generated_output[-1]\n       \n    \n    # Retrieve dimensions from a_G \n    m, n_H, n_W, n_C = a_G.get_shape().as_list()\n    \n    # Reshape a_C and a_G \n    a_C_unrolled = tf.reshape(a_C, shape=[m, n_H * n_W, n_C])\n    a_G_unrolled = tf.reshape(a_G, shape=[m, n_H * n_W, n_C])\n    \n    # compute the cost with tensorflow \n    J_content =  (1/(4*n_H*n_W*n_C) )*tf.reduce_sum(tf.square( tf.subtract(a_C_unrolled, a_G_unrolled ) ))\n    \n    \n    \n    return J_content\n\nthe gram matrix of A is 𝐺𝐴=𝐴𝐴𝑇.\n\ndef gram_matrix(A):\n    \"\"\"\n    Argument:\n    A -- matrix of shape (n_C, n_H*n_W)\n    \n    Returns:\n    GA -- Gram matrix of A, of shape (n_C, n_C)\n    \"\"\"  \n    \n    \n    GA = tf.linalg.matmul(\n    A, A, transpose_b=True)\n    \n\n    return GA\n\nCompute the style cost for a single layer.\n\ndef compute_layer_style_cost(a_S, a_G):\n    \"\"\"\n    Arguments:\n    a_S -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image S \n    a_G -- tensor of dimension (1, n_H, n_W, n_C), hidden layer activations representing style of the image G\n    \n    Returns: \n    J_style_layer -- tensor representing a scalar value, style cost defined above by equation (2)\n    \"\"\"\n\n    \n    # Retrieve dimensions from a_G \n    m, n_H, n_W, n_C = a_G.get_shape().as_list()\n    \n    # Reshape the images from (n_H * n_W, n_C) to have them of shape (n_C, n_H * n_W) \n    a_S = tf.transpose(a_S)\n    a_S=tf.reshape(a_S, shape=[n_C, n_H * n_W])\n    a_G = tf.transpose(a_G)#, shape=[n_C, n_H * n_W])\n    a_G=tf.reshape(a_G, shape=[n_C, n_H * n_W])\n    \n    print(np.shape(a_S))\n    \n    # Computing gram_matrices for both images S and G \n    GS = gram_matrix(a_S)\n    GG = gram_matrix(a_G)\n    \n    # Computing the loss (≈1 line)\n    J_style_layer = J_content =  (1/(4*(n_H*n_W)**2*n_C**2) )*tf.reduce_sum(tf.square( tf.subtract(GS, GG ) ))\n    \n    \n    \n    return J_style_layer\n\nCompute style cost function, Calls individual layers cost funcxtion and applies a weight based on variable STYLE_LAYERS\n\ndef compute_style_cost(style_image_output, generated_image_output, STYLE_LAYERS=STYLE_LAYERS):\n    \"\"\"\n    Computes the overall style cost from several chosen layers\n    \n    Arguments:\n    style_image_output -- our tensorflow model\n    generated_image_output --\n    STYLE_LAYERS -- A python list containing:\n                        - the names of the layers we would like to extract style from\n                        - a coefficient for each of them\n    \n    Returns: \n    J_style -- tensor representing a scalar value, style cost defined above by equation (2)\n    \"\"\"\n    \n    # initialize the overall style cost\n    J_style = 0\n\n    # Set a_S to be the hidden layer activation from the layer we have selected.\n    # The last element of the array contains the content layer image, which must not to be used.\n    a_S = style_image_output[:-1]\n\n    # Set a_G to be the output of the choosen hidden layers.\n    # The last element of the array contains the content layer image, which must not to be used.\n    a_G = generated_image_output[:-1]\n    for i, weight in zip(range(len(a_S)), STYLE_LAYERS):  \n        # Compute style_cost for the current layer\n        J_style_layer = compute_layer_style_cost(a_S[i], a_G[i])\n\n        # Add weight * J_style_layer of this layer to overall style cost\n        J_style += weight[1] * J_style_layer\n\n    return J_style\n\nA total cost function including both style and content costs\n\n@tf.function()\ndef total_cost(J_content, J_style, alpha = 10, beta = 40):\n    \"\"\"\n    Computes the total cost function\n    \n    Arguments:\n    J_content -- content cost coded above\n    J_style -- style cost coded above\n    alpha -- hyperparameter weighting the importance of the content cost\n    beta -- hyperparameter weighting the importance of the style cost\n    \n    Returns:\n    J -- total cost as defined by the formula above.\n    \"\"\"\n    \n    J = alpha*J_content +beta*J_style\n    \n    \n    return J\n\n ### 5.3 Randomly Initialize the Image to be Generated Now, you get to initialize the “generated” image as a noisy image created from the content_image.\n\nThe generated image is slightly correlated with the content image.\nBy initializing the pixels of the generated image to be mostly noise but slightly correlated with the content image, this will help the content of the “generated” image more rapidly match the content of the “content” image.\n\n\ngenerated_image = tf.Variable(tf.image.convert_image_dtype(content_image, tf.float32))\n# noise = tf.random.uniform(tf.shape(generated_image), 0, 0.5)\n# generated_image = tf.add(generated_image, noise)\ngenerated_image = tf.clip_by_value(generated_image, clip_value_min=0.0, clip_value_max=1.0)\n\ndefine a function which loads the VGG19 model and returns a list of the outputs for the middle layers.\n\ndef get_layer_outputs(vgg, layer_names):\n    \"\"\" Creates a vgg model that returns a list of intermediate output values.\"\"\"\n    outputs = [vgg.get_layer(layer[0]).output for layer in layer_names]\n\n    model = tf.keras.Model([vgg.input], outputs)\n    return model\n\nNow, define the content layer and build the model.\n\ncontent_layer = [('block5_conv4', 1)]\n\nvgg_model_outputs = get_layer_outputs(vgg, STYLE_LAYERS + content_layer)\n\nSave the outputs for the content and style layers in separate variables.\n\ncontent_target = vgg_model_outputs(content_image)  # Content encoder\nstyle_targets = vgg_model_outputs(style_image)     # Style enconder\n\n\n# Assign the content image to be the input of the VGG model.  \n# Set a_C to be the hidden layer activation from the layer we have selected\npreprocessed_content =  tf.Variable(tf.image.convert_image_dtype(content_image, tf.float32))\na_C = vgg_model_outputs(preprocessed_content)\n\n# Set a_G to be the hidden layer activation from same layer. Here, a_G references model['conv4_2'] \n# and isn't evaluated yet. Later in the code, we'll assign the image G as the model input.\na_G = vgg_model_outputs(generated_image)\n\n# Compute the content cost\nJ_content = compute_content_cost(a_C, a_G)\n\nprint(J_content)\n\ntf.Tensor(0.0, shape=(), dtype=float32)\n\n\nsets a_S to be the tensor giving the hidden layer activation for STYLE_LAYERS.\n\n# Assign the input of the model to be the \"style\" image \npreprocessed_style =  tf.Variable(tf.image.convert_image_dtype(style_image, tf.float32))\na_S = vgg_model_outputs(preprocessed_style)\n\n# Compute the style cost\nJ_style = compute_style_cost(a_S, a_G)\nprint(J_style)\n\n(64, 1210000)\n(128, 302500)\n(256, 75625)\n(512, 18769)\n(512, 4624)\ntf.Tensor(2067.7974, shape=(), dtype=float32)\n\n\nUtils that you will need to display the images generated by the style transfer model.\n\ndef clip_0_1(image):\n    \"\"\"\n    Truncate all the pixels in the tensor to be between 0 and 1\n    \n    Arguments:\n    image -- Tensor\n    J_style -- style cost coded above\n\n    Returns:\n    Tensor\n    \"\"\"\n    return tf.clip_by_value(image, clip_value_min=0.0, clip_value_max=1.0)\n\ndef tensor_to_image(tensor):\n    \"\"\"\n    Converts the given tensor into a PIL image\n    \n    Arguments:\n    tensor -- Tensor\n    \n    Returns:\n    Image: A PIL image\n    \"\"\"\n    tensor = tensor * 255\n    tensor = np.array(tensor, dtype=np.uint8)\n    if np.ndim(tensor) > 3:\n        assert tensor.shape[0] == 1\n        tensor = tensor[0]\n    return Image.fromarray(tensor)\n\n\nTrain a step\nlearning rate lower slower\n\noptimizer = tf.keras.optimizers.Adam(learning_rate=0.03)\n\n@tf.function()\ndef train_step(generated_image):\n    with tf.GradientTape() as tape:\n        # In this function you must use the precomputed encoded images a_S and a_C\n        # Compute a_G as the vgg_model_outputs for the current generated image\n        \n\n        a_G = vgg_model_outputs(generated_image)\n        # Compute the style cost\n     \n        J_style = compute_style_cost(a_S, a_G)\n\n        \n        # Compute the content cost\n        J_content = compute_content_cost(a_C, a_G)\n        # Compute the total cost\n        J = total_cost(J_content, J_style, alpha = 10, beta = 40)  \n        \n        \n        \n    grad = tape.gradient(J, generated_image)\n\n    optimizer.apply_gradients([(grad, generated_image)])\n    generated_image.assign(clip_0_1(generated_image))\n  \n    return J\n\n\n\n\nTrain the Model\n\n# Show the generated image at some epochs\n# Uncoment to reset the style transfer process. You will need to compile the train_step function again \ngenerated_image = tf.Variable(tf.image.convert_image_dtype(content_image, tf.float32))\nepochs = 5001\nfor i in range(epochs):\n    train_step(generated_image)\n    if i % 100 == 0:\n        print(f\"Epoch {i} \")\n    if i % 100 == 0:\n        image = tensor_to_image(generated_image)\n        imshow(image)\n        image.save(f\"image_{i}.jpg\")"
  },
  {
    "objectID": "posts/others/2021-11-17-StyleTransfer.html#some-examples",
    "href": "posts/others/2021-11-17-StyleTransfer.html#some-examples",
    "title": "ThomasHSimm",
    "section": "Some Examples",
    "text": "Some Examples"
  },
  {
    "objectID": "posts/others/2021-12-16-WorkingWithVideos.html#intoduction",
    "href": "posts/others/2021-12-16-WorkingWithVideos.html#intoduction",
    "title": "ThomasHSimm",
    "section": "Intoduction",
    "text": "Intoduction\nSome bits of python code to use videos\n\n#hide\nloca='C:\\\\Users\\\\44781\\\\Documents\\\\GitHub\\\\GolfSwingTSimm\\\\data\\\\golfDB.pkl'"
  },
  {
    "objectID": "posts/others/2021-12-16-WorkingWithVideos.html#some-imports",
    "href": "posts/others/2021-12-16-WorkingWithVideos.html#some-imports",
    "title": "ThomasHSimm",
    "section": "Some imports",
    "text": "Some imports\n\nimport pandas as pd\nimport os\nimport cv2\nimport numpy as np\n\ndf = pd.read_pickle(loca)\n\nidUSE = 6\n\ndf1=df.loc[idUSE]"
  },
  {
    "objectID": "posts/others/2021-12-16-WorkingWithVideos.html#youtube-videos",
    "href": "posts/others/2021-12-16-WorkingWithVideos.html#youtube-videos",
    "title": "ThomasHSimm",
    "section": "Youtube videos",
    "text": "Youtube videos\nCan have various issues so check https://pytube.io/en/latest/user/install.html if issues in use.\nI used !pip install pytube then after a week I was getting errors. So in terminal I did\n!pip install pytube then clone directory\ngit clone git://github.com/pytube/pytube.git and finally install\ncd pytube python -m pip install .\nImport the module, and here I’ll use the golf dataset from another post\n\nfrom pytube import YouTube\n\nytID=df1['youtube_id']\nyt = YouTube('http://youtube.com/watch?v='+ytID)\n\nThe youtube object contains several videos in different formats.\n\n#collapse-output\nyt.streams\n\n[<Stream: itag=\"17\" mime_type=\"video/3gpp\" res=\"144p\" fps=\"7fps\" vcodec=\"mp4v.20.3\" acodec=\"mp4a.40.2\" progressive=\"True\" type=\"video\">, <Stream: itag=\"18\" mime_type=\"video/mp4\" res=\"360p\" fps=\"30fps\" vcodec=\"avc1.42001E\" acodec=\"mp4a.40.2\" progressive=\"True\" type=\"video\">, <Stream: itag=\"22\" mime_type=\"video/mp4\" res=\"720p\" fps=\"30fps\" vcodec=\"avc1.64001F\" acodec=\"mp4a.40.2\" progressive=\"True\" type=\"video\">, <Stream: itag=\"137\" mime_type=\"video/mp4\" res=\"1080p\" fps=\"30fps\" vcodec=\"avc1.640028\" progressive=\"False\" type=\"video\">, <Stream: itag=\"248\" mime_type=\"video/webm\" res=\"1080p\" fps=\"30fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\">, <Stream: itag=\"136\" mime_type=\"video/mp4\" res=\"720p\" fps=\"30fps\" vcodec=\"avc1.4d401f\" progressive=\"False\" type=\"video\">, <Stream: itag=\"247\" mime_type=\"video/webm\" res=\"720p\" fps=\"30fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\">, <Stream: itag=\"135\" mime_type=\"video/mp4\" res=\"480p\" fps=\"30fps\" vcodec=\"avc1.4d401f\" progressive=\"False\" type=\"video\">, <Stream: itag=\"244\" mime_type=\"video/webm\" res=\"480p\" fps=\"30fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\">, <Stream: itag=\"134\" mime_type=\"video/mp4\" res=\"360p\" fps=\"30fps\" vcodec=\"avc1.4d401e\" progressive=\"False\" type=\"video\">, <Stream: itag=\"243\" mime_type=\"video/webm\" res=\"360p\" fps=\"30fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\">, <Stream: itag=\"133\" mime_type=\"video/mp4\" res=\"240p\" fps=\"30fps\" vcodec=\"avc1.4d4015\" progressive=\"False\" type=\"video\">, <Stream: itag=\"242\" mime_type=\"video/webm\" res=\"240p\" fps=\"30fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\">, <Stream: itag=\"160\" mime_type=\"video/mp4\" res=\"144p\" fps=\"30fps\" vcodec=\"avc1.4d400c\" progressive=\"False\" type=\"video\">, <Stream: itag=\"278\" mime_type=\"video/webm\" res=\"144p\" fps=\"30fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\">, <Stream: itag=\"139\" mime_type=\"audio/mp4\" abr=\"48kbps\" acodec=\"mp4a.40.5\" progressive=\"False\" type=\"audio\">, <Stream: itag=\"140\" mime_type=\"audio/mp4\" abr=\"128kbps\" acodec=\"mp4a.40.2\" progressive=\"False\" type=\"audio\">, <Stream: itag=\"249\" mime_type=\"audio/webm\" abr=\"50kbps\" acodec=\"opus\" progressive=\"False\" type=\"audio\">, <Stream: itag=\"250\" mime_type=\"audio/webm\" abr=\"70kbps\" acodec=\"opus\" progressive=\"False\" type=\"audio\">, <Stream: itag=\"251\" mime_type=\"audio/webm\" abr=\"160kbps\" acodec=\"opus\" progressive=\"False\" type=\"audio\">]\n\n\nWe can then filter the results based on criteria. By running yt.streams.filter?? we can see more on the code, as shown below\n\nSignature:\nyt.streams.filter(\n    fps=None,\n    res=None,\n    resolution=None,\n    mime_type=None,\n    type=None,\n    subtype=None,\n    file_extension=None,\n    abr=None,\n    bitrate=None,\n    video_codec=None,\n    audio_codec=None,\n    only_audio=None,\n    only_video=None,\n    progressive=None,\n    adaptive=None,\n    is_dash=None,\n    custom_filter_functions=None,\n)\n\n\nstreams= yt.streams.filter(file_extension='mp4',res='360p')\nstreams\n\n[<Stream: itag=\"18\" mime_type=\"video/mp4\" res=\"360p\" fps=\"30fps\" vcodec=\"avc1.42001E\" acodec=\"mp4a.40.2\" progressive=\"True\" type=\"video\">, <Stream: itag=\"134\" mime_type=\"video/mp4\" res=\"360p\" fps=\"30fps\" vcodec=\"avc1.4d401e\" progressive=\"False\" type=\"video\">]\n\n\nDownload the stream\n\nstreams.first().download(filename=ytID+'.mp4')\n\n'C:\\\\Users\\\\44781\\\\Documents\\\\GitHub\\\\THS_website\\\\_notebooks\\\\iPuVhnI8pJU.mp4'"
  },
  {
    "objectID": "posts/others/2021-12-16-WorkingWithVideos.html#video-player",
    "href": "posts/others/2021-12-16-WorkingWithVideos.html#video-player",
    "title": "ThomasHSimm",
    "section": "Video Player",
    "text": "Video Player\nTo play the videos I’ll be using VLC. There can be some issues with installing so I used the exe file from here https://get.videolan.org/vlc/3.0.11/win64/vlc-3.0.11-win64.exe\nAnother fix to install issues is inserting a line like os.add_dll_directory(\"Location\\anaconda3\\\\Lib\\\\site-packages\") into the script\nMore details of using VLC with Python can be found here https://bigl.es/tooling-tuesday-using-vlc-with-python/\nImporting vlc and creating a media object and then playing it.\n\nimport vlc\n\nvidLoc=ytID+'.mp4'\n\nmedia = vlc.MediaPlayer(vidLoc)\n\nmedia.play()\n\n0\n\n\n\n.play opens a player as shown above. Note there is no pause/play/rewind buttons and the player cannot be closed. The only way to close is to use the command media.stop() or restart the kernel\n\nmedia.stop()\n\nTo make it more like a standard video player the following can be used:\n\nmedia.play() to play\nmedia.pause() to pause\nmedia.set_rate(3) to increase speed of video\nmedia.get_time() and media.set_time(5) to get and set the time of video\nmedia.stop() to close the video\n\nIntsead of doing these on the command line it makes sense to have them as button presses.\nI’ll use easygui here as it’s quite easy and I’m not too bothered about the aesthetics or functionality. Because easygui is a bit clunky. https://bigl.es/tooling-tuesday-easygui/\nSo to use create a easygui buttonbox with different features to control the video. Once clicked the button will then close (can’t keep same button open with this) and open again.\n\nimport easygui \n\n\nwhile True:\n    choice = easygui.buttonbox(title=\"@Golf Media Player\",\n       choices=[\"Play\",\"<<\",\"<<<\",\">>\",\">>>\",\"Pause\",\"Stop\"])\n\n    if choice == \"Play\":\n        media.set_rate(1)\n        media.play()\n    elif choice == \"Pause\":\n        media.pause()\n    elif choice ==\">>\":\n        media.set_rate(2)\n    elif choice==\">>>\":\n        media.set_rate(4)\n    elif choice==\"<<\":\n        timo=media.get_time()\n        timo=timo-2.5*1000\n        if timo<0:\n            timo=0\n        media.set_time(timo)\n    elif choice==\"<<<\":\n        timo=media.get_time()\n        timo=timo-5*1000\n        if timo<0:\n            timo=0\n        media.set_time(timo)\n    else:\n        time_use = media.get_time()/1000\n        print('the time is {}'.format(time_use))\n        media.stop()\n        break\n\nthe time is 15.652"
  },
  {
    "objectID": "posts/others/2021-12-16-WorkingWithVideos.html#extracting-images-from-the-video",
    "href": "posts/others/2021-12-16-WorkingWithVideos.html#extracting-images-from-the-video",
    "title": "ThomasHSimm",
    "section": "Extracting images from the video",
    "text": "Extracting images from the video\nTo work with a video it is most often easier to convert it to an image.\nTo do this I’ll use cv2 https://pypi.org/project/opencv-python/\nFirst we create a cv2 object of the video with cap = cv2.VideoCapture(vidLoc) then success, image = cap.read() to get images as we scroll through the frames, as shown below.\n\n#collapse-output\ncap = cv2.VideoCapture(vidLoc)\n\nsuccess, image = cap.read()\nwhile success:\n    success, image = cap.read()\n    print(success)\n\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nFalse\n\n\nSome useful features of the cap object that will be used are:\n\ncap.get(cv2.CAP_PROP_POS_MSEC) the duration of the video\ncap.get(cv2.CAP_PROP_FRAME_COUNT) the frame count\nframe_count / fps the duration in seconds (the previous two divided)\ncv2.CAP_PROP_FRAME_WIDTH) frame width\ncv2.CAP_PROP_FRAME_HEIGHT) frame height\ncv2.copyMakeBorder() to create an image object to save and select which area of the video to save\ncv2.imwrite(fnom, img) to save the frame as an image\n\nWe could instead use other ways as shown https://learnopencv.com/read-write-and-display-a-video-using-opencv-cpp-python/ to extract images but I’ll use the above\nThe below is modified from https://github.com/wmcnally/golfdb\nIt creates images at a given frame where the video is cropped based on the variable bbox (which is between 0 and 1)\nNote also dim=600 which adjusts the size of the output image\n\n         \ndef getImages(cap,bbox,frame_use):\n    \n    x = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH) * bbox[0])\n    y = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) * bbox[1])\n    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH) * bbox[2])\n    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) * bbox[3])\n    \n    count = 0\n    success, image = cap.read()\n    while success:\n        count += 1\n        \n        if abs(count-frame_use)==0:\n                dim=600\n                crop_img = image[y:y + h, x:x + w]\n                crop_size = crop_img.shape[:2]\n                ratio = dim / max(crop_size)\n                new_size = tuple([int(x*ratio) for x in crop_size])\n                resized = cv2.resize(crop_img, (new_size[1], new_size[0]))\n                delta_w = dim - new_size[1]\n                delta_h = dim - new_size[0]\n                top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n                left, right = delta_w // 2, delta_w - (delta_w // 2)\n                b_img = cv2.copyMakeBorder(resized, top, bottom, left, right, cv2.BORDER_CONSTANT,\n                                           value=[0.406*255, 0.456*255, 0.485*255])  # ImageNet means (BGR)\n                cv2.imwrite(os.path.join(ytID+str(count)+\".jpg\"),b_img)\n                \n        elif count > frame_use:\n            break\n        success, image = cap.read()\n\n        \n\nAnd to run the above function\n\ncap = cv2.VideoCapture(vidLoc)\ngetImages(cap,df1['bbox'],100)\n\n\nevent_names = {\n    0: 'Address',\n    1: 'Toe-up',\n    2: 'Mid-backswing (arm parallel)',\n    3: 'Top',\n    4: 'Mid-downswing (arm parallel)',\n    5: 'Impact',\n    6: 'Mid-follow-through (shaft parallel)',\n    7: 'Finish'\n}\n_, img = cap.read()\n# cv2.imshow(event_names[0], img)\n\n\nimport matplotlib.pyplot as plt\n\nplt.imshow(img)\n\n<matplotlib.image.AxesImage at 0x23f1d3cc7f0>\n\n\n\n\n\n\nbbox\n\narray([0.1453125 , 0.00138889, 0.46796875, 0.99930556])"
  },
  {
    "objectID": "posts/others/2022-04-05-speechRecog.html#resources-sources",
    "href": "posts/others/2022-04-05-speechRecog.html#resources-sources",
    "title": "ThomasHSimm",
    "section": "Resources / Sources",
    "text": "Resources / Sources\n\nhttps://www.thepythoncode.com/article/using-speech-recognition-to-convert-speech-to-text-python\nhttps://www.geeksforgeeks.org/create-a-voice-recorder-using-python/\nhttps://stackoverflow.com/questions/52283840/i-cant-install-pyaudio-on-windows-how-to-solve-error-microsoft-visual-c-14\nhttps://github.com/Uberi/speech_recognition/blob/master/examples/audio_transcribe.py"
  },
  {
    "objectID": "posts/others/2022-04-05-speechRecog.html#installs",
    "href": "posts/others/2022-04-05-speechRecog.html#installs",
    "title": "ThomasHSimm",
    "section": "Installs",
    "text": "Installs\n\n\n!pip install speechrecognition pydub\n!pip3 install sounddevice\n!pip3 install wavio\n!pip install scipy"
  },
  {
    "objectID": "posts/others/2022-04-05-speechRecog.html#create-a-recording",
    "href": "posts/others/2022-04-05-speechRecog.html#create-a-recording",
    "title": "ThomasHSimm",
    "section": "Create a recording",
    "text": "Create a recording\n\n# import required libraries\nimport sounddevice as sd\nfrom scipy.io.wavfile import write\nimport wavio as wv\n  \n# Sampling frequency\nfreq = 44100\n  \n# Recording duration\nduration = 20\n  \n# Start recorder with the given values \n# of duration and sample frequency\nrecording = sd.rec(int(duration * freq), \n                   samplerate=freq, channels=2)\n  \n# Record audio for the given number of seconds\nsd.wait()\n  \n# This will convert the NumPy array to an audio\n# file with the given sampling frequency\n# write(\"recording0.wav\", freq, recording)\n  \n# Convert the NumPy array to audio file\nfileo=\"recording1.wav\"\nwv.write(fileo, recording, freq, sampwidth=2)\n\nTo make it an arbitrary duration\nhttps://python-sounddevice.readthedocs.io/en/0.3.12/examples.html#recording-with-arbitrary-duration"
  },
  {
    "objectID": "posts/others/2022-04-05-speechRecog.html#convert-audio-to-text",
    "href": "posts/others/2022-04-05-speechRecog.html#convert-audio-to-text",
    "title": "ThomasHSimm",
    "section": "Convert Audio to text",
    "text": "Convert Audio to text\n\nimport speech_recognition as sr\n\nAUDIO_FILE = fileo\nr = sr.Recognizer()\nwith sr.AudioFile(AUDIO_FILE) as source:\n    audio = r.record(source)  # read the entire audio file\n\n\ntry:\n    # we're just using the default API key\n    print(\"->> \" + r.recognize_google(audio))\nexcept sr.UnknownValueError:\n    print(\"Google Speech Recognition could not understand audio\")\nexcept sr.RequestError as e:\n    print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\n\n->> I heard you have a compilation of every good song ever done by anybody every great song by The Beach Boys All the underground hits all the modern lovers tracks I heard you have a vinyl record on German imports I heard that you have a white label every cm\n\n\nWhen reading out Losing My Edge by LCD Soundsystem\nI heard you have a compilation of every good song ever done by anybody. Every great song by the Beach Boys. All the underground hits. All the Modern Lovers tracks. I heard you have a vinyl of every Niagra record on German import. I heard that you have a white label of every semina"
  },
  {
    "objectID": "posts/others/2022-04-08-Tweepy.html#resources",
    "href": "posts/others/2022-04-08-Tweepy.html#resources",
    "title": "ThomasHSimm",
    "section": "Resources",
    "text": "Resources\n\nA. Getting started with twitter API - Twitter Developer Platform\nhttps://developer.twitter.com/en/docs/twitter-api/getting-started/about-twitter-api\n\n\nB. Tweepy Documentation\nhttps://docs.tweepy.org/en/stable/index.html\n\n\nC. A comprehensive guide for using the Twitter API v2 with Tweepy in Python - Suhem Parack\nhttps://dev.to/twitterdev/a-comprehensive-guide-for-using-the-twitter-api-v2-using-tweepy-in-python-15d9\n\n\nD. Making queries to Twitter API on tweepy - Roberto Aguilar\nhttps://medium.com/@robguilarr/making-queries-to-twitter-api-on-tweepy-66afeb7184a4\n\n\nE. Accessing the Twitter API with Python - Mihajlo Pavloski\nhttps://stackabuse.com/accessing-the-twitter-api-with-python/\n\n\nF. How to Apply for a Twitter Developer Account - jean-christophe-chouinard\nhttps://www.jcchouinard.com/apply-for-a-twitter-developer-account/\n\n\nG. How to get Twitter API Credentials (API Keys) - jean-christophe-chouinard\nhttps://www.jcchouinard.com/twitter-api-credentials/\n\n\nH. Twitter API with Python (Complete Guide) - jean-christophe-chouinard\nhttps://www.jcchouinard.com/twitter-api/"
  },
  {
    "objectID": "posts/others/2022-04-08-Tweepy.html#setting-up-twitter-api",
    "href": "posts/others/2022-04-08-Tweepy.html#setting-up-twitter-api",
    "title": "ThomasHSimm",
    "section": "Setting up twitter API",
    "text": "Setting up twitter API\n\n1. Apply for a developer account\nBefore using the Twitter API, you first need a Twitter account, and to have obtained some credentials. The process of getting credentials could change with time, but currently it is as follows:\n\nVisit the Application Management page at https://apps.twitter.com/, and sign in with your Twitter account\nClick on the \"Create New App\" button, fill in the details and agree the Terms of Service\nNavigate to \"Keys and Access Tokens\" section and take a note of your Consumer Key and Secret\nIn the same section click on \"Create my access token\" button\nTake note of your Access Token and Access Token Secret\nAnd that's all. The consumer key/secret is used to authenticate the app that is using the Twitter API, while the access token/secret authenticates the user. All of these parameters should be treated as passwords, and should not be included in your code in plain text. One suitable way is to store them in a JSON file \"twitter_credentials.json\" and load these values from your code when needed.\nSource E. Accessing the Twitter API with Python - Mihajlo Pavloski\nTwitter API access levels and versions\n\nWhile the Twitter API v2 is the primary Twitter API, the platform currently supports previous versions (v1.1, Gnip 2.0) as well. We recommend that all users start with v2 as this is where all future innovation will happen. \n\nThe Twitter API v2 includes a few access levels to help you scale your usage on the platform. In general, new accounts can quickly sign up for free, Essential access. Should you want additional access, you may choose to apply for free Elevated access and beyond. \nSource A. Getting started with twitter API - Twitter Developer Platform\n\nI applied for elevated access. This involved filling in several questions about what I would user twitter API for and a follow up email.\nFor more info on how to apply for a twitter development account see F. How to Apply for a Twitter Developer Account - jean-christophe-chouinard\n\n\n2. Create a project/app\nTo use the twitter API you need to create a twitter App. From this you can then get the security IDs, bearer_token, API_key etc.\nFor more details on this see G. How to get Twitter API Credentials (API Keys) - jean-christophe-chouinard\n\n\n3. Set up an environment\nTwitter API bestows us several endpoints at the moment we request our App access. From which 3 of them are for searching methods, those that bring samples of the tweets we want according to specific criteria.\napi.search_30_day()\n\nPremium Search for tweets from the last 30 days.\nMonthly limitation of 4500 tweets per minute, without exceeding 25K tweets per month — Sandbox\n\napi.search_full_archive()\n\nPremium Search for tweets from March of 2006.\nMonthly limitation of 3000 tweets per minute, without exceeding 5K tweets per month — Sandbox\n\napi.search_tweets()\n\nRegular Search for tweets from the last 6–9 days as maximum.\nMonthly limitation of 3000 tweets per minute — Sandbox, this is the one that we gonna use in this case.\nFrom D. Making queries to Twitter API on tweepy - Roberto Aguilar\nThese environments are found at https://developer.twitter.com/en/account/environments.\nAnd the name of the environment is included in the particular search (as shown below label=XXX.\n\n\n4. Install tweepy\npip install tweepy"
  },
  {
    "objectID": "posts/others/2022-04-08-Tweepy.html#tweepy-code",
    "href": "posts/others/2022-04-08-Tweepy.html#tweepy-code",
    "title": "ThomasHSimm",
    "section": "Tweepy code",
    "text": "Tweepy code\n\nimport os\nos.environ[\"BEARER_TOKEN\"]=\" insert here \"\nos.environ[\"API_key\"]='insert here'\nos.environ[\"API_secret\"]=' insert here'\nos.environ[\"access_token\"] = \" insert here \"\nos.environ[\"access_token_secret\"]=\" insert here \"\n\n\nImport and check it is authenticated\n\nimport tweepy\n\n# API keys that yous saved earlier\napi_key = os.environ.get(\"API_KEY\")\napi_secrets = os.environ.get(\"API_secret\")\naccess_token = os.environ.get(\"access_token\")\naccess_secret = os.environ.get(\"access_token_secret\")\n \n# Authenticate to Twitter\nauth = tweepy.OAuthHandler(api_key,api_secrets)\nauth.set_access_token(access_token,access_secret)\n \napi = tweepy.API(auth)\n \ntry:\n    api.verify_credentials()\n    print('Successful Authentication')\nexcept:\n    print('Failed authentication')\n\n\n\nSearch tweets\nTwo main ones:\nFor last 30 days:\nouta = api.search_30_day(label, query, *, tag, fromDate, toDate, maxResults,next)\nAnd for any times:\nouta = api.search_full_archive(label, query, *, tag, fromDate, toDate,                                maxResults, next)\n\nQuery\nfrom the python script:\nThe equivalent of one premium rule/filter, with up to 1,024 characters (256 with Sandbox dev environments).\n\nThis parameter should include ALL portions of the rule/filter,including all operators, and portions of the rule should not be separated into other parameters of the query.\nFollowing adapted from D. Making queries to Twitter API on tweepy - Roberto Aguilar\n\nSearch for a term\n\nquery = 'holiday' - Containing two words\nquery = 'holiday jet2' - Containing exact words\nquery='\"jet2 braces\" \"holiday\"' - Contain one word OR another\nquery='jet2 OR \"tuiuk\"' - Hashtags and mentions\nquery = '@jet2tweets #holiday' - Exclude words\nquery='@jet2tweets -travel' - Who the tweet is sent from and to\nquery='holiday from:jet2tweets'\nquery='holiday to:jet2tweets'\n\n\nDates\nDates are in the format YYYYMMDDHHmm\nSo 0:00 21st March 2021 = ‘202103210000’\n#hide #### Tag\nfrom script:\nTags can be used to segregate rules and their matching data into\ndifferent logical groups. If a rule tag is provided, the rule tag\nis included in the 'matching_rules' attribute.\n\nIt is recommended to assign rule-specific UUIDs to rule tags and\nmaintain desired mappings on the client side.\nFollowing adapted from D. Making queries to Twitter API on tweepy - Roberto Aguilar\n\nRetweets\n\ntag='-filter:retweets' without retweets\ntag='filter:retweets' and just RTs\n\nHas links\n\ntag='-filter:links'\n\nHas media\n\ntag='-filter:media'\n\nTrusted\n\ntag='-filter:trusted'\nMore filters can be seen in the Docs\n\n\n\nThe results file\nouta=api.search_full_archive(label=label,          query=query,toDate=end_time,fromDate=start_time)\nThe information from outa is accessed for each element using _json, i.e. \nouta[0]._json gives the first search result\n\nxx=outa[0]._json\nfor ii,x in enumerate(xx):\n    print(ii,x)\n\n\nIf the tweet is longer than 140 char the text property doesn’t display all the text of the tweet.\nThis will normally then be in the extended_tweet property but also sometimes in the retweeted_status property.\nSo to get around this put in exceptions.\n\ntextAll=[]\n\nfor numa in range(0,10):\n    xx=outa1[numa]._json\n    \n    try:\n        textAll.append(numa,'1', xx['created_at'],\n              xx['retweeted_status']['extended_tweet']['full_text'] )\n    except:\n        try:\n            textAll.append(numa,'2',xx['created_at'],\n                  xx['extended_tweet']['full_text'])\n        except:\n            textAll.append(numa,'3',xx['created_at'],xx['text'])"
  },
  {
    "objectID": "posts/others/2022-07-07-SQL.html#select-from",
    "href": "posts/others/2022-07-07-SQL.html#select-from",
    "title": "ThomasHSimm",
    "section": "Select, From",
    "text": "Select, From\nThe general syntax of SELECT statments is:\nselect FirstName, LastName from Employees ;\n\n\n\nFirstName\nLastName\n\n\n\n\nAndrew\nAdams\n\n\nNancy\nEdwards\n\n\nJane\nPeacock\n\n\nMargaret\nPark\n\n\nSteve\nJohnson\n\n\nMichael\nMitchell\n\n\nRobert\nKing\n\n\nLaura\nCallahan\n\n\n\nTo retrieve all columns from the Employees table we could use “*” instead of specifying individual column names:\nselect * from Employees ;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEmployeeId\nLastName\nFirstName\nTitle\nReportsTo\nBirthDate\nHireDate\nAddress\nCity\nState\nCountry\nPostalCode\nPhone\nFax\nEmail\n\n\n\n\n1\nAdams\nAndrew\nGeneral Manager\nNone\n1962-02-18 00:00:00\n2002-08-14 00:00:00\n11120 Jasper Ave NW\nEdmonton\nAB\nCanada\nT5K 2N1\n+1 (780) 428-9482\n+1 (780) 428-3457\nandrew@chinookcorp.com\n\n\n2\nEdwards\nNancy\nSales Manager\n1\n1958-12-08 00:00:00\n2002-05-01 00:00:00\n825 8 Ave SW\nCalgary\nAB\nCanada\nT2P 2T3\n+1 (403) 262-3443\n+1 (403) 262-3322\nnancy@chinookcorp.com\n\n\n3\nPeacock\nJane\nSales Support Agent\n2\n1973-08-29 00:00:00\n2002-04-01 00:00:00\n1111 6 Ave SW\nCalgary\nAB\nCanada\nT2P 5M5\n+1 (403) 262-3443\n+1 (403) 262-6712\njane@chinookcorp.com\n\n\n4\nPark\nMargaret\nSales Support Agent\n2\n1947-09-19 00:00:00\n2003-05-03 00:00:00\n683 10 Street SW\nCalgary\nAB\nCanada\nT2P 5G3\n+1 (403) 263-4423\n+1 (403) 263-4289\nmargaret@chinookcorp.com\n\n\n5\nJohnson\nSteve\nSales Support Agent\n2\n1965-03-03 00:00:00\n2003-10-17 00:00:00\n7727B 41 Ave\nCalgary\nAB\nCanada\nT3B 1Y7\n1 (780) 836-9987\n1 (780) 836-9543\nsteve@chinookcorp.com\n\n\n6\nMitchell\nMichael\nIT Manager\n1\n1973-07-01 00:00:00\n2003-10-17 00:00:00\n5827 Bowness Road NW\nCalgary\nAB\nCanada\nT3B 0C5\n+1 (403) 246-9887\n+1 (403) 246-9899\nmichael@chinookcorp.com\n\n\n7\nKing\nRobert\nIT Staff\n6\n1970-05-29 00:00:00\n2004-01-02 00:00:00\n590 Columbia Boulevard West\nLethbridge\nAB\nCanada\nT1K 5N8\n+1 (403) 456-9986\n+1 (403) 456-8485\nrobert@chinookcorp.com\n\n\n8\nCallahan\nLaura\nIT Staff\n6\n1968-01-09 00:00:00\n2004-03-04 00:00:00\n923 7 ST NW\nLethbridge\nAB\nCanada\nT1H 1Y8\n+1 (403) 467-3351\n+1 (403) 467-8772\nlaura@chinookcorp.com"
  },
  {
    "objectID": "posts/others/2022-07-07-SQL.html#where",
    "href": "posts/others/2022-07-07-SQL.html#where",
    "title": "ThomasHSimm",
    "section": "Where",
    "text": "Where\nThe WHERE clause can be added to your query to filter results or get specific rows of data. To retrieve data for all rows in the Employees table where the ID is less than 5:\nselect * from Employees where EmployeeID < 5 ;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEmployeeId\nLastName\nFirstName\nTitle\nReportsTo\nBirthDate\nHireDate\nAddress\nCity\nState\nCountry\nPostalCode\nPhone\nFax\nEmail\n\n\n\n\n1\nAdams\nAndrew\nGeneral Manager\nNone\n1962-02-18 00:00:00\n2002-08-14 00:00:00\n11120 Jasper Ave NW\nEdmonton\nAB\nCanada\nT5K 2N1\n+1 (780) 428-9482\n+1 (780) 428-3457\nandrew@chinookcorp.com\n\n\n2\nEdwards\nNancy\nSales Manager\n1\n1958-12-08 00:00:00\n2002-05-01 00:00:00\n825 8 Ave SW\nCalgary\nAB\nCanada\nT2P 2T3\n+1 (403) 262-3443\n+1 (403) 262-3322\nnancy@chinookcorp.com\n\n\n3\nPeacock\nJane\nSales Support Agent\n2\n1973-08-29 00:00:00\n2002-04-01 00:00:00\n1111 6 Ave SW\nCalgary\nAB\nCanada\nT2P 5M5\n+1 (403) 262-3443\n+1 (403) 262-6712\njane@chinookcorp.com\n\n\n4\nPark\nMargaret\nSales Support Agent\n2\n1947-09-19 00:00:00\n2003-05-03 00:00:00\n683 10 Street SW\nCalgary\nAB\nCanada\nT2P 5G3\n+1 (403) 263-4423\n+1 (403) 263-4289\nmargaret@chinookcorp.com\n\n\n\nIn case of character based columns the values of the predicates in the where clause need to be enclosed in single quotes. To retrieve the data for the Employees names with First Name “Jane” we would issue:\nselect LastName, FirstName from Employees where FirstName = 'Jane';\n\n\n\nLastName\nFirstName\n\n\n\n\nPeacock\nJane"
  },
  {
    "objectID": "posts/others/2022-07-07-SQL.html#in-or-not",
    "href": "posts/others/2022-07-07-SQL.html#in-or-not",
    "title": "ThomasHSimm",
    "section": "In, Or, Not",
    "text": "In, Or, Not\nIn is used when we want to specify a range of conditions.\nFor example, find values of employees with last name Adams or Park\nselect FirstName, LastName from Employees\nwhere LastName In ('Adams','Park');\n\n\n\nFirstName\nLastName\n\n\n\n\nAndrew\nAdams\n\n\nMargaret\nPark\n\n\n\nCan do a similar thing with the OR operator. When using OR it is often helpful to use with brackets ()\nselect FirstName, LastName from Employees\nwhere (LastName = 'Adams' OR LastName ='Park');\n\n\n\nFirstName\nLastName\n\n\n\n\nAndrew\nAdams\n\n\nMargaret\nPark\n\n\n\nWhy the order matters and use of ()\nselect FirstName, LastName from Employees\nwhere LastName = 'Adams' OR LastName ='Park'\nand reportsto=2;\n\n\n\nFirstName\nLastName\n\n\n\n\nAndrew\nAdams\n\n\nMargaret\nPark\n\n\n\nselect FirstName, LastName from Employees\nwhere (LastName = 'Adams' OR LastName ='Park')\nand reportsto=2;\n\n\n\nFirstName\nLastName\n\n\n\n\nMargaret\nPark\n\n\n\nThis is because SQL processes AND before OR\nIn benefits over or - Long list of options - In is faster - Don’t have to consider order with IN - Can contain another select\nThe not command is represented as <>\nfor example select the titles not beginning with c\nselect title from albums\nwhere substr(title,1,1) <> 'C'\n\n\n\nTitle\n\n\n\n\nFor Those About To Rock We Salute You\n\n\nBalls to the Wall\n\n\nRestless and Wild\n\n\nLet There Be Rock\n\n\nBig Ones\n\n\nJagged Little Pill\n\n\nFacelift\n\n\nWarner 25 Anos\n\n\nPlays Metallica By Four Cellos\n\n\nAudioslave\n\n\n\nIf it is used with IN then we use NOT\nfor example cities names not starting with a vowel\nselect distinct city from station\nwhere substr(city,1,1) not in ('a','e','i','o','u')\n\n\n\nCity\n\n\n\n\nKissee Mills\n\n\nLoma Mar\n\n\nSandy Hook\n\n\nTipton\n\n\nTurner\n\n\nSlidell\n\n\nNegreet"
  },
  {
    "objectID": "posts/others/2022-07-07-SQL.html#distinct",
    "href": "posts/others/2022-07-07-SQL.html#distinct",
    "title": "ThomasHSimm",
    "section": "Distinct",
    "text": "Distinct\nFind unique values\nselect distinct artistid from albums\n\n\n\nAlbumId\n\n\n\n\n1\n\n\n2\n\n\n3\n\n\n4\n\n\n5\n\n\n6\n\n\n7\n\n\n8\n\n\n9\n\n\n10"
  },
  {
    "objectID": "posts/others/2022-07-07-SQL.html#wildcards",
    "href": "posts/others/2022-07-07-SQL.html#wildcards",
    "title": "ThomasHSimm",
    "section": "Wildcards",
    "text": "Wildcards\nTo find parts within a string can use the % wildcard\n_ works in a similar way but matches a single character (not supported by DB2)\n\n\n\n\n\n\n\nWildcard\nAction\n\n\n\n\n‘%ly’\nFind all strings ending ‘ly’\n\n\n‘To%’\nFind all strings starting ‘To’\n\n\n‘t%@gmail.com’\nFind all gmail address strings starting ‘t’\n\n\n’_ill’\nFind all strings ending ill with one other chaharacter  e.g. Kill, Bill\n\n\n\nImplementation, to implement use where and like.\nFind Names ending in t\nselect FirstName, LastName from Employees\nwhere FirstName like '%t';\n\n\n\nFirstName\nLastName\n\n\n\n\nMargaret\nPark\n\n\nRobert\nKing\n\n\n\nDownsides of wildcards:\n\nTakes longer to run (particularly at end of pattern\nBetter to use another operator e.g. =,>,<"
  },
  {
    "objectID": "posts/others/2022-07-07-SQL.html#order-data",
    "href": "posts/others/2022-07-07-SQL.html#order-data",
    "title": "ThomasHSimm",
    "section": "Order Data",
    "text": "Order Data\norder by - take name of 1 or more columns - Can use a column not retrieved - The last clause in a statement - Can use a number to represent column number - Add dsc or asc after column name to order acending or descending\nOrder employee names by last name ascending then last name descending\n\n\n\nFirstName\nLastName\n\n\n\n\nAndrew\nAdams\n\n\nLaura\nCallahan\n\n\nNancy\nEdwards\n\n\nSteve\nJohnson\n\n\nRobert\nKing\n\n\nMichael\nMitchell\n\n\nMargaret\nPark\n\n\nJane\nPeacock"
  },
  {
    "objectID": "posts/others/2022-07-07-SQL.html#aggregate-functions",
    "href": "posts/others/2022-07-07-SQL.html#aggregate-functions",
    "title": "ThomasHSimm",
    "section": "Aggregate Functions",
    "text": "Aggregate Functions\n\naverage average of a column (avg)\ncount counts number of values\nmin finds the minimum value\nmax finds the maximum value\nsum sums the column values\n\nselect count(trackid) from tracks\nwhere albumid = 10\n\n\n\ncount(trackid)\n\n\n\n\n14"
  },
  {
    "objectID": "posts/others/2022-07-07-SQL.html#group-by-and-having",
    "href": "posts/others/2022-07-07-SQL.html#group-by-and-having",
    "title": "ThomasHSimm",
    "section": "Group by and Having",
    "text": "Group by and Having\nDo the same command as above but with group by, instead of where need to use having after the group by statement\nwhere filters before data is grouped and having after data is grouped\nselect count(trackid) from tracks\ngroup by albumid\nhaving albumid = 10\n\n\n\ncount(trackid)\n\n\n\n\n14\n\n\n\nor could get the number of tracks in all albums\nselect count(trackid) from tracks\ngroup by albumid\n\n\n\ncount(trackid)\n\n\n\n\n10\n\n\n1\n\n\n3\n\n\n8\n\n\n15\n\n\n13\n\n\n12\n\n\n14\n\n\n8\n\n\n14"
  },
  {
    "objectID": "posts/others/2022-07-07-SQL.html#windowing",
    "href": "posts/others/2022-07-07-SQL.html#windowing",
    "title": "ThomasHSimm",
    "section": "Windowing",
    "text": "Windowing\n\nA window function performs a calculation across a set of table rows that are somehow related to the current row.\nComparable to the type of calculation that can be done with an aggregate function.\nUnlike regular aggregate functions, windowing does not cause rows to become grouped into a single output row — the rows retain their separate identities.\nBehind the scenes, the window function is able to access more than just the current row of the query result.\nhttps://www.postgresql.org/docs/9.1/tutorial-window.html\n\nFor example, compare each song track’s length to the average salength for each composer\nselect composer, name,\nmilliseconds/(1000*60)                                    AS track_length,\navg(milliseconds/(1000*60)) over (partition by composer)  AS avg_track_length\nfrom ah_uyekita.chinook_track\n\n\n\ncomposer\nname\ntrack_length\navg_track_length\n\n\n\n\nAaron Copland\nFanfare for the Common Man\n3.3011\n3.3011\n\n\nAaron Goldberg\nOAM’s Blues\n4.4489\n4.4489\n\n\nA.Bouchard/J.Bouchard/S.Pearlman\nAstronomy\n6.6255\n6.6255\n\n\nAC/DC\nLet There Be Rock\n6.1109\n5.1110\n\n\nAC/DC\nOverdose\n6.1553 5.1110\n\n\n\nAC/DC\nProblem Child\n5.4173\n5.1110\n\n\nAC/DC\nBad Boy Boogie\n4.4621\n5.1110\n\n\nAC/DC\nGo Down\n5.5197\n5.1110\n\n\n\n\nWindow functions\n\nCUME_DIST Calculate the cumulative distribution of a value in a set of values\nDENSE_RANK Assign a rank value to each row within a partition of a result, with no gaps in rank values.\nFIRST_VALUE Get the value of the first row in an ordered partition of a result set.\nLAG Provide access to a row at a given physical offset that comes before the current row.\nLAST_VALUE Get the value of the last row in an ordered partition of a result set.\nLEAD Provide access to a row at a given physical offset that follows the current row.\nNTILE Distribute rows of an ordered partition into a number of groups or buckets\nPERCENT_RANK Calculate the percent rank of a value in a set of values.\nRANK Assign a rank value to each row within a partition of a result set\nROW_NUMBER Assign a unique sequential integer to rows within a partition of a result set, the first row starts from 1.\n\nhttps://www.sqlservertutorial.net/sql-server-window-functions/"
  },
  {
    "objectID": "posts/others/2022-07-07-SQL.html#subqueries",
    "href": "posts/others/2022-07-07-SQL.html#subqueries",
    "title": "ThomasHSimm",
    "section": "Subqueries",
    "text": "Subqueries\nThese are queries within other queries Which merge data from multiple sources together\nget customerid and city when their invoice total is more than 20\nselect CustomerID, City\nfrom customers\nwhere customerid in (select customerid\nfrom invoices\nwhere total>20)\n\n\n\nCustomerId\nCity\n\n\n\n\n6\nPrague\n\n\n26\nFort Worth\n\n\n45\nBudapest\n\n\n46\nDublin\n\n\n\nHow many albums does the band LEd Zeppelin have?\nselect count(*)\nfrom albums\nwhere artistid IN\n(select artistid\nfrom artists\nwhere Name ='Led Zeppelin')\n\n\n\ncount(*)\n\n\n\n\n14\n\n\n\nOr what are the name of the tracks for the artist Audioslave?\nselect Name\nfrom tracks\nwhere albumid IN\n(select albumid\nfrom albums\nwhere artistid IN\n(select artistid\nfrom artists\nwhere Name ='Audioslave'))\n\n\n\nName\n\n\n\n\nCochise\n\n\nShow Me How to Live\n\n\nGasoline\n\n\nWhat You Are\n\n\nLike a Stone\n\n\nSet It Off\n\n\nShadow on the Sun\n\n\nI am the Highway\n\n\nExploder\n\n\nHypnotize"
  },
  {
    "objectID": "posts/others/2022-07-07-SQL.html#joins",
    "href": "posts/others/2022-07-07-SQL.html#joins",
    "title": "ThomasHSimm",
    "section": "Joins",
    "text": "Joins\n\nefficient storage\neasier manipulation\ngreater scalability\nlogically models a process\ntables are related through common values or keys\ndata retrival from multiple tables in one query\nonly persist for the duration of the query\n\n\nCartesian cross joins\n\neach row from first table joins with all rows from the other table\noutput size of joins in A multiplied rows in B\ncomputationally taxing\nrarely used\n\n\nselect a.title, ar.name from albums as a\ncross join artists as ar\norder by a.title\n\n\n\nTitle\nName\n\n\n\n\n…And Justice For All\nAC/DC\n\n\n…And Justice For All\nAccept\n\n\n\ntotal rows = 95425\ntotal rows of albums = 347\ntotal rows of artists = 275\n\n\nInner join\n\nselect records that have matching values in both tables\nUse on to select what joining on\njoining more table affects database performance\nCan join multiple tables- no limit\n\n\nFor example get the artist name and title of each album. N.B. albums has columns AlbumID, Title and ArtistID only\nselect artists.Name, albums.Title\nfrom artists\nINNER JOIN albums\non artists.artistid = albums.artistID\n\n\n\nName\nTitle\n\n\n\n\nAC/DC\nFor Those About To Rock We Salute You\n\n\nAC/DC\nLet There Be Rock\n\n\nAccept\nBalls to the Wall\n\n\nAccept\nRestless and Wild\n\n\nAerosmith\nBig Ones\n\n\nAlanis Morissette\nJagged Little Pill\n\n\nAlice In Chains\nFacelift\n\n\nAntônio Carlos Jobim\nWarner 25 Anos\n\n\nAntônio Carlos Jobim\nChill: Brazil (Disc 2)\n\n\nApocalyptica\nPlays Metallica By Four Cellos\n\n\n\nOr as a multiple join\nSELECT o.orderId, c.CompanyName, e.LastName\nFROM ((orders o INNER JOIN customers c ON o.customerID = c.CustomerID)\n`INNER JOIN employees e ON o.EmployeeID = e.EmployeeID);\n\n\nSelf joins\n\nTakes the table and treats it like two separate tables\nJoin the original table to itself\n\nFor example, match cities from the same state\nselect A.city,A.state,B.city, B.state\nfrom station A, station B\nwhere A.city=B.city\nand A.state=B.state\norder by A.state;\n\n\n\ncity A\nstate A\ncity B\nstate B\n\n\n\n\nSeward\nAK\nSeward\nAK\n\n\nChignik Lagoon\nAK\nChignik Lagoon\nAK\n\n\nFive Points\nAL\nFive Points\nAL\n\n\nGroveoak\nAL\nGroveoak\nAL\n\n\nNotasulga\nAL\nNotasulga\nAL\n\n\nJackson\nAL\nJackson\nAL\n\n\n….\n….\n….\n…."
  },
  {
    "objectID": "posts/others/2022-07-07-SQL.html#advanced-joins",
    "href": "posts/others/2022-07-07-SQL.html#advanced-joins",
    "title": "ThomasHSimm",
    "section": "Advanced Joins",
    "text": "Advanced Joins\n\nLeft Joins\nReturns all records from the left table and the matched records from the righ table\nThe result is NULL from the right hand side if there is no match\nRight joins are the same but from the RHS. Can be converted to left join by reversing the order\n\nFor example, find all the customers who have an invoice\nselect c.FirstNAme, c.LastName, i.InvoiceId\nfrom customers c\nLEFT JOIN invoices i on c.customerid = i.customerid\norder by c.customerid\n\n\n\nFirstName\nLastName\nInvoiceId\n\n\n\n\nLuís\nGonçalves\n98\n\n\nLuís\nGonçalves\n121\n\n\nLuís\nGonçalves\n143\n\n\nLuís\nGonçalves\n195\n\n\nLuís\nGonçalves\n316\n\n\nLuís\nGonçalves\n327\n\n\nLuís\nGonçalves\n382\n\n\nLeonie\nKöhler\n1\n\n\nLeonie\nKöhler\n12\n\n\nLeonie\nKöhler\n67\n\n\nLeonie\nKöhler\n196\n\n\nLeonie\nKöhler\n219\n\n\nLeonie\nKöhler\n241\n\n\nLeonie\nKöhler\n293\n\n\nFrançois\nTremblay\n99\n\n\n\n\n\nFull outer join\nReturns all records where there is a match in either table\n“Give me everything”\nselect c.FirstNAme, c.LastName, i.InvoiceId\nfrom customers c\nFULL OUTER JOIN invoices i on c.customerid = i.customerid\norder by c.customerid"
  },
  {
    "objectID": "posts/others/2022-07-07-SQL.html#unions",
    "href": "posts/others/2022-07-07-SQL.html#unions",
    "title": "ThomasHSimm",
    "section": "Unions",
    "text": "Unions\nCombine two or more select statements - Each select must have the same number of columns - Columns must have similar data types - Columns in the same order - Less commonly used\ne.g. combine two string statements, the first a list of occupations and the second a summary of the above\n(select concat(name,'(',substr(occupation,1,1),')') from occupations)\nunion\n(select concat('There are a total number of ',count(*),' ',occupation,'s.')\nfrom occupations\ngroup by occupation\norder by count(occupation));"
  },
  {
    "objectID": "posts/others/2022-07-07-SQL.html#substr",
    "href": "posts/others/2022-07-07-SQL.html#substr",
    "title": "ThomasHSimm",
    "section": "Substr",
    "text": "Substr\nReturns part of a string\nsubstr(string name, string position, number of characters to return)\nIf string position is negative counts from the end\ne.g., find city names that start and end with a vowel\nselect city from station\nwhere substr(city,1,1) in ('a','e','i','o','u')\nand substr(city,-1,1) in ('a','e','i','o','u')"
  },
  {
    "objectID": "posts/others/2022-07-07-SQL.html#others",
    "href": "posts/others/2022-07-07-SQL.html#others",
    "title": "ThomasHSimm",
    "section": "Others",
    "text": "Others\nLimit 1 limit the results to 1\nconcat combine multiple parts\nselect concat(name, '(', substr(occupation,1,1), ')')\nfrom occupations\nround(X,5)\nrounds X to 5 decimal places\nor round(x) to nearest integer"
  },
  {
    "objectID": "posts/others/2022-07-07-SQL.html#mode",
    "href": "posts/others/2022-07-07-SQL.html#mode",
    "title": "ThomasHSimm",
    "section": "Mode",
    "text": "Mode\nMode seems like a good free way to perform SQL. I’ve yet to work out if the course provided me a link to access datasets or if these are freely available for everyone opeing a new account. The website is not easy to navigate and I only found access to my workspaces from a link I saved and not from links on the website.\nhttps://app.mode.com/thomassimm/reports/e9412b22b846/runs/c4e7c78695f3\n\nSpark SQL\n\nhttps://files.training.databricks.com/courses/ucdavis/Lessons.dbc\nSpark SQL and DataFrames and Datasets Guides -SQL Guide from Databricks\nLearning Spark, 2nd Edition (eBook compliments of Databricks).\nIntroduction - The Internals of Spark SQL (free gitbook)"
  },
  {
    "objectID": "posts/others/2022-07-07-SQL.html#appendix",
    "href": "posts/others/2022-07-07-SQL.html#appendix",
    "title": "ThomasHSimm",
    "section": "Appendix",
    "text": "Appendix\nhttps://dev.mysql.com/doc/refman/8.0/en/union.html\nhttps://www.w3schools.com/sql/default.asp\nhttps://blog.sqlauthority.com/category/sql-puzzle/\nhttps://sqlzoo.net/wiki/SQL_Tutorial\nhttps://mode.com/sql-tutorial/introduction-to-sql\nhttps://www.postgresql.org/docs/9.1/tutorial.html\nhttps://www.coursera.org/specializations/learn-sql-basics-data-science#courses"
  },
  {
    "objectID": "posts/tensorflow/2022-09-28-Tensorflow.html#general",
    "href": "posts/tensorflow/2022-09-28-Tensorflow.html#general",
    "title": "TensorFlow cheat sheet",
    "section": "General",
    "text": "General\nLibrary websites:\nkeras.io\ntensorflow.org"
  },
  {
    "objectID": "posts/tensorflow/2022-09-28-Tensorflow.html#courses",
    "href": "posts/tensorflow/2022-09-28-Tensorflow.html#courses",
    "title": "TensorFlow cheat sheet",
    "section": "Courses",
    "text": "Courses\nI have tried both of the following courses: DeepLearning.AI TensorFlow Developer Professional Certificate and TensorFlow 2 for Deep Learning Specialization.\n\nDeepLearning.AI TensorFlow Developer Professional Certificate\nTensorFlow 2 for Deep Learning Specialization\n\nThe Tensorflow2 course is a bit longer and goes into more depth, although there are additional extended courses for the deeplearning one. The deeplearning one can be done within the current 7 days trail period of coursera. The Tensorflow2 course is tricky to do in this timeframe. This is due to more material, the harder coursework, and waiting for capstone projects to be marked.\nIn the end I only did the first course of Tensorflow2 as I found the tests had material that wasn’t explained within the course and I found the lectures lacking in detail and the instructors became increasingly boring. I gave up after getting to the capstone in course 2 (of 3) when they asked a question about an NLP network that was never explained anywhere. However, the coursework is a good challenge, so it may be worth doing the course for this alone and learning from other sources in addition to this one.\nI prefered the DeepLearning courses as they were more in depth and didn’t assume as much prior knowledge, and the presentation was better. I am currently working through the follow-up course TensorFlow: Advanced Techniques Specialization and given time will do the NLP and MLOps courses.\nSome Others:\nOne by Udacity Intro to TensorFlow for Deep Learning is being offered for free and looks okay too.\nTensorFlow example tutorials written as Jupyter notebooks and run directly in Google Colab—a hosted notebook environment that requires no setup. Click the Run in Google Colab button. Part of the TensorFlow resources."
  },
  {
    "objectID": "posts/tensorflow/2022-09-28-Tensorflow.html#resources",
    "href": "posts/tensorflow/2022-09-28-Tensorflow.html#resources",
    "title": "TensorFlow cheat sheet",
    "section": "Resources",
    "text": "Resources\n\nTensorFlow’s website recommendations\nFrançois Chollet\n\nHis book Deep Learning with Python, Second Edition can be read online.\n\nDeep learning book by Ian Goodfellow and Yoshua Bengio and Aaron Courville\n\navailable free online\n\nProbabilistic Programming & Bayesian Methods for Hackers\n\navailable online"
  },
  {
    "objectID": "posts/tensorflow/2022-09-28-Tensorflow.html#model-creations",
    "href": "posts/tensorflow/2022-09-28-Tensorflow.html#model-creations",
    "title": "TensorFlow cheat sheet",
    "section": "Model creations",
    "text": "Model creations\nThe easiest way to create a model in Keras is through keras.Sequential, which creates a neural network as a stack of layers.\nSo in the examplel below - the 1st layer has units = 4 and input_shape=2 with a relu activation function - the 2nd layer has units = 3 with a relu activation function - the 3rd layer has units = 1\nThe 3rd layer is the output layer. Since there is no activation function this would be a regression problem to predict one value.\n\nFor non sequential models or with multiple inputs/outputs see functional API\n\nTabular Data\nhttps://www.kaggle.com/code/thomassimm/premier-league-predictions-using-tensorflow\n\nmodel = tf.keras.Sequential([\n        tf.keras.layers.Dense(512//8,activation='relu',input_shape=[input_shape]),\n        tf.keras.layers.Dense(512//8,activation='relu'),\n        tf.keras.layers.Dense(1,activation='sigmoid')        \n    ])\n\n\n\nImage Data\n\nmodel = tf.keras.models.Sequential([ \n          tf.keras.layers.Convolution2D( 64,(3,3),activation='relu',input_shape=(28,28,1) ),\n          tf.keras.layers.MaxPool2D(2,2),\n          tf.keras.layers.Flatten(),\n          tf.keras.layers.Dense(256//2,activation='relu'),\n          tf.keras.layers.Dense(1,activation='sigmoid') ])\n\n\n\nLanguage Data\nThe standard language model starts with an embedding layer, this then needs to be flattened to a vector, then we can add a dense layer before an output layer.\nThe Embedding layer creates a vector-space for the text data. So for example, the words beautiful and ugly may be in opposite directions. And words such as cat and kitten may be close together in vector space.\nGlobalAveragePooling1Dcan be replaced by Flatten()\n\nmodel = tf.keras.Sequential([ \n    tf.keras.layers.Embedding(num_words,embedding_dim,input_length=maxlen),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(24,activation='relu'),\n    tf.keras.layers.Dense(5,'softmax')\n])\n\nThe model above does not take account for the order of words,\nIf we want to do this we can insert an additional layer after the embedding layer. For example, by using the LSTM model as below\n\nmodel_lstm= tf.keras.Sequential([\n    tf.keras.layers.Embedding(VOCAB_SIZE,EMBEDDING_DIM,input_length=MAXLEN),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(24,activation='relu'),\n    tf.keras.layers.Dense(NUM_CLASSES,activation='softmax')   \n])\n\nWe can even insert a conolution layer after the embedding instead\ntf.keras.layers.Conv1D(128,5,activation='relu')\nFor two consecutive layers of RNNs use return_sequences=True\n\ntf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm1_dim, return_sequences=True)),\ntf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm2_dim)),"
  },
  {
    "objectID": "posts/tensorflow/2022-09-28-Tensorflow.html#compile-the-model",
    "href": "posts/tensorflow/2022-09-28-Tensorflow.html#compile-the-model",
    "title": "TensorFlow cheat sheet",
    "section": "Compile the model",
    "text": "Compile the model\nTo compile the model, need to define the following:\n\nthe optimizer to use, i.e. how to update the parameters to improve model during fitting\nthe loss, i.e. what defines the goodness of the fit\nany metrics to record\n\n\nopt = keras.optimizers.Adam(learning_rate=0.0001)\nmodel.compile(\n    optimizer=opt,\n    loss='binary_crossentropy',\n    metrics=['binary_accuracy'],\n)"
  },
  {
    "objectID": "posts/tensorflow/2022-09-28-Tensorflow.html#evaluate-predict-the-model",
    "href": "posts/tensorflow/2022-09-28-Tensorflow.html#evaluate-predict-the-model",
    "title": "TensorFlow cheat sheet",
    "section": "Evaluate / Predict the model",
    "text": "Evaluate / Predict the model\ntest_loss, test_accuracy = model.evaluate(scaled_test_images,\ntf.keras.utils.to_categorical(test_labels),\nverbose=0)\nand more or less outputs depending on metrics used\npred = model.predict(X_sample)"
  },
  {
    "objectID": "posts/tensorflow/2022-09-28-Tensorflow.html#saving-and-loading",
    "href": "posts/tensorflow/2022-09-28-Tensorflow.html#saving-and-loading",
    "title": "TensorFlow cheat sheet",
    "section": "Saving and Loading",
    "text": "Saving and Loading\n\nSaving / Loading weights\n\nmodel_weights_file='my_file'\n\nmodel.save_weights(model_weights_file)\n\nmodel.load_weights(model_weights_file)\n\n\n# All the model\n\nmodel.save('saved_model/my_model')\n\nnew_model = tf.keras.models.load_model('saved_model/my_model')"
  },
  {
    "objectID": "posts/tensorflow/2022-09-28-Tensorflow.html#callbacks",
    "href": "posts/tensorflow/2022-09-28-Tensorflow.html#callbacks",
    "title": "TensorFlow cheat sheet",
    "section": "Callbacks",
    "text": "Callbacks\nwithin model.fit(....)\ncallbacks=[callback_function]\nhttps://www.tensorflow.org/api_docs/python/tf/keras/callbacks\n\nEarlyStopping\n\nto stop the model early if some conditions are met\n\nModelCheckpoint\n\nsave the model/model weights (https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint)\nmain data stored similar to ‘.data-00000-of-00001’\nhttps://www.tensorflow.org/tutorials/keras/save_and_load#what_are_these_files\nCan give file names with variables using {}\n\nval_loss\nval_accuracy\nbatch\nepoch\n\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    min_delta=0.0001, # minimium amount of change to count as an improvement\n    patience=20, # how many epochs to wait before stopping\n    restore_best_weights=True,\n    monitor='val_binary_accuracy',\n)\n\nfilepath = os.path.join(cwd,'checkpoints_every_epoch/checkpoint.{epoch}.{batch}')\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(\n        save_weights_only=True,\n        save_best_only=True,\n        filepath=filepath,\n    )"
  },
  {
    "objectID": "posts/tensorflow/2022-09-28-Tensorflow.html#overfitting-strategies",
    "href": "posts/tensorflow/2022-09-28-Tensorflow.html#overfitting-strategies",
    "title": "TensorFlow cheat sheet",
    "section": "Overfitting strategies",
    "text": "Overfitting strategies\n\nDropout\nThe idea behind dropout is to randomly drop out some fraction of a layer’s input units every step of training, making it much harder for the network to learn those spurious patterns in the training data that leads to overfitting.\nInstead, it has to search for broad, general patterns, whose weight patterns tend to be more robust.\nYou could also think about dropout as creating a kind of ensemble of networks like with RandomForests.\nExample useage, apply 30% dropout to the next layer\nlayers.Dropout(rate=0.3),\nlayers.Dense(16)\nExample taken from kaggle https://www.kaggle.com/code/ryanholbrook/dropout-and-batch-normalization\n\n\n\nBatch Normalization\nNormalization is important in neural networks, it can really significantly improve the results of the values of the input and output are between 0 and 1.\nIn contrast, it is not important in other models such as random forests. Hence, the input data is often normalised such as train_datagen = ImageDataGenerator(rescale=1./255.) in image models.\nSo if it is good to normalize the input data it can also be good to normalize the layers of the network. This can be done with a BatchNormalization layer.A batch normalization layer looks at each batch as it comes in, first normalizing the batch with its own mean and standard deviation, and then also putting the data on a new scale with two trainable rescaling parameters. Batchnorm, in effect, performs a kind of coordinated rescaling of its inputs.\nAs stated in https://www.kaggle.com/code/ryanholbrook/dropout-and-batch-normalization batchnorm: - batchnorm is most often added as an aid to the optimization process - but it can sometimes also help prediction performance - Models with batchnorm tend to need fewer epochs to complete training. - And can fix various problems that can cause the training to get “stuck”. - it can be used at almost any point in a network.\nlayers.Dense(16, activation='relu'),\nlayers.BatchNormalization(),\n… or between a layer and its activation function:\nlayers.Dense(16),\nlayers.BatchNormalization(),\nlayers.Activation('relu'),\n… Or if you add it as the first layer of your network it can act as a kind of adaptive preprocessor like Sci-Kit Learn’s StandardScaler."
  },
  {
    "objectID": "posts/tensorflow/2022-09-28-Tensorflow.html#multiple-inputs-and-outputs",
    "href": "posts/tensorflow/2022-09-28-Tensorflow.html#multiple-inputs-and-outputs",
    "title": "TensorFlow cheat sheet",
    "section": "Multiple inputs and outputs",
    "text": "Multiple inputs and outputs\n3 inputs and 2 outputs. Simple model\nInputs: - temp_train - nocc_train - lumbp_train\nOutputs: - out1_train - out2_train\nThings to note: - inputs in the model is a list of the Input() parts - concatentate is used with the input list to provide input to the next layers of the model - outputs in the model is a list of the output layers - When compiling use dictionary to set loss and metrics to each output - or lists ['binary_crossentropy','binary_crossentropy'] - When fitting, for the inputs/outputs either: - provide a list of the inputs [temp_train,nocc_train, lumbp_train] - give as a dict {'layer_name':variable_name}\n\n## Functional: multiple inputs\n# N.B. lowercase 'c' concatenate\nimport tensorflow as tf \nfrom tensorflow.keras.layers import Input, Dense, concatenate\ninput_shape=(1,)\n\n# get individual inputs\ninputs_temp = Input(shape=input_shape,name='temp')\ninputs_nocc = Input(shape=input_shape,name='nocc')\ninputs_lumbp = Input(shape=input_shape,name='lumbp')\n\n# combine them\ninput_list = [inputs_temp,inputs_nocc,inputs_lumbp]\ninput_layer =concatenate(input_list)\n\n# add inputs to the model for two outputs\noutput_pred1  = Dense(2,activation='sigmoid',name='out_1')(input_layer)\noutput_pred2 = Dense(2,activation='sigmoid',name='out_2')(input_layer)\n\n# output layer\noutput_list = [output_pred1, output_pred2]\n\n# create the model object\nmodel = tf.keras.Model(inputs=input_list, outputs=output_list )\n\n# show the model\nmodel.summary()\n\n# Compile\nmodel.compile(\n        optimizer='SGD',\n        loss={'out_1':'binary_crossentropy',\n              'out_2':'binary_crossentropy'},\n        metrics={'out_1':['accuracy'],\n                 'out_2':['accuracy']},\n        loss_weights=[1,0.2]\n        )\n\ntf.keras.utils.plot_model(model)\n\n\n# Define training inputs and outputs\ninputs_train = {'temp': temp_train, 'nocc': nocc_train, 'lumbp': lumbp_train}\noutputs_train = {'out_1': out1_train, 'out_2': out2_train}\n\n# fit the model\nmodel.fit(inputs_train,outputs_train)"
  },
  {
    "objectID": "posts/tensorflow/2022-09-28-Tensorflow.html#inception-images",
    "href": "posts/tensorflow/2022-09-28-Tensorflow.html#inception-images",
    "title": "TensorFlow cheat sheet",
    "section": "Inception (images)",
    "text": "Inception (images)\n\nLoad the model pre-trained weights\nImport the model architecture\nGive the model the input shape for data\nLoad the weights into the model\nFreeze all the layers\nPick out the front part of the model, as the layers to the end are more specialized\nAdd extra layers to the model that can be fitted to\n\n\n# 1- Download the inception v3 weights\n!wget --no-check-certificate \\\n    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n\n# 2- Import the inception model  \nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\n\n# Create an instance of the inception model from the local pre-trained weights\nlocal_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n\n# 3- create the model and load in the weights\npre_trained_model = InceptionV3(input_shape = (150, 150, 3),\n                                  include_top = False, \n                                  weights = None) \n\n# 4- load weights into the model\npre_trained_model.load_weights(local_weights_file)\n\n# 5- Make all the layers in the pre-trained model non-trainable\nfor layers in pre_trained_model.layers:\n    layers.trainable = False\n    \n# 6- Pick out part of the model\nlast_desired_layer = pre_trained_model.get_layer('mixed7')    \nlast_output = last_desired_layer.output\n\n# 7- Add extra layers to the model\n\n# Flatten the output layer to 1 dimension\nx = layers.Flatten()(last_output)  \n\n# Add a fully connected layer with 1024 hidden units and ReLU activation\nx = layers.Dense(1024,activation='relu')(x)\n# Add a dropout rate of 0.2\nx = layers.Dropout(0.2)(x)  \n# Add a final sigmoid layer for classification\nx = layers.Dense(1,activation='sigmoid')(x) \n\n# Create the complete model by using the Model class\nmodel = Model(inputs=pre_trained_model.input, outputs=x)\n\n# Compile the model\nmodel.compile(optimizer = RMSprop(learning_rate=0.0001), \n            loss = 'binary_crossentropy',\n            metrics = ['accuracy'])"
  },
  {
    "objectID": "posts/tensorflow/2022-09-28-Tensorflow.html#summary-info-of-model",
    "href": "posts/tensorflow/2022-09-28-Tensorflow.html#summary-info-of-model",
    "title": "TensorFlow cheat sheet",
    "section": "Summary / info of model",
    "text": "Summary / info of model\nmodel.summary()\nGet summary of the model"
  },
  {
    "objectID": "posts/tensorflow/2022-09-28-Tensorflow.html#class-or-function---metrics-and-losses",
    "href": "posts/tensorflow/2022-09-28-Tensorflow.html#class-or-function---metrics-and-losses",
    "title": "TensorFlow cheat sheet",
    "section": "Class or Function - Metrics and Losses",
    "text": "Class or Function - Metrics and Losses\nIn general, classes use camel formatting CategoricalAccuracy whereas function use underscores and lower case categorical_accuracy and sometimes initials MAE\n\nhttps://www.tensorflow.org/api_docs/python/tf/keras/losses\nhttps://www.tensorflow.org/api_docs/python/tf/keras/metrics"
  },
  {
    "objectID": "posts/tensorflow/2022-09-28-Tensorflow.html#low-level-handling-of-metrics",
    "href": "posts/tensorflow/2022-09-28-Tensorflow.html#low-level-handling-of-metrics",
    "title": "TensorFlow cheat sheet",
    "section": "Low level handling of metrics",
    "text": "Low level handling of metrics\n\nmetric.update_state() to accumulate metric stats after each batch\nmetric.result get current value of metric to display\nmetric.reset_state() to reset metric value typically at the end of epoch"
  },
  {
    "objectID": "posts/tensorflow/2022-09-28-Tensorflow.html#categorical-binary-versus-multiple",
    "href": "posts/tensorflow/2022-09-28-Tensorflow.html#categorical-binary-versus-multiple",
    "title": "TensorFlow cheat sheet",
    "section": "Categorical: Binary versus Multiple",
    "text": "Categorical: Binary versus Multiple\nFor categorical data there is a slight difference between if there are only 2 categories or more.\nGoing from binary to multiple: - We need to change activation in model from sigmoid to softmax in final Dense layer - Change loss function from binary_crossentropy to categorical_crossentropy in compile - Making data one-hot encoded, i.e. columns for each outcome - Or use SparseCategoricalCrossentropy\n\nmodel_binary = tf.keras.Sequential([\n        tf.keras.layers.Dense(512//8,activation='relu',input_shape=[input_shape]),\n        tf.keras.layers.Dense(1,activation='sigmoid')        \n    ])\n\nmodel_multi = tf.keras.Sequential([\n        tf.keras.layers.Dense(512//8,activation='relu',input_shape=[input_shape]),\n        tf.keras.layers.Dense(4,activation='softmax')        \n    ])\n\n\nmodel_binary.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\n\nmodel_multi.compile(loss='categorical_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\n\n\n# One-hot encoding method\n\ny_binary =[1,0,0,0,0,0,1,1,1,1,0,1,0,1]\n\ny_multi=[1,2,4,6,1,3,4,2,4,2,5,2,1,4,2,1]\ny_multi=tf.keras.utils.to_categorical(y_multi)\ny_multi\n\narray([[0., 1., 0., 0., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 1.],\n       [0., 1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0.],\n       [0., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0.],\n       [0., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 1., 0.],\n       [0., 0., 1., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0.],\n       [0., 0., 1., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0.]], dtype=float32)\n\n\nAlternatively, with output data like [0, 1, 4, 0, 2, 3, 3, 0, …]\nuse: - SparseCategoricalCrossentropy(from_logits=True)\n\nmodel.compile(\n                 loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n                 optimizer='adam',\n                 metrics=['accuracy'])"
  },
  {
    "objectID": "posts/tensorflow/2022-09-28-Tensorflow.html#learning-rate",
    "href": "posts/tensorflow/2022-09-28-Tensorflow.html#learning-rate",
    "title": "TensorFlow cheat sheet",
    "section": "Learning rate",
    "text": "Learning rate\nFind the best learning rate by using callbacks\nThe learning rate to use on the data below would be where the loss is low (y-axis) but not too close to where it increases or is unstable.\nSo for this on the downward part of the curve between 10E-6 and 10E-5\n\n# Set the learning rate scheduler\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(\n    lambda epoch: 1e-8 * 10**(epoch / 20) )\n    \n# Set the training parameters\nmodel_tune.compile(loss=\"mse\", optimizer=optimizer)\n    \n# train the model\nhistory = model_tune.fit(dataset, epochs=100, callbacks=[lr_schedule])\n\n# plot the results\n# Define the learning rate array\nlrs = 1e-8 * (10 ** (np.arange(100) / 20))\n\n# Plot the loss in log scale\nplt.semilogx(lrs, history.history[\"loss\"])"
  },
  {
    "objectID": "posts/tensorflow/2022-09-28-Tensorflow.html#lambda-functions",
    "href": "posts/tensorflow/2022-09-28-Tensorflow.html#lambda-functions",
    "title": "TensorFlow cheat sheet",
    "section": "lambda functions",
    "text": "lambda functions\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/Lambda\ntf.keras.layers.Lambda(     function, output_shape=None, mask=None, arguments=None, **kwargs )\nAdd a function that works on the data within the model\n\n# expand the dimensions\ntf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\n                      input_shape=[window_size])\n\n# make the output larger (can be useful if predicting to large values, but previous layer have activation function so values are close to 1)\ntf.keras.layers.Lambda(lambda x: x * 100.0)"
  },
  {
    "objectID": "posts/tensorflow/2022-09-28-Tensorflow.html#force-cpugpu",
    "href": "posts/tensorflow/2022-09-28-Tensorflow.html#force-cpugpu",
    "title": "TensorFlow cheat sheet",
    "section": "Force CPU/GPU",
    "text": "Force CPU/GPU\nhttps://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras\n\n# Check available CPU/GPU devices\n\nprint(tf.config.list_physical_devices('CPU'))\n\nprint(tf.config.list_physical_devices('GPU'))\n\nwith tf.device(\"CPU:0\"):\n    model.fit(....)\n    \nwith tf.device(\"GPU:0\"):\n    model.fit(....)"
  },
  {
    "objectID": "posts/tensorflow/2022-10-07-Tensorflow-2.html#dataset-generators",
    "href": "posts/tensorflow/2022-10-07-Tensorflow-2.html#dataset-generators",
    "title": "TensorFlow cheat sheet 2",
    "section": "Dataset generators",
    "text": "Dataset generators\nHave you ever had to work with a dataset so large that it overwhelmed your machine’s memory? Or maybe you have a complex function that needs to maintain an internal state every time it’s called, but the function is too small to justify creating its own class. In these cases and more, generators and the Python yield statement are here to help\nhttps://realpython.com/introduction-to-python-generators/\nHow yield works:\n\ndef do_yield():\n    for i in range(20):\n        yield i\n        \ngot_yield =  do_yield()\n\nprint(got_yield)\nprint(next(got_yield))\nprint(next(got_yield))\n\nnext(got_yield)\n\nprint(next(got_yield))\ngot_yield\nprint(next(got_yield))\n\nWhen datasets are large and won’t fit into memory, a way to handle this is to use detaset generators. Where data is fed into the model without loading it into memory at once.Each time we iterate the generator, it yields the next value in the series.\nAn example is below. The function takes a path to a file but returns a yield statement, or a data generator, and not a line of the data.\nAs above x,y = next(text_datagen) gets the next line of the text.\nThis can be used when fitting to the model using model.fit_generator(text_datagen)\nSee also load images.\n\ndef get_data(filepath):\n    with open(filepath,'r') as f:\n        for row in f:\n            x=row[0]\n            y=row[1]\n            yield (x,y)\n            \ntext_datagen = get_data('file.txt')\n\nmodel.fit_generator(text_datagen, steps_per_epoch=1000, epochs=5)\n\n\n# or something more practical:\ndef get_generator(features, labels, batch_size=1):\n    for n in range(int(len(features)/batch_size)):\n        x = features[n*batch_size: (n+1)*batch_size]\n        y = labels[n*batch_size: (n+1)*batch_size]\n        yield (x,y)"
  },
  {
    "objectID": "posts/tensorflow/2022-10-07-Tensorflow-2.html#the-dataset-class",
    "href": "posts/tensorflow/2022-10-07-Tensorflow-2.html#the-dataset-class",
    "title": "TensorFlow cheat sheet 2",
    "section": "The dataset Class",
    "text": "The dataset Class\n\nx=np.random.randint(0,255,(100,20,2,2))\ny=np.random.randint(0,4,size=(100,1))\n\ndataset_1 = tf.data.Dataset.from_tensor_slices(x)\n\nprint(\">>\",dataset_1.element_spec)\nprint('>> N.B. first dimension inetrpreted as batch size')\n\ndataset_2 = tf.data.Dataset.from_tensor_slices(y)\nprint(\">>\",dataset_2.element_spec)\n\ndataset_zipped = tf.data.Dataset.zip((dataset_1,dataset_2))\nprint(\">>\",dataset_zipped.element_spec)\n\ndataset_comb = tf.data.Dataset.from_tensor_slices((x,y))\nprint(\">>\",dataset_comb.element_spec)\n\n>> TensorSpec(shape=(20, 2, 2), dtype=tf.int32, name=None)\n>> N.B. first dimension inetrpreted as batch size\n>> TensorSpec(shape=(1,), dtype=tf.int32, name=None)\n>> (TensorSpec(shape=(20, 2, 2), dtype=tf.int32, name=None), TensorSpec(shape=(1,), dtype=tf.int32, name=None))\n>> (TensorSpec(shape=(20, 2, 2), dtype=tf.int32, name=None), TensorSpec(shape=(1,), dtype=tf.int32, name=None))\n\n\nCan access the values by iterating\n\ndef check3s(dataset_comb):\n    dataset_iter = iter(dataset_comb)\n    for i,x in enumerate(dataset_iter):\n        if tf.squeeze(x[1])==3:\n            print('Has 3s')\n            return\n    return 'no 3s'\ncheck3s(dataset_comb)\n\nHas 3s\n\n\n\nFilter\nFilter certain values\n\ndef label_func(image,label):\n    return tf.squeeze(label) != 3\n\ndataset_comb = dataset_comb.filter(label_func)\n\ncheck3s(dataset_comb)\n\n'no 3s'\n\n\n\n\nMap\nModify values. Below creates one-hot encoding\n\ndef map_func(image, x):\n    return (image,tf.one_hot(x,depth=3) )\ndataset_comb_2=dataset_comb.map(map_func)\n\nfor i,x in enumerate(dataset_comb):\n    if i<5:\n        print(i,x[1].numpy())    \n    else:\n        break\n        \nfor i,x in enumerate(dataset_comb_2):\n    if i<5:\n        print(i,x[1].numpy())    \n    else:\n        break\n\n0 [0]\n1 [1]\n2 [2]\n3 [2]\n4 [2]\n0 [[1. 0. 0.]]\n1 [[0. 1. 0.]]\n2 [[0. 0. 1.]]\n3 [[0. 0. 1.]]\n4 [[0. 0. 1.]]\n\n\n\ndataset.batch(20), drop_remainder=True set batch size to 16 and remove any remaining samples if not divisible\ndataset.repeat(10) set the number of epochs. No value inside is indefinitely\ndataset.shuffle(100) shuffle the data, no of sample in the buffer\ndataset.filter(function_name) filter the values use lambda or a function that returns a boolean\ndataset.map(func_name) transform the values\n\ne.g. dataset.map(lambda x:x*2) doubles all values\n\ndataset.take(1) take a value from the dataset"
  },
  {
    "objectID": "posts/tensorflow/2022-10-07-Tensorflow-2.html#tensor-math",
    "href": "posts/tensorflow/2022-10-07-Tensorflow-2.html#tensor-math",
    "title": "TensorFlow cheat sheet 2",
    "section": "Tensor Math",
    "text": "Tensor Math\n\nimport tensorflow.keras.backend as K\n\nx = K.arange(0,10)\ny = K.square(x)\ny_mean = K.mean(y)\n\nprint(f\"x = {x},\\ny = {y},\\ny_mean = {y_mean}\")\n\nx = [0 1 2 3 4 5 6 7 8 9],\ny = [ 0  1  4  9 16 25 36 49 64 81],\ny_mean = 28\n\n\n\n#hide-input\nprint(f\"tf.add([1,2],[3,4]) = {tf.add([1,2],[3,4])}\\n\")\nprint(\"Or with operator overloading\")\nprint(f\"tf.Variable([1,2])+tf.Variable([3,4]) = {tf.Variable([1,2])+tf.Variable([3,4])}\\n\")\nprint(f\"x = tf.Variable([[1,2],[3,4]])\\n\")\nx=tf.Variable([[1,2],[3,4]])\nprint(f\"tf.square(x) = {tf.square(x)}\\n\")\nprint(\"Reduces dimension by adding up components\")\nprint(f\"tf.reduce_sum(x) = {tf.reduce_sum(x)}\\n\")\n      \n\ntf.add([1,2],[3,4]) = [4 6]\n\nOr with operator overloading\ntf.Variable([1,2])+tf.Variable([3,4]) = [4 6]\n\nx = tf.Variable([[1,2],[3,4]])\n\ntf.square(x) = [[ 1  4]\n [ 9 16]]\n\nReduces dimension by adding up components\ntf.reduce_sum(x) = 10"
  },
  {
    "objectID": "posts/tensorflow/2022-10-07-Tensorflow-2.html#tensor-operations",
    "href": "posts/tensorflow/2022-10-07-Tensorflow-2.html#tensor-operations",
    "title": "TensorFlow cheat sheet 2",
    "section": "Tensor Operations",
    "text": "Tensor Operations\n\nEvaluated immediately\nTensorFlow supports two types of code execution, graph-based where all of the data and ops are loaded into a graph before evaluating them within a session, or eager based where all of the code is executed line by line.\nIf eager mode was off, tensor would not be evaluated so\nx_sq = tf.square(2)\nprint(x_sq)\nthe print statement would just show details of the tensor object, such as its name, shape, data type and all that but it would not yet store the number 4 as a value.\nOtherwise values are evaluated immediately in custom eager mode = On\n\n\nBroadcasting\nBroadcasting is where adding or subtracting two tensors of different dimensions is handled in a way where the tensor with fewer dimensions is replicated to match the dimensions of the tensor with more dimensions.\na = tf.constant([[1,2],[3,4]])\n>>tf.add(a,1)\n=tf.Tensor([[2,3],[4,5]])\nOr overloading can utilise Python syntax such as\n>>a ** 2\n=tf.Tensor([[1,4],[9,16]])\nOr using numpy math operations. TensorFlow will convert the tensor objects a and b into ndarrays, and then pass those ndarrays to the np.cos function.\n>>np.cos(a)\n=array([[ 0.54030231, -0.41614684],        [-0.9899925 , -0.65364362]])\nDon’t need to preconvert from the ndarray data type into a tensor data type. TensorFlow handles this automatically.\nndarray = np.ones([3,3])\n=[[1. 1. 1.] [1. 1. 1.] [1. 1. 1.]]\ntf.multiply(ndarray,3)\n=tf.Tensor( [[1. 1. 1.] [1. 1. 1.] [1. 1. 1.]], shape=(3,3), dtype=float64)\nTensors can be easily converted back to numpy arrays using tensor.numpy()"
  },
  {
    "objectID": "posts/tensorflow/2022-10-07-Tensorflow-2.html#simple-example-of-using-gradient-tape-to-calculate-gradient-of-a-function",
    "href": "posts/tensorflow/2022-10-07-Tensorflow-2.html#simple-example-of-using-gradient-tape-to-calculate-gradient-of-a-function",
    "title": "TensorFlow cheat sheet 2",
    "section": "Simple example of using gradient tape to calculate gradient of a function",
    "text": "Simple example of using gradient tape to calculate gradient of a function\n\ndef myfunc(x):\n    return tf.math.sin(x) + tf.math.exp(x/3)\n\nw = tf.Variable(np.arange(0, np.pi*2,.2))\nwith tf.GradientTape() as tape:\n    loss = myfunc(w)\ngradient = tape.gradient(loss, w).numpy()\n\nplt.plot(w.numpy(), loss,'.-')\nplt.grid(True)\nplt.plot(w.numpy(), gradient,'x--');\nplt.legend(['Loss','Gradient of Loss']);"
  },
  {
    "objectID": "posts/tensorflow/2022-10-07-Tensorflow-2.html#using-watch",
    "href": "posts/tensorflow/2022-10-07-Tensorflow-2.html#using-watch",
    "title": "TensorFlow cheat sheet 2",
    "section": "Using watch",
    "text": "Using watch\nIf use watch on a variable the following variables referencing that variable are also watched\nBut the calls to new functions need to be within the with statement, but the gradient getting doesn’t\n\nx = tf.Variable(np.arange(0, np.pi*2,.2))\n\nwith tf.GradientTape() as t:\n    t.watch(x)\n    \n    y = tf.sin(x)\n    z = tf.exp(y)\ndz_dx = t.gradient(z,x)\n\nplt.plot(x.numpy(),z.numpy())\nplt.plot(x.numpy(),dz_dx.numpy(),'--')\nplt.legend(['z','dz/dx'])\nplt.grid(True)"
  },
  {
    "objectID": "posts/tensorflow/2022-10-10-Tensorflow-CustomLayersModels.html#coding-custom-layer",
    "href": "posts/tensorflow/2022-10-10-Tensorflow-CustomLayersModels.html#coding-custom-layer",
    "title": "TensorFlow Low-Level cheat sheet",
    "section": "Coding custom layer",
    "text": "Coding custom layer\nA layer class is inherited from Kera’s Layer class. Hence, MyLayer(Layer):\n\ndef __init__(self, units=32):\n\nInitializes the class, accepts parameters and sets up internal variables.\nsuper().__init__() returns a temporary object of the superclass (proxy object) the Layer class. This allows us to acces methods of the base class. - make sure to pass class name and self in super() - or **kwargs\nThen build and call functions can be added to create the layer and when it is called. N.B. build can often be moved to __init__.\n\ndef build(self, input_shape):\n\nruns when instance is created\ncreates state of layers (weights)\n\ndef call(self, inputs):\n\ncall does the computation\n\n\nThe values for the parameters can be explicitly set using functions like tf.random_normal_initializer() and sepcifying a shape.\nThe key to call is defining how the parts in build and __init__ are put together to create the computation of the layer.\nAlso note that in the call the format is the same as for the functional API, where the previous layer is added at the end of the next layer.\nAll the variables (weights and biases) can also be accesed with layer_class.variables.\n\n# Create a custom layer\nfrom tensorflow.keras.layers import Layer\nimport tensorflow as tf\n\nclass SimpleDense(Layer):\n    \n    def __init__(self,units=32):\n        super(SimpleDense, self).__init__()\n        self.units = units\n        \n    def build(self, input_shape):\n        w_init = tf.random_normal_initializer()\n        self.w = tf.Variable(\n                initial_value=w_init(shape=(input_shape[-1], self.units), dtype='float32'),\n                trainable=True,name='kernel',\n                                )\n        b_init = tf.zeros_initializer()\n        self.b = tf.Variable(\n                initial_value=b_init(shape=(self.units), dtype='float32'),\n                trainable=True, name='bias',\n                                )\n    def call(self, inputs):\n        return tf.matmul(inputs, self.w)+self.b\n    \ndense_layer = SimpleDense(units=32)\n\nx = tf.ones((1,1))\n\nprint(dense_layer(x) )\nprint()\nprint(f\"Weights = {dense_layer.weights[0].numpy()} \\n\\nand biases {dense_layer.weights[1].numpy()}\")\nprint()\nprint([ var.numpy() for var in dense_layer.variables])\n\ntf.Tensor(\n[[-0.07436698 -0.02011585  0.05582675  0.05404044  0.02519816  0.02855827\n   0.00046192  0.01360966  0.04663334  0.04556176  0.04592257  0.04647708\n  -0.03950281 -0.00014847 -0.03248019  0.08354251  0.07218082 -0.01156685\n   0.04577427 -0.06801199 -0.02725383 -0.02071865  0.08600459 -0.00035707\n  -0.03410981  0.00493511 -0.05133317 -0.12937713 -0.13792662 -0.01709494\n  -0.05110807 -0.01718794]], shape=(1, 32), dtype=float32)\n\nWeights = [[-0.07436698 -0.02011585  0.05582675  0.05404044  0.02519816  0.02855827\n   0.00046192  0.01360966  0.04663334  0.04556176  0.04592257  0.04647708\n  -0.03950281 -0.00014847 -0.03248019  0.08354251  0.07218082 -0.01156685\n   0.04577427 -0.06801199 -0.02725383 -0.02071865  0.08600459 -0.00035707\n  -0.03410981  0.00493511 -0.05133317 -0.12937713 -0.13792662 -0.01709494\n  -0.05110807 -0.01718794]] \n\nand biases [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0.]\n\n[array([[-0.07436698, -0.02011585,  0.05582675,  0.05404044,  0.02519816,\n         0.02855827,  0.00046192,  0.01360966,  0.04663334,  0.04556176,\n         0.04592257,  0.04647708, -0.03950281, -0.00014847, -0.03248019,\n         0.08354251,  0.07218082, -0.01156685,  0.04577427, -0.06801199,\n        -0.02725383, -0.02071865,  0.08600459, -0.00035707, -0.03410981,\n         0.00493511, -0.05133317, -0.12937713, -0.13792662, -0.01709494,\n        -0.05110807, -0.01718794]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n      dtype=float32)]\n\n\nOr equivalently…\n\n# Create a custom layer\nfrom tensorflow.keras.layers import Layer\nimport tensorflow as tf\n\nclass MyLayer(Layer):\n    \n    def __init__(self,units,input_dim):\n        super(MyLayer, self).__init__()\n        self.w = self.add_weight(shape=(input_dim,units),\n                                initializer='random_normal',\n                                trainable=True)\n        self.b = self.add_weight(shape=(units,),\n                                 initializer='zeros',\n                                trainable=True)\n    def call(self, inputs):\n        return tf.matmul(inputs, self.w)+self.b\n    \ndense_layer = MyLayer(3,5)\n\nx = tf.ones((1,5))\n\nprint(dense_layer(x) )\nprint()\nprint(f\"Weights = {dense_layer.weights[0].numpy()} \\n\\nand biases {dense_layer.weights[1].numpy()}\")"
  },
  {
    "objectID": "posts/tensorflow/2022-10-10-Tensorflow-CustomLayersModels.html#create-a-dropout-layer-as-a-custom-layer",
    "href": "posts/tensorflow/2022-10-10-Tensorflow-CustomLayersModels.html#create-a-dropout-layer-as-a-custom-layer",
    "title": "TensorFlow Low-Level cheat sheet",
    "section": "Create a Dropout layer as a custom layer",
    "text": "Create a Dropout layer as a custom layer\nN.B. uses tf.nn primitive Neural Net (NN) Operations.\n\nclass MyDropout(Layer):\n\n    def __init__(self, rate):\n        super(MyDropout, self).__init__()\n        self.rate = rate\n        \n    def call(self, inputs):\n        # Define forward pass for dropout layer\n        return tf.nn.dropout(inputs, rate=self.rate)"
  },
  {
    "objectID": "posts/tensorflow/2022-10-10-Tensorflow-CustomLayersModels.html#add-activation-functions",
    "href": "posts/tensorflow/2022-10-10-Tensorflow-CustomLayersModels.html#add-activation-functions",
    "title": "TensorFlow Low-Level cheat sheet",
    "section": "Add activation functions",
    "text": "Add activation functions\nTo add activation functions in the layer\n\nadd activation=None to the __init__ inputs so it accepts an activation but defaults to None if doesn’t recieve one\nadd the activation to the self variable\n\nuse self.activation = tf.keras.activations.get(activation)\ni.e. so we can pass a string or a function\n\ncall the activation function within the call\n\ni.e. return  self.activation(tf.matmul(inputs, self.w)+self.b)"
  },
  {
    "objectID": "posts/tensorflow/2022-10-10-Tensorflow-CustomLayersModels.html#custom-layers-in-a-model",
    "href": "posts/tensorflow/2022-10-10-Tensorflow-CustomLayersModels.html#custom-layers-in-a-model",
    "title": "TensorFlow Low-Level cheat sheet",
    "section": "Custom layers in a model",
    "text": "Custom layers in a model\nCan create the model by passing the layer into keras.Sequential as a list. In the same way as done with other layer elements.\n\nxs = np.arange(-1,5,dtype=float)\nys = xs*2 -1\n\nmodel = tf.keras.Sequential([SimpleDense(units=1)])\n\nmodel.compile(optimizer='sgd',loss='mse')\nmodel.fit(xs,ys,epochs=500,verbose=0)\nprint(model.predict([10.]))\n\n[[18.981386]]"
  },
  {
    "objectID": "posts/tensorflow/2022-10-10-Tensorflow-CustomLayersModels.html#model-example-residual-networks",
    "href": "posts/tensorflow/2022-10-10-Tensorflow-CustomLayersModels.html#model-example-residual-networks",
    "title": "TensorFlow Low-Level cheat sheet",
    "section": "Model example Residual Networks",
    "text": "Model example Residual Networks\n\n\nfrom tensorflow.keras.layers import Conv2D, Dense, Layer\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\n\nclass CNNResidual(Layer):\n    def __init__(self, layers, filters, **kwargs):\n        super(**kwargs).__init__()\n        self.hidden = [Conv2D(filters, (3,3), activation='relu') for _ in range(layers)]\n    def call(self, inputs):\n        x = inputs\n        for layer in self.hidden:\n            x = layer(x)\n        return inputs + x\n    \nclass DNNResidual(Layer):\n    def __init__(self, layers, neurons, **kwargs):\n        super(**kwargs).__init__()\n        self.hidden = [Dense(neurons, activation='relu') for _ in range(layers)]\n    def call(self, inputs):\n        x = inputs\n        for layer in self.hidden:\n            x = layer(x)\n        return inputs + x\n\nclass MyResidual(Model):\n    def __init(self, **kwargs):\n        self.hidden1 = Dense(30, activation='relu')\n        self.block1 = CNNResidual(2, 32)\n        self.block2 = DNNResidual(2, 64)\n        self.out = Dense(1)\n    def call(self, inputs):\n        x = self.hidden1(inputs)\n        x = self.block1(x)\n        for _ in range(1,4):\n            x = self.block2(x)\n        return self.out(x)"
  },
  {
    "objectID": "posts/tensorflow/2022-10-10-Tensorflow-Images.html#load-from-directory",
    "href": "posts/tensorflow/2022-10-10-Tensorflow-Images.html#load-from-directory",
    "title": "TensorFlow Images cheat sheet",
    "section": "Load from directory",
    "text": "Load from directory\nIf files are in folders can use flow_from_directory the files would need to be separated by class and training/validation as follows for a classification\ne.g. files in folders like this:\n\n/tmp/cats-v-dogs/validation/cats\n/tmp/cats-v-dogs/validation/dogs\n/tmp/cats-v-dogs/training/cats\n`/tmp/cats-v-dogs/training/dogs\n\n\ntrain_datagen = ImageDataGenerator()\n\ntrain_generator = train_datagen.flow_from_directory(directory=TRAINING_DIR,\n                                                      batch_size=256,\n                                                      class_mode='binary',\n                                                      target_size=(150, 150))\n# Test your generators\ntrain_generator, validation_generator = train_val_generators(TRAINING_DIR, VALIDATION_DIR)\n\n# Put in the fit\nmodel.fit(train_generator,\n                    epochs=15,\n                    verbose=1,\n                    validation_data=validation_generator)"
  },
  {
    "objectID": "posts/tensorflow/2022-10-10-Tensorflow-Images.html#an-example-of-transfer-learning-on-images-with-inception",
    "href": "posts/tensorflow/2022-10-10-Tensorflow-Images.html#an-example-of-transfer-learning-on-images-with-inception",
    "title": "TensorFlow Images cheat sheet",
    "section": "An example of transfer learning on images with Inception",
    "text": "An example of transfer learning on images with Inception\n\nLoad the model pre-trained weights\nImport the model architecture\nGive the model the input shape for data\n\npre_trained_model = InceptionV3(input_shape = (150, 150, 3),                                   include_top = False,                                    weights = None)\nBetter to keep the same shape as the model uses and change your data to match it than to change input_shape to match your data.\ninclude_top=False removes top most layer of the model- the output layer\nweights=None just uses the model architecture- note the weights are loaded on a later line\n\nLoad the weights into the model\nFreeze all the layers\nPick out the front part of the model, as the layers to the end are more specialized\nAdd extra layers to the model that can be fitted to\n\nNote this uses the functional API\n\nMatch the image size of our images to that needed by the model\n\nOur model expects 150X150X3 but our data is 50X50. So we need to multiply our image size by 3. This is done by the UpSampling2D layer\nmodel = tf.keras.layers.UpSampling2D(size=(3,3))(model)\nor use resize if image is bigger tf.image.resize(image, (150, 150,))\n\n# 1- Download the inception v3 weights\n!wget --no-check-certificate \\\n    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n\n# 2- Import the inception model  \nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\n\n# Create an instance of the inception model from the local pre-trained weights\nlocal_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n\n# 3- create the model and load in the weights\npre_trained_model = InceptionV3(input_shape = (150, 150, 3),\n                                  include_top = False, \n                                  weights = None) \n\n# 4- load weights into the model\npre_trained_model.load_weights(local_weights_file)\n\n# 5- Make all the layers in the pre-trained model non-trainable\nfor layers in pre_trained_model.layers:\n    layers.trainable = False\n    \n# 6- Pick out part of the model\nlast_desired_layer = pre_trained_model.get_layer('mixed7')    \nlast_output = last_desired_layer.output\n\n# 7- Add extra layers to the model\n\n# Flatten the output layer to 1 dimension\nx = layers.Flatten()(last_output)  \n\n# Add a fully connected layer with 1024 hidden units and ReLU activation\nx = layers.Dense(1024,activation='relu')(x)\n# Add a dropout rate of 0.2\nx = layers.Dropout(0.2)(x)  \n# Add a final sigmoid layer for classification\nx = layers.Dense(1,activation='sigmoid')(x) \n\n# 8. Increase input image sizes to match that needed by model using \n# a layer before the existing model starts\n\nmodel = tf.keras.layers.UpSampling2D(size=(3,3))(model)\n\n# Create the complete model by using the Model class\nmodel = Model(inputs=pre_trained_model.input, outputs=x)\n\n\n\n# Compile the model\nmodel.compile(optimizer = RMSprop(learning_rate=0.0001), \n            loss = 'binary_crossentropy',\n            metrics = ['accuracy'])"
  },
  {
    "objectID": "posts/tensorflow/2022-10-10-Tensorflow-Images.html#r-cnn",
    "href": "posts/tensorflow/2022-10-10-Tensorflow-Images.html#r-cnn",
    "title": "TensorFlow Images cheat sheet",
    "section": "R-CNN",
    "text": "R-CNN\nR-CNN (R=region) a region based CNN to implement selective search with neural networks.\n\nTakes input images\nExtract regions using selective search method (~2k)\nExtract features using CNN from each region\n\nwarped to match AlexNet inputs\n\nClassify with support vector machine (SVM) instead of dense layers\nPlus regression to get bounding box of images\nVery slow & computationally expensive\n\n\nRoss Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik, “Rich feature hierarchies for accurate object detection and semantic segmentation” 2014"
  },
  {
    "objectID": "posts/tensorflow/2022-10-10-Tensorflow-Images.html#fast-r-cnn",
    "href": "posts/tensorflow/2022-10-10-Tensorflow-Images.html#fast-r-cnn",
    "title": "TensorFlow Images cheat sheet",
    "section": "Fast R-CNN",
    "text": "Fast R-CNN\nThe aim was to improve issues above with RCNN.\n\nEntire image is fed into the ConvNet\n\nno selective search- computationally expensive\nthis Convnet is trained on finding features\nproduces a feature map of the image\n\nEach feature map can then be fed into fully connected dense layer\n\nget feature vector of image\n\nFeature vector fed into layers to do regression and classification\n\n\nRoss Girshick, Fast R-CNN, 2015"
  },
  {
    "objectID": "posts/tensorflow/2022-10-10-Tensorflow-Images.html#faster-r-cnn",
    "href": "posts/tensorflow/2022-10-10-Tensorflow-Images.html#faster-r-cnn",
    "title": "TensorFlow Images cheat sheet",
    "section": "Faster R-CNN",
    "text": "Faster R-CNN\n\nentire image into Convnet\nsliding window to find areas of interest\nsomething called a Region Proposed Network is used with data to find and create anchor boxes on image\nThe cropped and passed to Dense layers for classification and regression.\n\n Deep Learning for Computer Vision with Python"
  },
  {
    "objectID": "posts/tensorflow/2022-10-10-Tensorflow-Images.html#object-detection-in-tensorflow",
    "href": "posts/tensorflow/2022-10-10-Tensorflow-Images.html#object-detection-in-tensorflow",
    "title": "TensorFlow Images cheat sheet",
    "section": "Object detection in TensorFlow",
    "text": "Object detection in TensorFlow\nhttps://www.tensorflow.org/hub\n\nTensorFlow Hub is a repository of trained machine learning models ready for fine-tuning and deployable anywhere. Reuse trained models like BERT and Faster R-CNN with just a few lines of code.\n\nCopy url from hub page https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1 page for faster rcnn. And copy url is “https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1”\n\nimport tensorflow as tf\nimport tensorflow-hub as hub\n\nmodule_handle = \"https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1\"\n\ndetector = hub.load(module_handle).signatures['default']\n\n\nAn example can be found here https://www.tensorflow.org/hub/tutorials/object_detection"
  },
  {
    "objectID": "posts/tensorflow/2022-10-10-Tensorflow-Images.html#object-detection-api",
    "href": "posts/tensorflow/2022-10-10-Tensorflow-Images.html#object-detection-api",
    "title": "TensorFlow Images cheat sheet",
    "section": "Object Detection API",
    "text": "Object Detection API\nhttps://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/tf2_object_detection.ipynb\nhttps://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2.md https://github.com/tensorflow/models/tree/master/research/object_detection https://www.tensorflow.org/guide/checkpoint"
  },
  {
    "objectID": "posts/tensorflow/2022-10-10-Tensorflow-NLP.html",
    "href": "posts/tensorflow/2022-10-10-Tensorflow-NLP.html",
    "title": "TensorFlow NLP cheat sheet",
    "section": "",
    "text": "Basic Implementation\nThe standard language model starts with an embedding layer, this then needs to be flattened to a vector, then we can add a dense layer before an output layer.\nThe Embedding layer creates a vector-space for the text data. So for example, the words beautiful and ugly may be in opposite directions. And words such as cat and kitten may be close together in vector space.\nGlobalAveragePooling1Dcan be replaced by Flatten()\n\nmodel = tf.keras.Sequential([ \n    tf.keras.layers.Embedding(num_words,embedding_dim,input_length=maxlen),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(24,activation='relu'),\n    tf.keras.layers.Dense(5,'softmax')\n])\n\nThe model above does not take account for the order of words,\nIf we want to do this we can insert an additional layer after the embedding layer. For example, by using the LSTM model as below\n\nmodel_lstm= tf.keras.Sequential([\n    tf.keras.layers.Embedding(VOCAB_SIZE,EMBEDDING_DIM,input_length=MAXLEN),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(24,activation='relu'),\n    tf.keras.layers.Dense(NUM_CLASSES,activation='softmax')   \n])\n\nWe can even insert a conolution layer after the embedding instead\ntf.keras.layers.Conv1D(128,5,activation='relu')\nFor two consecutive layers of RNNs use return_sequences=True\n\ntf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm1_dim, return_sequences=True)),\ntf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm2_dim)),\n\n\n\nText data Tokenizer\n\nCreate a Tokenizer instance\nFit tokenizer to text data with tokenizer.fit_on_texts(text_data)\nConvert text to sequences with sequences = tokenizer.texts_to_sequences(text_data)\n\nFor example, the following words have the indices: apple->1, brain->2, cat->3, that->4, is->5\nAnd a sequence of text within the data can be converted to a sequence: “that cat apple is brain” -> (4, 3, 1, 5, 2)\n\nGet the word index word_index = tokenizer.word_index\nGet the text back from the sequences text = tokenizer.sequences_to_texts(sequences)\n\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntokenizer = Tokenizer(oov_token=\"<OOV>\",num_words=10_000)\ntokenizer.fit_on_texts(text_data)\n\nsequences = label_tokenizer.texts_to_sequences(text_data)"
  }
]