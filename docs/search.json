[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "index.html#communicating-code-and-data",
    "href": "index.html#communicating-code-and-data",
    "title": "TH Simm Python Pages",
    "section": "Communicating code and data",
    "text": "Communicating code and data\n\nPresentation"
  },
  {
    "objectID": "index.html#links",
    "href": "index.html#links",
    "title": "TH Simm Python Pages",
    "section": "Links",
    "text": "Links\nWebsite built with Quarto visit for more info https://quarto.org/docs/websites."
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres.html#content",
    "href": "posts/Communicating_Code/CommCodePres.html#content",
    "title": "Overview: Communicating code and data",
    "section": "Content",
    "text": "Content\n\nNotebooks overview\nConverting Notebooks\nExample useage of notebooks"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres.html#jupyter-notebooks",
    "href": "posts/Communicating_Code/CommCodePres.html#jupyter-notebooks",
    "title": "Overview: Communicating code and data",
    "section": "Jupyter Notebooks",
    "text": "Jupyter Notebooks\nFrom TalkPython: Awesome Jupyter Libraries and Extensions\n\nJupyter is an amazing environment for exploring data and generating executable reports with Python. But there are many external tools, extensions, and libraries to make it so much better and make you more productive.\n\n\nA notebook consists of two parts\n\nmarkdown part where we can:\n\nwrite text, add images, links, html, LaTeX etc\n\ncode part which runs and displays output of code\n\n\nSome links:\n\nJupyter Book\nA curated list of awesome Jupyter projects\nCode Documentation QA of Code\nFastAI guide for better blogs"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres.html#example-of-a-notebook",
    "href": "posts/Communicating_Code/CommCodePres.html#example-of-a-notebook",
    "title": "Overview: Communicating code and data",
    "section": "Example of a notebook",
    "text": "Example of a notebook\nAn example notebook"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres.html#example-of-a-notebook-output",
    "href": "posts/Communicating_Code/CommCodePres.html#example-of-a-notebook-output",
    "title": "Overview: Communicating code and data",
    "section": "Example of a notebook: output",
    "text": "Example of a notebook: output\n\nimport matplotlib.pyplot as plt\nplt.plot(df2['date_of_sampling'])"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres.html#example-of-a-notebook-output-2",
    "href": "posts/Communicating_Code/CommCodePres.html#example-of-a-notebook-output-2",
    "title": "Overview: Communicating code and data",
    "section": "Example of a notebook: output 2",
    "text": "Example of a notebook: output 2\n\nimport altair as alt\nfrom vega_datasets import data\n\nmovies = alt.UrlData(\n    data.movies.url,\n    format=alt.DataFormat(parse={\"Release_Date\":\"date\"})\n)\nratings = ['G', 'NC-17', 'PG', 'PG-13', 'R']\ngenres = ['Action', 'Adventure', 'Black Comedy', 'Comedy',\n       'Concert/Performance', 'Documentary', 'Drama', 'Horror', 'Musical',\n       'Romantic Comedy', 'Thriller/Suspense', 'Western']\n\nbase = alt.Chart(movies, width=200, height=200).mark_point(filled=True).transform_calculate(\n    Rounded_IMDB_Rating = \"floor(datum.IMDB_Rating)\",\n    Hundred_Million_Production =  \"datum.Production_Budget > 100000000.0 ? 100 : 10\",\n    Release_Year = \"year(datum.Release_Date)\"\n).transform_filter(\n    alt.datum.IMDB_Rating > 0\n).transform_filter(\n    alt.FieldOneOfPredicate(field='MPAA_Rating', oneOf=ratings)\n).encode(\n    x=alt.X('Worldwide_Gross:Q', scale=alt.Scale(domain=(100000,10**9), clamp=True)),\n    y='IMDB_Rating:Q',\n    tooltip=\"Title:N\"\n)\n\n# A slider filter\nyear_slider = alt.binding_range(min=1969, max=2018, step=1)\nslider_selection = alt.selection_single(bind=year_slider, fields=['Release_Year'], name=\"Release Year_\")\n\n\nfilter_year = base.add_selection(\n    slider_selection\n).transform_filter(\n    slider_selection\n).properties(title=\"Slider Filtering\")\n\n# A dropdown filter\ngenre_dropdown = alt.binding_select(options=genres)\ngenre_select = alt.selection_single(fields=['Major_Genre'], bind=genre_dropdown, name=\"Genre\")\n\nfilter_genres = base.add_selection(\n    genre_select\n).transform_filter(\n    genre_select\n).properties(title=\"Dropdown Filtering\")\n\n#color changing marks\nrating_radio = alt.binding_radio(options=ratings)\n\nrating_select = alt.selection_single(fields=['MPAA_Rating'], bind=rating_radio, name=\"Rating\")\nrating_color_condition = alt.condition(rating_select,\n                      alt.Color('MPAA_Rating:N', legend=None),\n                      alt.value('lightgray'))\n\nhighlight_ratings = base.add_selection(\n    rating_select\n).encode(\n    color=rating_color_condition\n).properties(title=\"Radio Button Highlighting\")\n\n# Boolean selection for format changes\ninput_checkbox = alt.binding_checkbox()\ncheckbox_selection = alt.selection_single(bind=input_checkbox, name=\"Big Budget Films\")\n\nsize_checkbox_condition = alt.condition(checkbox_selection,\n                                        alt.SizeValue(25),\n                                        alt.Size('Hundred_Million_Production:Q')\n                                       )\n\nbudget_sizing = base.add_selection(\n    checkbox_selection\n).encode(\n    size=size_checkbox_condition\n).properties(title=\"Checkbox Formatting\")\n\n( filter_year | filter_genres) &  (highlight_ratings | budget_sizing  )"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres.html#communicating-when-code-is-a-large-element-of-what-is-being-presented",
    "href": "posts/Communicating_Code/CommCodePres.html#communicating-when-code-is-a-large-element-of-what-is-being-presented",
    "title": "Overview: Communicating code and data",
    "section": "Communicating when code is a large element of what is being presented",
    "text": "Communicating when code is a large element of what is being presented\n\nMicrosoft Word/ppt- type methods aren’t set-up well to include code\nProgramming files (e.g. .py) aren’t set-up well to share\nVideoing code with outputs is an option, but don’t translate to other formats (i.e. we may also need to do a written format of this)\nApps (e.g. streamlit) can be good.\n\nBut the code is hidden\n\nProgramming notebooks (e.g..ipynb) offer a good and easy to share code but with some limitations\n\nAn easier way is to convert the notebooks to html\n\ne.g. maybe someone doesn’t have python installed"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres.html#notebook-benefits",
    "href": "posts/Communicating_Code/CommCodePres.html#notebook-benefits",
    "title": "Overview: Communicating code and data",
    "section": "Notebook Benefits",
    "text": "Notebook Benefits\n\nNotebooks are intuitive\n\nYou have the code then the result of the code\nPlus can add details of how code works\nAnd it’s linear\n\nCan get things up and working quickly\nAid with communicating code\nEncourages Writing\n\nand writing things down aids thinking in the now and understanding what you did and why in the future\n\n\nCan use shell commands e.g. !pip install pandas\nCan use magic commands e.g. %%time to time a cell\n\nWith the ONS moving towards Python/R from Excel and a varied level of skills. The first of these is particularly important to aid communicating code"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres.html#what-i-have-used-to-convert-notebooks",
    "href": "posts/Communicating_Code/CommCodePres.html#what-i-have-used-to-convert-notebooks",
    "title": "Overview: Communicating code and data",
    "section": "What I have used to convert notebooks",
    "text": "What I have used to convert notebooks\n\nfastpages\n\nPreviously I converted notebooks to html via fastpages but this is now deprecated and they are recommending the use of quarto.\n\nquarto\n\nSo far I have found quarto really good and flexible (N.B. R works too)\nEasy to convert a notebook to multiple formats, including html, powerpoint, pdf, word doc\nBUT Quarto is not possible within ONS (as far as I can tell currently)\n\nnbconvert is another option I tried\n\nbut it doesn’t seem to have the functionality of fastpages or quarto.\n\nJupyter Books seems to be the best option within ONS\n\nMaybe not as good as quarto but it works!"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres.html#others",
    "href": "posts/Communicating_Code/CommCodePres.html#others",
    "title": "Overview: Communicating code and data",
    "section": "Others",
    "text": "Others\n\nI know some people use Sphinx,\n\nis recommended by QA\nFrom what I can tell sphinx on it’s own is not as easy to use as notebooks\nBut there is a jupyter extension nbsphinx\nJupyter Books uses Sphinx heavily under the hood\n\nnbdev\n\nI think is connected to quarto\n\nVoila\n\nVoilà turns Jupyter notebooks into standalone web applications.\nLooks good, bit like streamlit\nbut seems to interfere with other libraries and not checked whether works in ONS\nmercury seems similar\n\nAnything else people use and recommend?"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres.html#quarto-outputs",
    "href": "posts/Communicating_Code/CommCodePres.html#quarto-outputs",
    "title": "Overview: Communicating code and data",
    "section": "Quarto Outputs",
    "text": "Quarto Outputs\nWe can then create different files from this .ipynb Jupyter notebook using the following code:\n\nquarto render testPres.ipynb --to pptx\nquarto render testPres.ipynb --to pdf\nquarto render testPres.ipynb --to html\nquarto render testPres.ipynb --to revealjs\n\nor for Jupyter Books - jupyter-book build .\\PesticideDocs\\"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres.html#creating-a-webpage-from-this",
    "href": "posts/Communicating_Code/CommCodePres.html#creating-a-webpage-from-this",
    "title": "Overview: Communicating code and data",
    "section": "Creating a webpage from this",
    "text": "Creating a webpage from this\nTakes about 30 mins including installing chosen converter. (But can be done much quicker)\n\ncreate a Github repo for your website\nchoose the converter (e.g. Jupyter Books)\n\nAnd follow their instructions\n\ngo to settings -> Pages within the repo\n\nfew options to do\n\nOptional: add your own website url to it\n\nLink how to do this here"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres.html#example-documenting-code",
    "href": "posts/Communicating_Code/CommCodePres.html#example-documenting-code",
    "title": "Overview: Communicating code and data",
    "section": "Example: Documenting Code",
    "text": "Example: Documenting Code\n\nHere is my website for my research project on pesticides in UK food.\nThis is not the same as documentation for a package but there are parallels\n\nThis does a few things:\n\nDocuments the analysis steps I have taken including the code and outputs\n\nUseful for data transparency, useability of the code if needs modifiying/adapting, and why I did XYZ\n\nProvides a way to present the data\n\nThere is a streamlit app, but sometimes I like to be able to see the code"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres.html#example-discussing-code",
    "href": "posts/Communicating_Code/CommCodePres.html#example-discussing-code",
    "title": "Overview: Communicating code and data",
    "section": "Example: Discussing Code",
    "text": "Example: Discussing Code\n\nGP Tables example"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres.html#example-tool-to-aid-learning",
    "href": "posts/Communicating_Code/CommCodePres.html#example-tool-to-aid-learning",
    "title": "Overview: Communicating code and data",
    "section": "Example: Tool to aid learning",
    "text": "Example: Tool to aid learning\nA big area I have been using Jupyter Notebooks for is to aid learning\n\nIf you want to understand something it helps to write it down\nHaving the code next to it is a big advantage\nAnd if stored on github you can access it anywhere\n\nTensoflow cheat sheet"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres.html#example-debugging-code",
    "href": "posts/Communicating_Code/CommCodePres.html#example-debugging-code",
    "title": "Overview: Communicating code and data",
    "section": "Example: Debugging Code",
    "text": "Example: Debugging Code\n\nSince starting at ONS I have been working with understanding an existing project and latterly adding code to it\nThe project consists of multiple python files across several folders\n\nMy Python was good but lots of the functions and their useage weren’t immediately obvious to me\n\nbreak-points in VS Studio is really good to step through the code and work out what happens in the code.\n\nI had not used before with Python (but had lots with MATLAB), and it’s really useful\n\nBut it can be limited what you can do\n\ndifficult to probe code if want to write more than 1 line of code\nthe experience/knowledge exists as you go through it but no documentation to refer to later, e.g. function X does this when I give it Y etc\n\nBy copying and pasting code into Jupyter cells I could see and document how they worked (e.g. changing inputs)\n\nThis (copying and pasting) would get around code changes too (which would be an issue if modules were just imported)\nbecause this was all done in Jupyter notebook I can have a ipynb code file and a html file showing how the code works\nI could even save a pickle file of the variables at a particularly point to understand how the code would work from this point"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres.html#presenting-in-multiple-formats",
    "href": "posts/Communicating_Code/CommCodePres.html#presenting-in-multiple-formats",
    "title": "Overview: Communicating code and data",
    "section": "Presenting in multiple formats",
    "text": "Presenting in multiple formats\n\nJupyter notebooks can be used on their own or as html\nBut can also be used to create presentations, pdf/word documentation or even books\nThis presentation was done with Quarto using the revealjs format\n\nSo it is a presentation format but with a html file\n\nSome of these file types can be difficult within ONS framework to do\n\nI hit a wall when trying to go beyond html and docs with Jupyter books due to dependencies"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres.html#presenting-in-multiple-formats-video",
    "href": "posts/Communicating_Code/CommCodePres.html#presenting-in-multiple-formats-video",
    "title": "Overview: Communicating code and data",
    "section": "Presenting in multiple formats: video",
    "text": "Presenting in multiple formats: video\nVideo"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres.html#questions-comments",
    "href": "posts/Communicating_Code/CommCodePres.html#questions-comments",
    "title": "Overview: Communicating code and data",
    "section": "Questions/ Comments",
    "text": "Questions/ Comments\n\nThoughts on:\n\nusing notebooks\ndocumenting code\nencouraging communication of code across ONS areas and experiences\n\nCan we share html files? Or do we have to work within the current framework?\nAnything else?"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_html.html#what-i-have-used-to-convert-notebooks-to-html",
    "href": "posts/Communicating_Code/CommCodePres_html.html#what-i-have-used-to-convert-notebooks-to-html",
    "title": "Communicating code: Website",
    "section": "What I have used to convert notebooks to html",
    "text": "What I have used to convert notebooks to html\n\nfastpages\n\nPreviously I converted notebooks to html via fastpages but this is now deprecated and they are recommending the use of quarto.\n\nquarto\n\nSo far I have found quarto really good and flexible (N.B. R works too)\nEasy to convert a notebook to multiple formats, including html, powerpoint, pdf, word doc\nBUT Quarto is not possible if installing from non pip sources is an issue (as far as I can tell currently)\n\nnbconvert is another option I tried\n\nbut it doesn’t seem to have the functionality of fastpages or quarto.\n\nJupyter Books seems to be the best option within companies with installation issues\n\nMaybe not as good as quarto but it works!"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_html.html#others",
    "href": "posts/Communicating_Code/CommCodePres_html.html#others",
    "title": "Communicating code: Website",
    "section": "Others",
    "text": "Others\n\nI know some people use Sphinx,\n\nis recommended by QA\nFrom what I can tell sphinx on it’s own is not as easy to use as notebooks\nBut there is a jupyter extension nbsphinx\nJupyter Books uses Sphinx heavily under the hood\n\nnbdev\n\nI think is connected to quarto\n\nVoila\n\nVoilà turns Jupyter notebooks into standalone web applications.\nLooks good, bit like streamlit\nbut seems to interfere with other libraries\nmercury seems similar"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_html.html#creating-html-other-formats",
    "href": "posts/Communicating_Code/CommCodePres_html.html#creating-html-other-formats",
    "title": "Communicating code: Website",
    "section": "Creating html (& other formats)",
    "text": "Creating html (& other formats)\n\nQuarto\nInstallation is via a package i.e. .msi for Windows or .pkg for Mac. Which can cause issues.\nWorks with both ipynb and qmd files, which are both a mixture of markdown and executable code.\nThe only thing that needs to be done with the notebook is add a YAML block at the start of the notebook, like the following (raq not markdown was used):\n---\ntitle: \"Communicating code: Website\"\nsubtitle: \"Using the notebook format for a website\"\nauthor: \"Thomas H. Simm\"\nformat:\n  html:\n    toc: true\ntitle-slide-attributes:\n  data-background-size: contain\n  data-background-opacity: \"0.5\"\njupyter: python3\n---\nWe can create different files from this .ipynb Jupyter notebook using the following code:\n\nquarto render testPres.ipynb --to pptx\nquarto render testPres.ipynb --to pdf\nquarto render testPres.ipynb --to html\nquarto render testPres.ipynb --to revealjs\n\nFurther, formatting for projects (i.e. for website) can be done within the configuration file _quarto.yml\nproject:\n  type: website\n  output-dir: _site\n\nwebsite:\n  title: \"ThomasHSimm\"\n  favicon: /posts/Picture3.png\n  body-header: <img src=\"/posts/header2.png\" height=200>\n\n  navbar:\n    right:\n      - about.qmd\n      - icon: github\n        href: https://github.com/ThomasHSimm\n      - icon: mortarboard-fill\n        href: https://scholar.google.com/citations?hl=en&user=HdPDn1sAAAAJ\nformat:\n  html:\n    theme: \n      light: flatly\n      dark: darkly\n    css: styles.css"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_html.html#jupyter-books",
    "href": "posts/Communicating_Code/CommCodePres_html.html#jupyter-books",
    "title": "Communicating code: Website",
    "section": "Jupyter Books",
    "text": "Jupyter Books\nWe can create different files from this .ipynb Jupyter notebook using the following code:\n\njupyter-book build .\\PesticideDocs\\\njupyter-book build <path-to-book>\njupyter-book build <path-to-book> --builder pdfhtml\njupyter-book build <path-to-book> --builder singlehtml\n\nThe only difference in notebook is that it needs to have One header in a markdown cell for the table of contents, e.g. \n# Title of page\n\nConfiguration file\nA seperate files _config.yml is used to define how the html (or other) files will look\n# Book settings\n# Learn more at https://jupyterbook.org/customize/config.html\n\ntitle: Defra Pesticide Testing, Data Analysis\nauthor: Thomas Simm\nlogo: ONS-logo.png\nexclude_patterns: [_build, Thumbs.db, .DS_Store, \"**.ipynb_checkpoints\"]\n\n\n# Force re-execution of notebooks on each build.\n# See https://jupyterbook.org/content/execute.html\nexecute:\n  execute_notebooks: force\n\n# Define the name of the latex output file for PDF builds\nlatex:\n  latex_documents:\n    targetname: book.tex\n\n# Add a bibtex file so that we can create citations\nbibtex_bibfiles:\n  - references.bib\n\n# Information about where the book exists on the web\nrepository:\n  url: https://github.com/ThomasHSimm/Pesticide  # Online location of your book\n  path_to_book: docs  # Optional path to your book, relative to the repository root\n  branch: master  # Which branch of the repository should be used when creating links (optional)\n\n# Add GitHub buttons to your book\n# See https://jupyterbook.org/customize/config.html#add-a-link-to-your-repository\n# HTML-specific settings\nhtml:\n  favicon                   : \"_images/favicon.jpg\"  # A path to a favicon image\n  use_edit_page_button      : false  # Whether to add an \"edit this page\" button to pages. If `true`, repository information in repository: must be filled in\n  use_repository_button     : false  # Whether to add a link to your repository button\n  use_issues_button         : false  # Whether to add an \"open an issue\" button\n  use_multitoc_numbering    : true   # Continuous numbering across parts/chapters\n  extra_navbar              : Powered by <a href=\"https://jupyterbook.org\">Jupyter Book</a>\n                              <br>Home website <a href=\"https://thomashsimm.com/\">thomashsimm.com</a> # Will be displayed underneath the left navbar.\n  extra_footer              : \"\"  # Will be displayed underneath the footer.\n  google_analytics_id       : \"\"  # A GA id that can be used to track book views.\n  home_page_in_navbar       : true  # Whether to include your home page in the left Navigation Bar\n  baseurl                   : \"\"  # The base URL where your book will be hosted. Used for creating image previews and social links. e.g.: https://mypage.com/mybook/\n  comments:\n    hypothesis              : false\n    utterances              : false\n  announcement              : \"\" # A banner announcement at the top of the site.\n\nAnd in addition to the config file a table of contents file is required _toc.yml:\n# Table of contents\n# Learn more at https://jupyterbook.org/customize/toc.html\n\nformat: jb-book\nroot: intro\nchapters:\n- file: Pesticide_Plots\n- file: References\n- file: UK_areas\n- file: using_jupyter_books"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_html.html#creating-a-webpage-from-this",
    "href": "posts/Communicating_Code/CommCodePres_html.html#creating-a-webpage-from-this",
    "title": "Communicating code: Website",
    "section": "Creating a webpage from this",
    "text": "Creating a webpage from this\nTakes about 30 mins including installing the chosen converter. (But can be done much quicker)\n\ncreate a Github repo for your website\nchoose the converter (e.g. Jupyter Books)\n\nAnd follow their instructions\n\ngo to settings -> Pages within the repo\n\nfew options to do\n\nOptional: add your own website url to it\n\nLink how to do this here\nIn Quarto a command from your PC in the repo, publishes the website:\nquarto publish quarto-pub\nOr equivalently with Jupyter Books:\nghp-import -n -p -f _build/html"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_html.html#creating-directly-from-the-repo",
    "href": "posts/Communicating_Code/CommCodePres_html.html#creating-directly-from-the-repo",
    "title": "Communicating code: Website",
    "section": "Creating directly from the repo",
    "text": "Creating directly from the repo\nIf we instead want to convert notebook files directly from a repo to create a website then this can be done with Netlify.\nThis is useful if using Gitlab (i.e. not Github) or don’t want all the extra html files cluttering the repo.\n\nSteps:\nhttps://jupyterbook.org/en/stable/publish/netlify.html\n\nSign up and connect Github/Gitlab\nAdd a requirements.txt file and also toc.yml to directory\nOn netlify -> Add new site -> import from an existing repo\nInsert something like below\n\nN.B. the command:\npip install -r requirements.txt && jupyter-book build .\nand folder location \n\n\nExample:\n\nGitlab repo\nResulting website https://thomashsimm.netlify.app/intro.html\n\nAnd from the inner folder"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_notebooks.html#jupyter-notebooks",
    "href": "posts/Communicating_Code/CommCodePres_notebooks.html#jupyter-notebooks",
    "title": "Communicating code: Notebooks",
    "section": "Jupyter Notebooks",
    "text": "Jupyter Notebooks\nFrom TalkPython: Awesome Jupyter Libraries and Extensions\n\nJupyter is an amazing environment for exploring data and generating executable reports with Python. But there are many external tools, extensions, and libraries to make it so much better and make you more productive.\n\n\nA notebook consists of two parts\n\nmarkdown part where we can:\n\nwrite text, add images, links, html, LaTeX etc\n\ncode part which runs and displays output of code\n\n\nSome links:\n\nJupyter Book\nA curated list of awesome Jupyter projects\nCode Documentation QA of Code\nFastAI guide for better blogs"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_notebooks.html#example-of-a-notebook",
    "href": "posts/Communicating_Code/CommCodePres_notebooks.html#example-of-a-notebook",
    "title": "Communicating code: Notebooks",
    "section": "Example of a notebook",
    "text": "Example of a notebook\nAn example notebook"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_notebooks.html#markdown-in-a-notebook",
    "href": "posts/Communicating_Code/CommCodePres_notebooks.html#markdown-in-a-notebook",
    "title": "Communicating code: Notebooks",
    "section": "Markdown in a notebook",
    "text": "Markdown in a notebook\nSome useful commands:\n\n# Notebooks Markdown and Code and ## Markdown in a notebook\n![](ghtop_images/pest.png) looks like this\n\n\n\nAnd the same with a mp4 file ![](ghtop_images/revealjs.mp4)\n\nVideo\n\n> If we want text like this\n\n\nIf we want text like this\n\n\nOr if we want code use `a = b + c`\n\nor:\n```\na = b\na = a + c\n```\na = b + c\n\nHTML works too\n\n<img src=\"ghtop_images/pest.png\"></img>"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_notebooks.html#code-in-a-notebook",
    "href": "posts/Communicating_Code/CommCodePres_notebooks.html#code-in-a-notebook",
    "title": "Communicating code: Notebooks",
    "section": "Code in a notebook",
    "text": "Code in a notebook\nExample interactive format using altair:\n\nimport altair as alt\nfrom vega_datasets import data\n\nmovies = alt.UrlData(\n    data.movies.url,\n    format=alt.DataFormat(parse={\"Release_Date\":\"date\"})\n)\nratings = ['G', 'NC-17', 'PG', 'PG-13', 'R']\ngenres = ['Action', 'Adventure', 'Black Comedy', 'Comedy',\n       'Concert/Performance', 'Documentary', 'Drama', 'Horror', 'Musical',\n       'Romantic Comedy', 'Thriller/Suspense', 'Western']\n\nbase = alt.Chart(movies, width=200, height=200).mark_point(filled=True).transform_calculate(\n    Rounded_IMDB_Rating = \"floor(datum.IMDB_Rating)\",\n    Hundred_Million_Production =  \"datum.Production_Budget > 100000000.0 ? 100 : 10\",\n    Release_Year = \"year(datum.Release_Date)\"\n).transform_filter(\n    alt.datum.IMDB_Rating > 0\n).transform_filter(\n    alt.FieldOneOfPredicate(field='MPAA_Rating', oneOf=ratings)\n).encode(\n    x=alt.X('Worldwide_Gross:Q', scale=alt.Scale(domain=(100000,10**9), clamp=True)),\n    y='IMDB_Rating:Q',\n    tooltip=\"Title:N\"\n)\n\n# A slider filter\nyear_slider = alt.binding_range(min=1969, max=2018, step=1)\nslider_selection = alt.selection_single(bind=year_slider, fields=['Release_Year'], name=\"Release Year_\")\n\n\nfilter_year = base.add_selection(\n    slider_selection\n).transform_filter(\n    slider_selection\n).properties(title=\"Slider Filtering\")\n\n# A dropdown filter\ngenre_dropdown = alt.binding_select(options=genres)\ngenre_select = alt.selection_single(fields=['Major_Genre'], bind=genre_dropdown, name=\"Genre\")\n\nfilter_genres = base.add_selection(\n    genre_select\n).transform_filter(\n    genre_select\n).properties(title=\"Dropdown Filtering\")\n\n#color changing marks\nrating_radio = alt.binding_radio(options=ratings)\n\nrating_select = alt.selection_single(fields=['MPAA_Rating'], bind=rating_radio, name=\"Rating\")\nrating_color_condition = alt.condition(rating_select,\n                      alt.Color('MPAA_Rating:N', legend=None),\n                      alt.value('lightgray'))\n\nhighlight_ratings = base.add_selection(\n    rating_select\n).encode(\n    color=rating_color_condition\n).properties(title=\"Radio Button Highlighting\")\n\n# Boolean selection for format changes\ninput_checkbox = alt.binding_checkbox()\ncheckbox_selection = alt.selection_single(bind=input_checkbox, name=\"Big Budget Films\")\n\nsize_checkbox_condition = alt.condition(checkbox_selection,\n                                        alt.SizeValue(25),\n                                        alt.Size('Hundred_Million_Production:Q')\n                                       )\n\nbudget_sizing = base.add_selection(\n    checkbox_selection\n).encode(\n    size=size_checkbox_condition\n).properties(title=\"Checkbox Formatting\")\n\n( filter_year | filter_genres) &  (highlight_ratings | budget_sizing  )"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_notebooks.html#simpler-code-output",
    "href": "posts/Communicating_Code/CommCodePres_notebooks.html#simpler-code-output",
    "title": "Communicating code: Notebooks",
    "section": "Simpler code output",
    "text": "Simpler code output\nx = np.arange(0,np.pi,.01)\ny = np.sin(x)\nplt.plot(x,y)\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nx = np.arange(0,np.pi,.01)\ny = np.sin(x)\nplt.plot(x,y)"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_notebooks.html#notebooks-my-view",
    "href": "posts/Communicating_Code/CommCodePres_notebooks.html#notebooks-my-view",
    "title": "Communicating code: Notebooks",
    "section": "Notebooks: my view",
    "text": "Notebooks: my view\nAlthough notebooks have their validish detractors I don’t like notebooks.- Joel Grus Youtube I think if you approach them in the right way they are a super powerful tool.\nThe negatives seem to be:\n\nencourage bad practice in code (a genuine problem)\nissues around order of what cell is run (easily got around with good practice)\nissues around lack of auto complete (I don’t see the issue, use in visual studio autocomplete is there)\nno grammar/spelling correction\nissues with using git and version control\n\nthere are ways around this though"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_notebooks.html#notebook-benefits",
    "href": "posts/Communicating_Code/CommCodePres_notebooks.html#notebook-benefits",
    "title": "Communicating code: Notebooks",
    "section": "Notebook Benefits",
    "text": "Notebook Benefits\n\nNotebooks are intuitive\n\nYou have the code then the result of the code\nPlus can add details of how code works\nAnd it’s linear\n\nCan get things up and working quickly\nAid with communicating code\nEncourages Writing\n\nand writing things down aids thinking in the now and understanding what you did and why in the future\n\nFastAI guide for better blogs\n\nCan use shell commands e.g. !pip install pandas\nCan use magic commands e.g. %%time to time a cell\nEasy to convert code to a pipeline\n\nWith many companies moving towards Python/R from Excel and a varied level of skills. The first of these is particularly important to aid communicating code"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_notebooks.html#communicating-when-code-is-a-large-element-of-what-is-being-presented",
    "href": "posts/Communicating_Code/CommCodePres_notebooks.html#communicating-when-code-is-a-large-element-of-what-is-being-presented",
    "title": "Communicating code: Notebooks",
    "section": "Communicating when code is a large element of what is being presented",
    "text": "Communicating when code is a large element of what is being presented\n\nMicrosoft Word/ppt- type methods aren’t set-up well to include code\nProgramming files (e.g. .py) aren’t set-up well to share\nVideoing code with outputs is an option, but don’t translate to other formats (i.e. we may also need to do a written format of this)\nApps (e.g. streamlit) can be good.\n\nBut the code is hidden\n\nProgramming notebooks (e.g..ipynb) offer a good and easy to share code but with some limitations\n\nAn easier way is to convert the notebooks to html\n\ne.g. maybe someone doesn’t have python installed"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_notebooks.html#example-documenting-code",
    "href": "posts/Communicating_Code/CommCodePres_notebooks.html#example-documenting-code",
    "title": "Communicating code: Notebooks",
    "section": "Example: Documenting Code",
    "text": "Example: Documenting Code\n\nHere is my website for my research project on pesticides in UK food.\nThis is not the same as documentation for a package but there are parallels\n\nThis does a few things:\n\nDocuments the analysis steps I have taken including the code and outputs\n\nUseful for data transparency, useability of the code if needs modifiying/adapting, and why I did XYZ\n\nProvides a way to present the data\n\nThere is a streamlit app, but sometimes I like to be able to see the code"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_notebooks.html#example-tool-to-aid-learning",
    "href": "posts/Communicating_Code/CommCodePres_notebooks.html#example-tool-to-aid-learning",
    "title": "Communicating code: Notebooks",
    "section": "Example: Tool to aid learning",
    "text": "Example: Tool to aid learning\nA big area I have been using Jupyter Notebooks for is to aid learning\n\nIf you want to understand something it helps to write it down\nHaving the code next to it is a big advantage\nAnd if stored on github you can access it anywhere\n\nTensoflow cheat sheet"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_notebooks.html#example-debugging-code",
    "href": "posts/Communicating_Code/CommCodePres_notebooks.html#example-debugging-code",
    "title": "Communicating code: Notebooks",
    "section": "Example: Debugging Code",
    "text": "Example: Debugging Code\n\nSince starting at ONS I have been working with understanding an existing project and latterly adding code to it\nThe project consists of multiple python files across several folders\n\nMy Python was good but lots of the functions and their useage weren’t immediately obvious to me\n\nbreak-points in VS Studio is really good to step through the code and work out what happens in the code.\n\nI had not used before with Python (but had lots with MATLAB), and it’s really useful\n\nBut it can be limited what you can do\n\ndifficult to probe code if want to write more than 1 line of code\nthe experience/knowledge exists as you go through it but no documentation to refer to later, e.g. function X does this when I give it Y etc\n\nBy copying and pasting code into Jupyter cells I could see and document how they worked (e.g. changing inputs)\n\nThis (copying and pasting) would get around code changes too (which would be an issue if modules were just imported)\nbecause this was all done in Jupyter notebook I can have a ipynb code file and a html file showing how the code works\nI could even save a pickle file of the variables at a particularly point to understand how the code would work from this point"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_pres.html#content",
    "href": "posts/Communicating_Code/CommCodePres_pres.html#content",
    "title": "Communicating code: Presentations",
    "section": "Content",
    "text": "Content\n\nQuarto\n\npowerpoint https://quarto.org/docs/presentations/\nhtml\n\nJupyter books\nStreamlit"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_pres.html#creating-the-template",
    "href": "posts/Communicating_Code/CommCodePres_pres.html#creating-the-template",
    "title": "Communicating code: Presentations",
    "section": "Creating the template",
    "text": "Creating the template\n(Office info correct for Office 365 Feb 2023, Version 2301 Build 16.0.16026.20002)\nIf your workplace has a custom template or you have one you always use, you can incorporate this into quarto.\nHowever, quarto is quite specific on the form this template takes, and requires the following elements - Title Slide - Title and Content - Section Header - Two Content - Comparison - Content with Caption - Blank\nBy selecting Layout from the Home tab in powerpoint the different layouts can be seen\n\nThey can then be modified by going to View tab - Slide Master.\nIf using your own template you will need to match the names of the slides given above. These can be found by hovering over the slides on the left or right clicking on one and selecting “Rename Layout”\n\nAlternatively, create a custom template using quarto and then modify this. The following command creates the template:\nquarto pandoc -o template.pptx --print-default-data-file reference.pptx\nThen go to View tab - Slide Master and modify each slide layout.\nNote if you are trying to match a template, some tips: - go to Design -> Slide Size and match this to your template - when View tab - Slide Master is selected go to first tab (see above it will be left indented) on one you are copying from and select all on this then paste to the new template - these will be background images and other things that want to be passed to all slides - Check other slides for images and font-styles etc to match to the new template"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_pres.html#load-the-template",
    "href": "posts/Communicating_Code/CommCodePres_pres.html#load-the-template",
    "title": "Communicating code: Presentations",
    "section": "Load the template",
    "text": "Load the template\nTo load the template the first cell in the notebook needs to be modified as follows to reference the template.pptx file.\nformat:\n  pptx:\n    reference-doc: template.pptx\n    slide-level: 2\nIn addition, we can also specify here the rule by which a new slide is defined. If slide-level: 2 is used a new slide is defined by “##’ and a new section header by ‘#’. So if we used ‘###’ this would be a heading within the slide.\nIf slide-level: 1 is used a new slide is defined by “#’ and ‘##’ this would be a heading within the slide (this is normally the default)."
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_pres.html#check-the-slides",
    "href": "posts/Communicating_Code/CommCodePres_pres.html#check-the-slides",
    "title": "Communicating code: Presentations",
    "section": "Check the slides",
    "text": "Check the slides\nI have found creation of slides to powerpoint more prone to strange results than if .doc/.pdf/.html are used.\nSo check the slides, see if interactive content or code has been included (probably not) and if the slide content goes outside the slide.\n\nIn the example above - There is overlap of text on a slide - Strange ouput of a slide - Code output not displayed"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_pres.html#adding-style-to-revealjs",
    "href": "posts/Communicating_Code/CommCodePres_pres.html#adding-style-to-revealjs",
    "title": "Communicating code: Presentations",
    "section": "Adding style to revealjs",
    "text": "Adding style to revealjs\nA simple way to add template like details to a revealjs file is to add a style.css sheet.\nIn the example below, the style sheet adds logo.png to the bottom right of each sheet\nThe file style.css looks like this:\n.reveal .slide-logo {\n  display: block;\n  position: fixed;\n  top: unset !important;\n  left: unset !important;\n  bottom: 50px;\n  right: 12px;\n  height: 100px !important;\n  width: 100x !important;\n  max-width: unset !important;\n  max-height: unset !important;\n}\nAnd the revealjs part at the top of the jupyter notebook looks like this\nrevealjs:\n    slide-number: true\n    height: 1080\n    width: 1920\n    logo: logo.png\n    css: style.css\nSo this would then look like the following, with the logo (logo.png) in the bottom right, and size and positioning given by the css file"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_pres.html#what-the-revealjs-file-looks-like",
    "href": "posts/Communicating_Code/CommCodePres_pres.html#what-the-revealjs-file-looks-like",
    "title": "Communicating code: Presentations",
    "section": "What the revealjs file looks like",
    "text": "What the revealjs file looks like\nVideo"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_pres.html#streamlit-functionality-overview",
    "href": "posts/Communicating_Code/CommCodePres_pres.html#streamlit-functionality-overview",
    "title": "Communicating code: Presentations",
    "section": "Streamlit Functionality: overview",
    "text": "Streamlit Functionality: overview\nStreamlit allows various functionality:\n\ntextbox\nimages/videos\ncharts/tables\nmenus/buttons\netc"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_pres.html#streamlit-functionality-streamlit_layout",
    "href": "posts/Communicating_Code/CommCodePres_pres.html#streamlit-functionality-streamlit_layout",
    "title": "Communicating code: Presentations",
    "section": "Streamlit Functionality: streamlit_layout",
    "text": "Streamlit Functionality: streamlit_layout\nBut unlike some apps (am thinking MATLAB GUIs) you can’t create the look and functionality separately. So if you want something in a certain position it can be tricky. HTML can be used with st.markdown to give more control but it isn’t recommended to use by streamlit.\nInstead, to create the layout as you would like they have the following features:"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_pres.html#streamlit-functionality-columns-and-sidebar",
    "href": "posts/Communicating_Code/CommCodePres_pres.html#streamlit-functionality-columns-and-sidebar",
    "title": "Communicating code: Presentations",
    "section": "Streamlit Functionality: columns and sidebar",
    "text": "Streamlit Functionality: columns and sidebar\nThe most useable are the first two: columns and sidebar\nColumns allows us to split the app vertically. The code is fairly simple:\nEither colL, colM, colR = st.columns(3) for 3 equal columns or to split columns with different sizes:\ncolL, _, colR = st.columns((10, 5, 20))\nwith colL:\n    st.write('On the left')\nwith colR:\n    st.write('On the right twice as big as left')\nst.sidebar just adds a sidebar to the app that can be hidden or shown.\nAnything in the sidebar is just prefixed by st.sidebar so:\nst.sidebar.write('I am in the sidebar')\nst.write('I am in the main app')\nst.sidebar.write('I am back in the sidebar')"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_pres.html#streamlit-functionality-html",
    "href": "posts/Communicating_Code/CommCodePres_pres.html#streamlit-functionality-html",
    "title": "Communicating code: Presentations",
    "section": "Streamlit Functionality: html",
    "text": "Streamlit Functionality: html\nIt is possible to add various additional personalisations using html. BUT it does come with security risks and so is [not recommended]](https://github.com/streamlit/streamlit/issues/152)\n\nBut it does allow much more control over the layout of the app that can be useful for a presentation: - Can add a background image - Can add background color to a textbox - Control over positioning of widgets - lots more\nHTML is implementated using st.markdown with unsafe_allow_html=True inside the former"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_pres.html#streamlit-functionality-html-examples",
    "href": "posts/Communicating_Code/CommCodePres_pres.html#streamlit-functionality-html-examples",
    "title": "Communicating code: Presentations",
    "section": "Streamlit Functionality: html examples",
    "text": "Streamlit Functionality: html examples\nadd background to a text box\ntext = \"Code Examples\"\n        st.markdown(f'<center><p style=font-family:\"Calibri\";background-color:#FFFFFF;color:#000000;font-size:42px;border-radius:10%><b>{text}</b></p></center>', unsafe_allow_html=True)\n\nOr to add a background image\nimport streamlit as st\nimport base64\n\n@st.cache(allow_output_mutation=True)\ndef get_base64_of_bin_file(bin_file):\n    with open(bin_file, 'rb') as f:\n        data = f.read()\n    return base64.b64encode(data).decode()\n\ndef set_png_as_page_bg(png_file):\n    bin_str = get_base64_of_bin_file(png_file) \n    page_bg_img = '''\n    <style>\n    .stApp {\n    background-image: url(\"data:image/png;base64,%s\");\n    background-size: contain;\n    background-repeat: no-repeat;\n    background-attachment: scroll; # doesn't work\n    }\n    </style>\n    ''' % bin_str\n    st.markdown(page_bg_img, unsafe_allow_html=True)\n    return"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_pres.html#streamlit-functionality-echo",
    "href": "posts/Communicating_Code/CommCodePres_pres.html#streamlit-functionality-echo",
    "title": "Communicating code: Presentations",
    "section": "Streamlit Functionality: echo",
    "text": "Streamlit Functionality: echo\n\nSometimes you want your Streamlit app to contain both your usual Streamlit graphic elements and the code that generated those elements. That’s where st.echo() comes in\n\nEasier to display this by an example:\n\nIn the example above the right of the image is given below (st.columns is used, where the input for the function is found from the left column).\n\nst.echo is used with the with statement.\neverything within the with is printed to the screen and executed\n\nwith st.echo():\n    # Everything inside this block will be both printed to the screen\n    # and executed.\n\n    def do_pd_replace(text, text_search, text_sub):\n        col_name = \"Start string\"\n        df = pd.DataFrame(data=[text], columns=[col_name])\n\n        df[\"Final String\"] = df[col_name].replace(\n            text_search, text_sub, regex=True\n        )\n\n        st.dataframe(df)\n        st.write(f\"text_search = '{text_search}' and text_sub = '{text_sub}'\")\n        st.write(f\"Input string = '{text}'\")\n        st.write(f\"Output string = '{df['Final String'].values[0]}'\")\n\n    do_pd_replace(text, text_search, text_sub)"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_pres.html#streamlit-functionality-pages",
    "href": "posts/Communicating_Code/CommCodePres_pres.html#streamlit-functionality-pages",
    "title": "Communicating code: Presentations",
    "section": "Streamlit Functionality: pages",
    "text": "Streamlit Functionality: pages\nBy simply creating a folder called pages and putting other streamlit .py files in the folder they can then be accessed in the sidebar.\n\nA main file needs to be outside the pages folder\nThe .py files in pages behave as if they were outside the folder (i.e. when loading files/functions)"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_pres.html#example-streamlit-presentation",
    "href": "posts/Communicating_Code/CommCodePres_pres.html#example-streamlit-presentation",
    "title": "Communicating code: Presentations",
    "section": "Example Streamlit Presentation",
    "text": "Example Streamlit Presentation\nVideo"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_tables.html#excel-files",
    "href": "posts/Communicating_Code/CommCodePres_tables.html#excel-files",
    "title": "Communicating code: Tables",
    "section": "Excel files",
    "text": "Excel files\n\nChatGPT: What are the problems of Excel and xlsx files?\n\n\nLimited scalability: Excel has a limit on the number of rows and columns it can handle effectively.\nInaccurate data: The manual nature of data entry in Excel can lead to errors and inaccuracies.\nFragility: Complex Excel spreadsheets can break easily if any formula or cell reference is altered.\nLack of security: Excel files are vulnerable to unauthorized access and malicious attacks.\nSlow performance: Large and complex Excel files can become slow to open and use, leading to decreased productivity.\nCompatibility issues: XLSX files may not be compatible with older versions of Excel, or with other software applications.\nLimited collaboration: Sharing Excel files and making changes to them can be difficult, especially when multiple people are involved.\n\nFor me it is the Slow performance alongside: (a) we are doing the data manipulation outside Excel anyway and (b) having to have another application open\n\nods with Excel\nAbout 10 s to open 3 ods files with Excel\n\n\n.ods converted to html in firefox browser\nAbout 6 s to open 3 converted ods files in a browser\n\n\n.ods converted to html firefox browser no new tabs\nAlmost instant when open converted ods files in same browser"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_tables.html#what-aspect-of-tables-i-am-considering",
    "href": "posts/Communicating_Code/CommCodePres_tables.html#what-aspect-of-tables-i-am-considering",
    "title": "Communicating code: Tables",
    "section": "What aspect of tables I am considering",
    "text": "What aspect of tables I am considering"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_tables.html#convert-xlsx-to-html",
    "href": "posts/Communicating_Code/CommCodePres_tables.html#convert-xlsx-to-html",
    "title": "Communicating code: Tables",
    "section": "Convert xlsx to html?",
    "text": "Convert xlsx to html?\n\nOpening xlsx files in Excel is slow\nConverting to html if we don’t want to edit could be an option\nIf we are moving to Python/R aren’t non-Excel options worth considering??\n\nConverting xlsx files to html\n\nSeems the most obvious course\nBut it doesn’t seem that easy with code\n\nbut easy within Excel"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_tables.html#what-does-chatgpt-say-to-convert-excel-file",
    "href": "posts/Communicating_Code/CommCodePres_tables.html#what-does-chatgpt-say-to-convert-excel-file",
    "title": "Communicating code: Tables",
    "section": "What does chatgpt say to convert excel file?",
    "text": "What does chatgpt say to convert excel file?\n\nConvert an excel file to html with python"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_tables.html#what-does-chatgpt-say-without-pandas",
    "href": "posts/Communicating_Code/CommCodePres_tables.html#what-does-chatgpt-say-without-pandas",
    "title": "Communicating code: Tables",
    "section": "What does chatgpt say without pandas?",
    "text": "What does chatgpt say without pandas?\n\nconvert excel file to html in python without pandas include the excel formatting such as column width\n\n\n\nimport pandas as pd\n\nimport os\nfrom pathlib import Path\nimport sys\n\nmodule_path = Path( os.getcwd() )\nmodule_path = module_path.parent.parent.parent.__str__() + '\\\\Pesticide'\n\ncwd = module_path\n\nfolder_path = os.path.join(cwd,'data')\n\nsys.path.insert(0, module_path)\n\ndf2 = pd.read_csv(os.path.join(folder_path,'combined_df.csv') ,index_col=0 )\n# change data type of columns\ndf2['date_of_sampling'] = pd.to_datetime(df2['date_of_sampling'])"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_tables.html#pandas",
    "href": "posts/Communicating_Code/CommCodePres_tables.html#pandas",
    "title": "Communicating code: Tables",
    "section": "pandas",
    "text": "pandas\n\nSince (in Python) we are mainly working with pandas. Let’s consider how pandas outputs can be modified.\npandas options\n\nSome code functionality\n# precision of all columns\npd.set_option(\"display.precision\", 2)\n# Or map as a string\ndf2['amount_pc_str'] = df2['amount_pc'].map(lambda x: '%.3f' % x)\n# some other options\npd.set_option('max_colwidth', 20)\npd.set_option('display.max_columns', None)\npd.set_option('display.expand_frame_repr', False)\npd.set_option('max_colwidth', 0)"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_tables.html#pandas-basic",
    "href": "posts/Communicating_Code/CommCodePres_tables.html#pandas-basic",
    "title": "Communicating code: Tables",
    "section": "pandas basic",
    "text": "pandas basic\n\ndf2\n\n\n\n\n\n  \n    \n      \n      sample_id\n      date_of_sampling\n      description\n      country_of_origin\n      retail_outlet\n      address\n      brand_name\n      packer_/_manufacturer_/_importer\n      product\n      address_postcode\n      packer_postcode\n      address_area\n      packer_area\n      chem_name\n      amount_detected\n      mrl\n      amount_pc\n    \n  \n  \n    \n      0\n      1958/2016\n      2016-08-08\n      Bramley Apples\n      UK\n      Asda\n      Creechbarrow Road, Taunton TA1 2AN\n      Asda\n      Asda Stores Ltd Leeds, UK LS11 5AD\n      Apple\n      TA1 2AN\n      LS11 5AD\n      Somerset\n      West Yorkshire\n      boscalid\n      0.03\n      2.0\n      0.015\n    \n    \n      1\n      1958/2016\n      2016-08-08\n      Bramley Apples\n      UK\n      Asda\n      Creechbarrow Road, Taunton TA1 2AN\n      Asda\n      Asda Stores Ltd Leeds, UK LS11 5AD\n      Apple\n      TA1 2AN\n      LS11 5AD\n      Somerset\n      West Yorkshire\n      pyraclostrobin\n      0.01\n      0.5\n      0.020\n    \n    \n      2\n      0230/2016\n      2016-08-08\n      Bramley Apples\n      UK\n      Co-op\n      Northgate, Louth LN11 0LT\n      Co-op\n      Co-operative Group Ltd Manchester M60 0AG\n      Apple\n      LN11 0LT\n      M60 0AG\n      Lincolnshire\n      Greater Manchester\n      boscalid\n      0.05\n      2.0\n      0.025\n    \n    \n      3\n      0230/2016\n      2016-08-08\n      Bramley Apples\n      UK\n      Co-op\n      Northgate, Louth LN11 0LT\n      Co-op\n      Co-operative Group Ltd Manchester M60 0AG\n      Apple\n      LN11 0LT\n      M60 0AG\n      Lincolnshire\n      Greater Manchester\n      flonicamid (sum)\n      0.02\n      0.2\n      0.100\n    \n    \n      4\n      0230/2016\n      2016-08-08\n      Bramley Apples\n      UK\n      Co-op\n      Northgate, Louth LN11 0LT\n      Co-op\n      Co-operative Group Ltd Manchester M60 0AG\n      Apple\n      LN11 0LT\n      M60 0AG\n      Lincolnshire\n      Greater Manchester\n      pyraclostrobin\n      0.03\n      0.5\n      0.060\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      35155\n      2858/2020 Organic\n      2020-10-20\n      Organic Sweet Potatoes\n      Spain\n      Tesco\n      300 Beverley Way, New Malden KT3 4PJ\n      Tesco\n      Tesco Stores Ltd Welwyn Garden City AL7 1GA\n      Sweet_Potatoes_Q4_(BNA)\n      KT3 4PJ\n      AL7 1GA\n      Greater London\n      Hertfordshire\n      0\n      0.00\n      0.0\n      0.000\n    \n    \n      35156\n      0562/2020 Organic\n      2020-10-05\n      Organic Duchy Sweet Potatoes\n      Egypt\n      Waitrose\n      Mill Lane, Swindon SN1 7BX\n      Waitrose\n      Waitrose Ltd Doncastle Road, Bracknell, Berksh...\n      Sweet_Potatoes_Q4_(BNA)\n      SN1 7BX\n      RG12 8YA\n      Wiltshire\n      Berkshire\n      0\n      0.00\n      0.0\n      0.000\n    \n    \n      35157\n      0563/2020\n      2020-10-05\n      Sweet Potatoes\n      USA\n      Waitrose\n      Mill Lane, Swindon SN1 7BX\n      Waitrose\n      Waitrose Ltd Doncastle Road, Bracknell, Berksh...\n      Sweet_Potatoes_Q4_(BNA)\n      SN1 7BX\n      RG12 8YA\n      Wiltshire\n      Berkshire\n      0\n      0.00\n      0.0\n      0.000\n    \n    \n      35158\n      2601/2020\n      2020-10-14\n      Sweet Potatoes\n      USA\n      Waitrose\n      Ossington Way, Newark NG24 1FF\n      Waitrose\n      Waitrose Ltd Doncastle Road, Bracknell, Berksh...\n      Sweet_Potatoes_Q4_(BNA)\n      NG24 1FF\n      RG12 8YA\n      Nottinghamshire\n      Berkshire\n      0\n      0.00\n      0.0\n      0.000\n    \n    \n      35159\n      2601/2020\n      2020-10-14\n      Sweet Potatoes\n      USA\n      Waitrose\n      Ossington Way, Newark NG24 1FF\n      Waitrose\n      Waitrose Ltd Doncastle Road, Bracknell, Berksh...\n      Sweet_Potatoes_Q4_(BNA)\n      NG24 1FF\n      RG12 8YA\n      Nottinghamshire\n      Berkshire\n      0\n      0.00\n      0.0\n      0.000\n    \n  \n\n35160 rows × 17 columns"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_tables.html#pandas-overview",
    "href": "posts/Communicating_Code/CommCodePres_tables.html#pandas-overview",
    "title": "Communicating code: Tables",
    "section": "pandas overview",
    "text": "pandas overview\n\nUsing pandas we can control various outputs\nBut these still need a format to display within\nAnd display functionality is not easy\n\nOr convert to a html file\ndf2.iloc[:500].to_html('df2_500.html')\nBut using a style sheet as shown in stack overflow by Parfait\n\ndf_out = df2.iloc[:500].copy()\n\npd.set_option('colheader_justify', 'center')   # FOR TABLE <th>\n\nhtml_string = '''\n<html>\n  <head><title>HTML Pandas Dataframe with CSS</title></head>\n  <link rel=\"stylesheet\" type=\"text/css\" href=\"df_style.css\"/>\n  <body>\n    {table}\n  </body>\n</html>.\n'''\n\n# OUTPUT AN HTML FILE\nwith open('df2_500.html', 'w') as f:\n    f.write(html_string.format(table=df_out.to_html(classes='mystyle')))"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_tables.html#section",
    "href": "posts/Communicating_Code/CommCodePres_tables.html#section",
    "title": "Communicating code: Tables",
    "section": "",
    "text": "https://www.python-excel.org/"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_tables.html#ipydatagrid",
    "href": "posts/Communicating_Code/CommCodePres_tables.html#ipydatagrid",
    "title": "Communicating code: Tables",
    "section": "ipydatagrid",
    "text": "ipydatagrid\nhttps://github.com/bloomberg/ipydatagrid\n\n\nfrom ipydatagrid import DataGrid, TextRenderer, VegaExpr\nimport ipydatagrid\ndatagrid = DataGrid(df2, selection_mode=\"cell\", editable=True,\n                   base_row_size=32, base_column_size=150)\n\ndatagrid = DataGrid(df2, base_row_size=30, base_column_size=150)\ndatagrid"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_tables.html#itables-code",
    "href": "posts/Communicating_Code/CommCodePres_tables.html#itables-code",
    "title": "Communicating code: Tables",
    "section": "itables code",
    "text": "itables code\nfrom itables import init_notebook_mode\n\nimport itables\ninit_notebook_mode(all_interactive=True)\n\nitables.show(df2)\n\nfrom itables import init_notebook_mode\n\nimport itables\ninit_notebook_mode(all_interactive=True)\n\nitables.show(df2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n      \n      sample_id\n      date_of_sampling\n      description\n      country_of_origin\n      retail_outlet\n      address\n      brand_name\n      packer_/_manufacturer_/_importer\n      product\n      address_postcode\n      packer_postcode\n      address_area\n      packer_area\n      chem_name\n      amount_detected\n      mrl\n      amount_pc\n    \n  Loading... (need help?)"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_tables.html#dash",
    "href": "posts/Communicating_Code/CommCodePres_tables.html#dash",
    "title": "Communicating code: Tables",
    "section": "Dash",
    "text": "Dash\nhttps://dash.plotly.com/datatable\n\nDownloaded 800,000 times per month, Dash is the original low-code framework for rapidly building data apps in Python, R, Julia, and F# (experimental).\n\nhttps://medium.com/plotly/introducing-jupyterdash-811f1f57c02e\n\nimport plotly.express as px\nfrom jupyter_dash import JupyterDash\nimport dash_core_components as dcc\nimport dash_html_components as html\nfrom dash.dependencies import Input, Output# Load Data\ndf = px.data.tips()# Build App\napp = JupyterDash(__name__)\napp.layout = html.Div([\n    html.H1(\"JupyterDash Demo\"),\n    dcc.Graph(id='graph'),\n    html.Label([\n        \"colorscale\",\n        dcc.Dropdown(\n            id='colorscale-dropdown', clearable=False,\n            value='plasma', options=[\n                {'label': c, 'value': c}\n                for c in px.colors.named_colorscales()\n            ])\n    ]),\n])# Define callback to update graph\n@app.callback(\n    Output('graph', 'figure'),\n    [Input(\"colorscale-dropdown\", \"value\")]\n)\ndef update_figure(colorscale):\n    return px.scatter(\n        df, x=\"total_bill\", y=\"tip\", color=\"size\",\n        color_continuous_scale=colorscale,\n        render_mode=\"webgl\", title=\"Tips\"\n    )# Run app and display result inline in the notebook\napp.run_server(mode='inline')\n\nC:\\Users\\44781\\AppData\\Local\\Temp\\ipykernel_15260\\3294666565.py:3: UserWarning: \nThe dash_core_components package is deprecated. Please replace\n`import dash_core_components as dcc` with `from dash import dcc`\n  import dash_core_components as dcc\nC:\\Users\\44781\\AppData\\Local\\Temp\\ipykernel_15260\\3294666565.py:4: UserWarning: \nThe dash_html_components package is deprecated. Please replace\n`import dash_html_components as html` with `from dash import html`\n  import dash_html_components as html"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_tables.html#streamlit",
    "href": "posts/Communicating_Code/CommCodePres_tables.html#streamlit",
    "title": "Communicating code: Tables",
    "section": "Streamlit",
    "text": "Streamlit\n\nA faster way to build and share data apps\n\n\nDash can be run within a notebook but is principally an app.\nStreamlit is a similar app.\nBut much easier to code.\n\nimport pandas as pd\nimport streamlit as st\nall_dfs = pd.read_csv(\"./data/combined_df.csv\")\nst.dataframe(all_dfs.head())"
  },
  {
    "objectID": "posts/Communicating_Code/CommCodePres_tables.html#and-more",
    "href": "posts/Communicating_Code/CommCodePres_tables.html#and-more",
    "title": "Communicating code: Tables",
    "section": "And more",
    "text": "And more\n\nDataTables\n\nDataTables is a plug-in for the jQuery Javascript library. It is a highly flexible tool, built upon the foundations of progressive enhancement, that adds all of these advanced features to any HTML table.\n\n\n\nJupyter widgets\nIf you are looking for Jupyter widgets, have a look at (taken from https://mwouts.github.io/itables/references.html) - QGrid by Quantopian - IPyaggrid by Louis Raison and Olivier Borderies - IPySheet by QuantStack."
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#content",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#content",
    "title": "Presentation: Communicating code and data",
    "section": "Content",
    "text": "Content\n\nNotebooks\n\nWhat are they?\nExamples\nPros and cons\n\nApps\n\nVoila\nStreamlit\n\nWebsites and HTML\n\nConverting notebooks to HTML and websites\n\nPresentations\n\nUsing notebooks for presentations\n\nTabular Data\n\nComments on Excel\nThoughts on code alternatives"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#jupyter-notebooks",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#jupyter-notebooks",
    "title": "Presentation: Communicating code and data",
    "section": "Jupyter Notebooks",
    "text": "Jupyter Notebooks\nFrom TalkPython: Awesome Jupyter Libraries and Extensions\n\nJupyter is an amazing environment for exploring data and generating executable reports with Python. But there are many external tools, extensions, and libraries to make it so much better and make you more productive.\n\n\nA notebook consists of two parts\n\nmarkdown part where we can:\n\nwrite text, add images, links, html, LaTeX etc\n\ncode part which runs and displays output of code\n\n\nSome links:\n\nJupyter Book\nA curated list of awesome Jupyter projects\nCode Documentation QA of Code\nFastAI guide for better blogs"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#example-of-a-notebook",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#example-of-a-notebook",
    "title": "Presentation: Communicating code and data",
    "section": "Example of a notebook",
    "text": "Example of a notebook\nAn example notebook"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#markdown-in-a-notebook-1",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#markdown-in-a-notebook-1",
    "title": "Presentation: Communicating code and data",
    "section": "Markdown in a notebook 1",
    "text": "Markdown in a notebook 1\nSome useful commands:\n\n# Notebooks General and ## Markdown in a notebook 1\n![](ghtop_images/pest.png) looks like this"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#markdown-in-a-notebook-2",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#markdown-in-a-notebook-2",
    "title": "Presentation: Communicating code and data",
    "section": "Markdown in a notebook 2",
    "text": "Markdown in a notebook 2\n\nAnd the same with a mp4 file ![](ghtop_images/revealjs.mp4)\nOr a youtube video \"\""
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#markdown-in-a-notebook-3",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#markdown-in-a-notebook-3",
    "title": "Presentation: Communicating code and data",
    "section": "Markdown in a notebook 3",
    "text": "Markdown in a notebook 3\n\n> If we want text like this\n\n\nIf we want text like this\n\n\nOr if we want code use `a = b + c`\n\nor:\n```\na = b\na = a + c\n```\n\na = b\n\na = a + c"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#markdown-in-a-notebook-4",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#markdown-in-a-notebook-4",
    "title": "Presentation: Communicating code and data",
    "section": "Markdown in a notebook 4",
    "text": "Markdown in a notebook 4\n\nHTML works too\n\n<img src=\"ghtop_images/pest.png\"></img>"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#code-in-a-notebook",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#code-in-a-notebook",
    "title": "Presentation: Communicating code and data",
    "section": "Code in a notebook",
    "text": "Code in a notebook\nExample interactive format using altair:"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#simpler-code-output",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#simpler-code-output",
    "title": "Presentation: Communicating code and data",
    "section": "Simpler code output",
    "text": "Simpler code output\nx = np.arange(0,np.pi,.01)\ny = np.sin(x)\nplt.plot(x,y)"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#other-code-stuff",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#other-code-stuff",
    "title": "Presentation: Communicating code and data",
    "section": "Other code stuff",
    "text": "Other code stuff\n\nCan use shell commands e.g. !pip install pandas\nCan use magic commands e.g. %%time to time a cell\n\n%%time\ny=0\nfor x in range(0,100):\n    y+=x\nprint(f\"y is {y}\")\n\n\ny is 4950\nCPU times: total: 0 ns\nWall time: 0 ns"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#but-not-everyone-loves-notebooks",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#but-not-everyone-loves-notebooks",
    "title": "Presentation: Communicating code and data",
    "section": "But Not everyone loves notebooks :(",
    "text": "But Not everyone loves notebooks :(\nNotebooks have their validish detractors I don’t like notebooks.- Joel Grus Youtube"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#notebooks-opinion",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#notebooks-opinion",
    "title": "Presentation: Communicating code and data",
    "section": "Notebooks Opinion",
    "text": "Notebooks Opinion\nAlthough notebooks have their validish detractors I don’t like notebooks.- Joel Grus Youtube I think if you approach them in the right way they are a super powerful tool.\nThe negatives seem to be:\n\nencourage bad practice in code (a genuine problem)\nissues around order of what cell is run (easily got around with good practice)\nissues around lack of auto complete (I don’t see the issue, use in visual studio autocomplete is there)\nno grammar/spelling correction\nissues with using git and version control\n\nthere are ways around this though\n\nanything else?"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#notebook-benefits",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#notebook-benefits",
    "title": "Presentation: Communicating code and data",
    "section": "Notebook Benefits",
    "text": "Notebook Benefits\n\nNotebooks are intuitive\n\nYou have the code then the result of the code\nCan add text or images\nAnd it’s linear\n\nCan get things up and working quickly\nAid with communicating code\nEncourages Writing\n\nand writing things down aids thinking in the now and understanding what you did and why in the future\n\nFastAI guide for better blogs\n\nEasy to convert code to a pipeline\nTreat it as a notebook\n\nin the same way an artist would have a sketchbook to help make final piece\n\nWith many companies moving towards Python/R from Excel and a varied level of skills.\n\n“Aid with communicating code” is particularly important"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#example-documenting-code",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#example-documenting-code",
    "title": "Presentation: Communicating code and data",
    "section": "Example: Documenting Code",
    "text": "Example: Documenting Code\n\nHere is my website for my research project on pesticides in UK food\nThis is not the same as documentation for a package but there are parallels\n\nThis does a few things:\n\nDocuments the analysis steps I have taken including the code and outputs\n\nUseful for data transparency, useability of the code if needs modifiying/adapting, and why I did XYZ\n\nProvides a way to present the data\n\nThere is a streamlit app, but sometimes I like to be able to see the code"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#example-tool-to-aid-learning",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#example-tool-to-aid-learning",
    "title": "Presentation: Communicating code and data",
    "section": "Example: Tool to aid learning",
    "text": "Example: Tool to aid learning\nA big area I have been using Jupyter Notebooks for is to aid learning\n\nIf you want to understand something it helps to write it down\nHaving the code next to it is a big advantage\nAnd if stored on github you can access it anywhere\n\nTensoflow cheat sheet"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#example-debugging-code",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#example-debugging-code",
    "title": "Presentation: Communicating code and data",
    "section": "Example: Debugging Code",
    "text": "Example: Debugging Code\n\nSince starting at ONS I have been working with understanding an existing project and latterly adding code to it\nThe project consists of multiple python files across several folders\n\nMy Python was good but lots of the functions and their useage weren’t immediately obvious to me\n\nbreak-points in VS Studio is really good to step through the code and work out what happens in the code.\n\nI had not used before with Python (but had lots with MATLAB), and it’s really useful\n\nBut it can be limited what you can do\n\ndifficult to probe code if want to write more than 1 line of code\nthe experience/knowledge exists as you go through it but no documentation to refer to later, e.g. function X does this when I give it Y etc"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#example-debugging-code-2",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#example-debugging-code-2",
    "title": "Presentation: Communicating code and data",
    "section": "Example: Debugging Code 2",
    "text": "Example: Debugging Code 2\n\nBy copying and pasting code into Jupyter cells I could see and document how they worked (e.g. changing inputs)\n\nThis (copying and pasting) would get around code changes too (which would be an issue if modules were just imported)\nbecause this was all done in Jupyter notebook I can have a ipynb code file and a html file showing how the code works\nI could even save a pickle file of the variables at a particularly point to understand how the code would work from this point"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#apps-overview",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#apps-overview",
    "title": "Presentation: Communicating code and data",
    "section": "Apps Overview",
    "text": "Apps Overview\nThere are many packages that can be used to convert python code to an app\nUsing Notebooks directly\n\nVoila\nmercury\n\nApps without notebooks\n\nPySimpleGUI\n\nSimple and useful but not the best for displaying data\n\nDash\n\nLooks really good, but also super complicated\n\nStreamlit\n\nEasy and looks good"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#voila",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#voila",
    "title": "Presentation: Communicating code and data",
    "section": "Voila",
    "text": "Voila\n\nVoila is relatively simple to use\nrun with something like voila .\\Excel_Voila.ipynb\nconverts notebook to an app\ncan use things like ipywidgets for interactivity\nthe reason I’m interested in it’s use is streamlit doesn’t seem to give flexibility to modify table output I’d like"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#streamlit-overview",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#streamlit-overview",
    "title": "Presentation: Communicating code and data",
    "section": "Streamlit Overview",
    "text": "Streamlit Overview\n\nStreamlit is an open-source Python library that makes it easy to create and share beautiful, custom web apps for machine learning and data science. In just a few minutes you can build and deploy powerful data apps. So let’s get started!\n\nPrincipally used to create apps, but some of the functionality works well for code/data presentations"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#streamlit-functionality-overview",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#streamlit-functionality-overview",
    "title": "Presentation: Communicating code and data",
    "section": "Streamlit Functionality: overview",
    "text": "Streamlit Functionality: overview\nStreamlit allows various functionality:\n\ntextbox\nimages/videos\ncharts/tables\nmenus/buttons\netc"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#streamlit-functionality-streamlit_layout",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#streamlit-functionality-streamlit_layout",
    "title": "Presentation: Communicating code and data",
    "section": "Streamlit Functionality: streamlit_layout",
    "text": "Streamlit Functionality: streamlit_layout\nBut unlike some apps (am thinking MATLAB GUIs) you can’t create the look and functionality separately. So if you want something in a certain position it can be tricky. HTML can be used with st.markdown to give more control but it isn’t recommended to use by streamlit.\nInstead, to create the layout as you would like they have the following features:"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#streamlit-functionality-columns-and-sidebar",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#streamlit-functionality-columns-and-sidebar",
    "title": "Presentation: Communicating code and data",
    "section": "Streamlit Functionality: columns and sidebar",
    "text": "Streamlit Functionality: columns and sidebar\nThe most useable are the first two: columns and sidebar\nColumns allows us to split the app vertically. The code is fairly simple:\nEither colL, colM, colR = st.columns(3) for 3 equal columns or to split columns with different sizes:\ncolL, _, colR = st.columns((10, 5, 20))\nwith colL:\n    st.write('On the left')\nwith colR:\n    st.write('On the right twice as big as left')\nst.sidebar just adds a sidebar to the app that can be hidden or shown.\nAnything in the sidebar is just prefixed by st.sidebar so:\nst.sidebar.write('I am in the sidebar')\nst.write('I am in the main app')\nst.sidebar.write('I am back in the sidebar')"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#streamlit-functionality-html",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#streamlit-functionality-html",
    "title": "Presentation: Communicating code and data",
    "section": "Streamlit Functionality: html",
    "text": "Streamlit Functionality: html\nIt is possible to add various additional personalisations using html. - BUT it does come with security risks and so is [not recommended]](https://github.com/streamlit/streamlit/issues/152)\n\nAllows much more control over the layout of the app that can be useful for a presentation: - Can add a background image - Can add background color to a textbox - Control over positioning of widgets - lots more\nHTML is implementated using st.markdown with unsafe_allow_html=True inside the former"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#streamlit-functionality-html-examples",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#streamlit-functionality-html-examples",
    "title": "Presentation: Communicating code and data",
    "section": "Streamlit Functionality: html examples",
    "text": "Streamlit Functionality: html examples\nadd background to a text box\ntext = \"Code Examples\"\n        st.markdown(f'<center><p style=font-family:\"Calibri\";background-color:#FFFFFF;color:#000000;font-size:42px;border-radius:10%><b>{text}</b></p></center>', unsafe_allow_html=True)\n\nOr to add a background image\nimport streamlit as st\nimport base64\n\n@st.cache(allow_output_mutation=True)\ndef get_base64_of_bin_file(bin_file):\n    with open(bin_file, 'rb') as f:\n        data = f.read()\n    return base64.b64encode(data).decode()\n\ndef set_png_as_page_bg(png_file):\n    bin_str = get_base64_of_bin_file(png_file) \n    page_bg_img = '''\n    <style>\n    .stApp {\n    background-image: url(\"data:image/png;base64,%s\");\n    background-size: contain;\n    background-repeat: no-repeat;\n    background-attachment: scroll; # doesn't work\n    }\n    </style>\n    ''' % bin_str\n    st.markdown(page_bg_img, unsafe_allow_html=True)\n    return"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#streamlit-functionality-echo",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#streamlit-functionality-echo",
    "title": "Presentation: Communicating code and data",
    "section": "Streamlit Functionality: echo",
    "text": "Streamlit Functionality: echo\n\nSometimes you want your Streamlit app to contain both your usual Streamlit graphic elements and the code that generated those elements. That’s where st.echo() comes in\n\nEasier to display this by an example:\n\nIn the example above the right of the image is given below (st.columns is used, where the input for the function is found from the left column).\n\nst.echo is used with the with statement.\neverything within the with is printed to the screen and executed\n\nwith st.echo():\n    # Everything inside this block will be both printed to the screen\n    # and executed.\n\n    def do_pd_replace(text, text_search, text_sub):\n        col_name = \"Start string\"\n        df = pd.DataFrame(data=[text], columns=[col_name])\n\n        df[\"Final String\"] = df[col_name].replace(\n            text_search, text_sub, regex=True\n        )\n\n        st.dataframe(df)\n        st.write(f\"text_search = '{text_search}' and text_sub = '{text_sub}'\")\n        st.write(f\"Input string = '{text}'\")\n        st.write(f\"Output string = '{df['Final String'].values[0]}'\")\n\n    do_pd_replace(text, text_search, text_sub)"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#streamlit-functionality-pages",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#streamlit-functionality-pages",
    "title": "Presentation: Communicating code and data",
    "section": "Streamlit Functionality: pages",
    "text": "Streamlit Functionality: pages\nBy simply creating a folder called pages and putting other streamlit .py files in the folder they can then be accessed in the sidebar.\n\nA main file needs to be outside the pages folder\nThe .py files in pages behave as if they were outside the folder (i.e. when loading files/functions)"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#example-streamlit-presentation",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#example-streamlit-presentation",
    "title": "Presentation: Communicating code and data",
    "section": "Example Streamlit Presentation",
    "text": "Example Streamlit Presentation\nVideo"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#what-i-have-used-to-convert-notebooks-to-html",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#what-i-have-used-to-convert-notebooks-to-html",
    "title": "Presentation: Communicating code and data",
    "section": "What I have used to convert notebooks to html",
    "text": "What I have used to convert notebooks to html\n\nfastpages\n\nI have used fastpages, but this is now deprecated and they are recommending the use of quarto\n\nquarto\n\nSo far I have found quarto really good and flexible (N.B. R works too)\nEasy to convert a notebook to multiple formats, including html, powerpoint, pdf, word doc\nBUT Quarto is not possible if installing from non pip sources is an issue (as far as I can tell currently)\n\nnbconvert is another option I tried\n\nbut it doesn’t seem to have the functionality of fastpages or quarto\n\nJupyter Books seems to be the best option within companies with installation issues\n\nMaybe not as good as quarto but it works!"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#others",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#others",
    "title": "Presentation: Communicating code and data",
    "section": "Others",
    "text": "Others\n\nI know some people use Sphinx,\n\nis recommended by QA\nFrom what I can tell sphinx on it’s own is not as easy to use as notebooks\nBut there is a jupyter extension nbsphinx\nJupyter Books uses Sphinx heavily under the hood\n\nnbdev\n\nI think is connected to quarto\n\nVoila\n\nVoilà turns Jupyter notebooks into standalone web applications.\nLooks good, bit like streamlit\nbut seems to interfere with other libraries\nmercury seems similar"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#creating-html-other-formats",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#creating-html-other-formats",
    "title": "Presentation: Communicating code and data",
    "section": "Creating html (& other formats)",
    "text": "Creating html (& other formats)\nQuarto\nInstallation is via a package i.e. .msi for Windows or .pkg for Mac. Which can cause issues.\nWorks with both ipynb and qmd files, which are both a mixture of markdown and executable code.\nThe only thing that needs to be done with the notebook is add a YAML block at the start of the notebook, like the following (raq not markdown was used):\n---\ntitle: \"Communicating code: Website\"\nsubtitle: \"Using the notebook format for a website\"\nauthor: \"Thomas H. Simm\"\nformat:\n  html:\n    toc: true\ntitle-slide-attributes:\n  data-background-size: contain\n  data-background-opacity: \"0.5\"\njupyter: python3\n---\nWe can create different files from this .ipynb Jupyter notebook using the following code:\n\nquarto render testPres.ipynb --to pptx\nquarto render testPres.ipynb --to pdf\nquarto render testPres.ipynb --to html\nquarto render testPres.ipynb --to revealjs"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#quarto-1",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#quarto-1",
    "title": "Presentation: Communicating code and data",
    "section": "Quarto",
    "text": "Quarto\nFurther, formatting for projects (i.e. for website) can be done within the configuration file _quarto.yml\nproject:\n  type: website\n  output-dir: _site\n\nwebsite:\n  title: \"ThomasHSimm\"\n  favicon: /posts/Picture3.png\n  body-header: <img src=\"/posts/header2.png\" height=200>\n\n  navbar:\n    right:\n      - about.qmd\n      - icon: github\n        href: https://github.com/ThomasHSimm\n      - icon: mortarboard-fill\n        href: https://scholar.google.com/citations?hl=en&user=HdPDn1sAAAAJ\nformat:\n  html:\n    theme: \n      light: flatly\n      dark: darkly\n    css: styles.css"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#jupyter-books",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#jupyter-books",
    "title": "Presentation: Communicating code and data",
    "section": "Jupyter Books",
    "text": "Jupyter Books\nWe can create different files from this .ipynb Jupyter notebook using the following code:\n\njupyter-book build .\\PesticideDocs\\\njupyter-book build <path-to-book>\njupyter-book build <path-to-book> --builder pdfhtml\njupyter-book build <path-to-book> --builder singlehtml\n\nThe only difference in notebook is that it needs to have One header in a markdown cell for the table of contents, e.g. \n# Title of page"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#configuration-file",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#configuration-file",
    "title": "Presentation: Communicating code and data",
    "section": "Configuration file",
    "text": "Configuration file\nA seperate files _config.yml is used to define how the html (or other) files will look\n# Book settings\n# Learn more at https://jupyterbook.org/customize/config.html\n\ntitle: Defra Pesticide Testing, Data Analysis\nauthor: Thomas Simm\nlogo: ONS-logo.png\nexclude_patterns: [_build, Thumbs.db, .DS_Store, \"**.ipynb_checkpoints\"]\n\n\n# Force re-execution of notebooks on each build.\n# See https://jupyterbook.org/content/execute.html\nexecute:\n  execute_notebooks: force\n\n# Define the name of the latex output file for PDF builds\nlatex:\n  latex_documents:\n    targetname: book.tex\n\n# Add a bibtex file so that we can create citations\nbibtex_bibfiles:\n  - references.bib\n\n# Information about where the book exists on the web\nrepository:\n  url: https://github.com/ThomasHSimm/Pesticide  # Online location of your book\n  path_to_book: docs  # Optional path to your book, relative to the repository root\n  branch: master  # Which branch of the repository should be used when creating links (optional)\n\n# Add GitHub buttons to your book\n# See https://jupyterbook.org/customize/config.html#add-a-link-to-your-repository\n# HTML-specific settings\nhtml:\n  favicon                   : \"_images/favicon.jpg\"  # A path to a favicon image\n  use_edit_page_button      : false  # Whether to add an \"edit this page\" button to pages. If `true`, repository information in repository: must be filled in\n  use_repository_button     : false  # Whether to add a link to your repository button\n  use_issues_button         : false  # Whether to add an \"open an issue\" button\n  use_multitoc_numbering    : true   # Continuous numbering across parts/chapters\n  extra_navbar              : Powered by <a href=\"https://jupyterbook.org\">Jupyter Book</a>\n                              <br>Home website <a href=\"https://thomashsimm.com/\">thomashsimm.com</a> # Will be displayed underneath the left navbar.\n  extra_footer              : \"\"  # Will be displayed underneath the footer.\n  google_analytics_id       : \"\"  # A GA id that can be used to track book views.\n  home_page_in_navbar       : true  # Whether to include your home page in the left Navigation Bar\n  baseurl                   : \"\"  # The base URL where your book will be hosted. Used for creating image previews and social links. e.g.: https://mypage.com/mybook/\n  comments:\n    hypothesis              : false\n    utterances              : false\n  announcement              : \"\" # A banner announcement at the top of the site."
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#table-of-content",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#table-of-content",
    "title": "Presentation: Communicating code and data",
    "section": "Table of content",
    "text": "Table of content\nAnd in addition to the config file a table of contents file is required _toc.yml:\n# Table of contents\n# Learn more at https://jupyterbook.org/customize/toc.html\n\nformat: jb-book\nroot: intro\nchapters:\n- file: Pesticide_Plots\n- file: References\n- file: UK_areas\n- file: using_jupyter_books"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#creating-a-webpage-from-this",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#creating-a-webpage-from-this",
    "title": "Presentation: Communicating code and data",
    "section": "Creating a webpage from this",
    "text": "Creating a webpage from this\nTakes about 30 mins including installing the chosen converter. (But can be done much quicker)\n\ncreate a Github repo for your website\nchoose the converter (e.g. Jupyter Books)\n\nAnd follow their instructions\n\ngo to settings -> Pages within the repo\n\nfew options to do\n\nOptional: add your own website url to it\n\nLink how to do this here\nIn Quarto a command from your PC in the repo, publishes the website:\nquarto publish quarto-pub\nOr equivalently with Jupyter Books:\nghp-import -n -p -f _build/html"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#creating-directly-from-the-repo",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#creating-directly-from-the-repo",
    "title": "Presentation: Communicating code and data",
    "section": "Creating directly from the repo",
    "text": "Creating directly from the repo\nIf we instead want to convert notebook files directly from a repo to create a website then this can be done with Netlify.\nThis is useful if using Gitlab (i.e. not Github) or don’t want all the extra html files cluttering the repo.\nSteps:\nhttps://jupyterbook.org/en/stable/publish/netlify.html\n\nSign up and connect Github/Gitlab\nAdd a requirements.txt file and also toc.yml to directory\nOn netlify -> Add new site -> import from an existing repo\nInsert something like below\n\nN.B. the command:\npip install -r requirements.txt && jupyter-book build .\nand folder location"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#on-netlify",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#on-netlify",
    "title": "Presentation: Communicating code and data",
    "section": "On netlify",
    "text": "On netlify\n\nExample:\n\nGitlab repo\nResulting website https://thomashsimm.netlify.app/intro.html"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#quarto-presentations",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#quarto-presentations",
    "title": "Presentation: Communicating code and data",
    "section": "Quarto Presentations",
    "text": "Quarto Presentations\nQuarto supports a variety of formats for creating presentations, including:\n\nrevealjs — reveal.js (HTML)\npptx — PowerPoint (MS Office)\nbeamer — Beamer (LaTeX/PDF)\n\nI’ll consider the first two"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#quarto-powerpoint-overview",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#quarto-powerpoint-overview",
    "title": "Presentation: Communicating code and data",
    "section": "Quarto PowerPoint overview",
    "text": "Quarto PowerPoint overview\nThe steps to make a PowerPoint presentation from a notebook:\n\nCreate the inbuilt template.pptx file\nAdjust it to match your own template\nAt the top of the notebook insert format for pptx including the template file\nChoose how you will define a new page\nYou will probably need to manually check the slides and adjust as required\n\nespecially for interactive content and code"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#creating-the-template",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#creating-the-template",
    "title": "Presentation: Communicating code and data",
    "section": "Creating the template",
    "text": "Creating the template\n(Office info correct for Office 365 Feb 2023, Version 2301 Build 16.0.16026.20002)\nIf your workplace has a custom template or you have one you always use, you can incorporate this into quarto.\nHowever, quarto is quite specific on the form this template takes, and requires the following elements\n\nTitle Slide\nTitle and Content\nSection Header\nTwo Content\nComparison\nContent with Caption\nBlank"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#creating-a-ppt-template",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#creating-a-ppt-template",
    "title": "Presentation: Communicating code and data",
    "section": "Creating a ppt template",
    "text": "Creating a ppt template\nBy selecting Layout from the Home tab in powerpoint the different layouts can be seen\n\nThey can then be modified by going to View tab - Slide Master."
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#creating-a-ppt-template-2",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#creating-a-ppt-template-2",
    "title": "Presentation: Communicating code and data",
    "section": "Creating a ppt template 2",
    "text": "Creating a ppt template 2\nIf using your own template you will need to match the names of the slides given above. These can be found by hovering over the slides on the left or right clicking on one and selecting “Rename Layout”"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#creating-a-ppt-template-3",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#creating-a-ppt-template-3",
    "title": "Presentation: Communicating code and data",
    "section": "Creating a ppt template 3",
    "text": "Creating a ppt template 3\nAlternatively, create a custom template using quarto and then modify this. The following command creates the template:\nquarto pandoc -o template.pptx --print-default-data-file reference.pptx\nThen go to View tab - Slide Master and modify each slide layout.\nNote if you are trying to match a template, some tips: - go to Design -> Slide Size and match this to your template - when View tab - Slide Master is selected go to first tab (see above it will be left indented) on one you are copying from and select all on this then paste to the new template - these will be background images and other things that want to be passed to all slides - Check other slides for images and font-styles etc to match to the new template"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#load-the-template",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#load-the-template",
    "title": "Presentation: Communicating code and data",
    "section": "Load the template",
    "text": "Load the template\nTo load the template the first cell in the notebook needs to be modified as follows to reference the template.pptx file.\nformat:\n  pptx:\n    reference-doc: template.pptx\n    slide-level: 2\nIn addition, we can also specify here the rule by which a new slide is defined. If slide-level: 2 is used a new slide is defined by “##’ and a new section header by ‘#’. So if we used ‘###’ this would be a heading within the slide.\nIf slide-level: 1 is used a new slide is defined by “#’ and ‘##’ this would be a heading within the slide (this is normally the default)."
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#check-the-slides",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#check-the-slides",
    "title": "Presentation: Communicating code and data",
    "section": "Check the slides",
    "text": "Check the slides\nI have found creation of slides to powerpoint more prone to strange results than if .doc/.pdf/.html are used.\nSo check the slides, see if interactive content or code has been included (probably not) and if the slide content goes outside the slide.\n\nIn the example above - There is overlap of text on a slide - Strange ouput of a slide - Code output not displayed"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#adding-style-to-revealjs",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#adding-style-to-revealjs",
    "title": "Presentation: Communicating code and data",
    "section": "Adding style to revealjs",
    "text": "Adding style to revealjs\nA simple way to add template like details to a revealjs file is to add a style.css sheet.\nIn the example below, the style sheet adds logo.png to the bottom right of each sheet\nThe file style.css looks like this:\n.reveal .slide-logo {\n  display: block;\n  position: fixed;\n  top: unset !important;\n  left: unset !important;\n  bottom: 50px;\n  right: 12px;\n  height: 100px !important;\n  width: 100x !important;\n  max-width: unset !important;\n  max-height: unset !important;\n}"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#adding-style-to-revealjs-1",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#adding-style-to-revealjs-1",
    "title": "Presentation: Communicating code and data",
    "section": "Adding style to revealjs",
    "text": "Adding style to revealjs\nAnd the revealjs part at the top of the jupyter notebook looks like this\nrevealjs:\n    slide-number: true\n    height: 1080\n    width: 1920\n    logo: logo.png\n    css: style.css\nSo this would then look like the following, with the logo (logo.png) in the bottom right, and size and positioning given by the css file"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#what-the-revealjs-file-looks-like",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#what-the-revealjs-file-looks-like",
    "title": "Presentation: Communicating code and data",
    "section": "What the revealjs file looks like",
    "text": "What the revealjs file looks like\nVideo"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#excel-files",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#excel-files",
    "title": "Presentation: Communicating code and data",
    "section": "Excel files",
    "text": "Excel files\n\nChatGPT: What are the problems of Excel and xlsx files?\n\n\nLimited scalability: Excel has a limit on the number of rows and columns it can handle effectively.\nInaccurate data: The manual nature of data entry in Excel can lead to errors and inaccuracies.\nFragility: Complex Excel spreadsheets can break easily if any formula or cell reference is altered.\nLack of security: Excel files are vulnerable to unauthorized access and malicious attacks.\nSlow performance: Large and complex Excel files can become slow to open and use, leading to decreased productivity.\nCompatibility issues: XLSX files may not be compatible with older versions of Excel, or with other software applications.\nLimited collaboration: Sharing Excel files and making changes to them can be difficult, especially when multiple people are involved.\n\nFor me it is the Slow performance alongside: (a) we are doing the data manipulation outside Excel anyway and (b) having to have another application open"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#what-aspect-of-tables-i-am-considering",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#what-aspect-of-tables-i-am-considering",
    "title": "Presentation: Communicating code and data",
    "section": "What aspect of tables I am considering",
    "text": "What aspect of tables I am considering"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#loading-data",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#loading-data",
    "title": "Presentation: Communicating code and data",
    "section": "Loading data",
    "text": "Loading data\n.ods with Excel\nAbout 10 s to open 3 ods files with Excel\n.ods converted to html in firefox browser\nAbout 6 s to open 3 converted ods files in a browser\n.ods converted to html firefox browser no new tabs\nAlmost instant when open converted ods files in same browser"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#convert-xlsx-to-html",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#convert-xlsx-to-html",
    "title": "Presentation: Communicating code and data",
    "section": "Convert xlsx to html?",
    "text": "Convert xlsx to html?\n\nOpening xlsx files in Excel is slow\nConverting to html if we don’t want to edit could be an option\nIf we are moving to Python/R aren’t non-Excel options worth considering??\n\nConverting xlsx files to html\n\nSeems the most obvious course\nBut it doesn’t seem that easy with code\n\nbut easy within Excel"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#pandas",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#pandas",
    "title": "Presentation: Communicating code and data",
    "section": "pandas",
    "text": "pandas\n\nSince (in Python) we are mainly working with pandas. Let’s consider how pandas outputs can be modified.\npandas options\n\nSome code functionality\n# precision of all columns\npd.set_option(\"display.precision\", 2)\n# Or map as a string\ndf2['amount_pc_str'] = df2['amount_pc'].map(lambda x: '%.3f' % x)\n# some other options\npd.set_option('max_colwidth', 20)\npd.set_option('display.max_columns', None)\npd.set_option('display.expand_frame_repr', False)\npd.set_option('max_colwidth', 0)"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#pandas-basic",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#pandas-basic",
    "title": "Presentation: Communicating code and data",
    "section": "pandas basic",
    "text": "pandas basic\n\n\n\n\n\n\n  \n    \n      \n      sample_id\n      date_of_sampling\n      description\n      country_of_origin\n      retail_outlet\n      address\n      brand_name\n      packer_/_manufacturer_/_importer\n      product\n      address_postcode\n      packer_postcode\n      address_area\n      packer_area\n      chem_name\n      amount_detected\n      mrl\n      amount_pc\n    \n  \n  \n    \n      0\n      1958/2016\n      2016-08-08\n      Bramley Apples\n      UK\n      Asda\n      Creechbarrow Road, Taunton TA1 2AN\n      Asda\n      Asda Stores Ltd Leeds, UK LS11 5AD\n      Apple\n      TA1 2AN\n      LS11 5AD\n      Somerset\n      West Yorkshire\n      boscalid\n      0.03\n      2.0\n      0.015\n    \n    \n      1\n      1958/2016\n      2016-08-08\n      Bramley Apples\n      UK\n      Asda\n      Creechbarrow Road, Taunton TA1 2AN\n      Asda\n      Asda Stores Ltd Leeds, UK LS11 5AD\n      Apple\n      TA1 2AN\n      LS11 5AD\n      Somerset\n      West Yorkshire\n      pyraclostrobin\n      0.01\n      0.5\n      0.020\n    \n    \n      2\n      0230/2016\n      2016-08-08\n      Bramley Apples\n      UK\n      Co-op\n      Northgate, Louth LN11 0LT\n      Co-op\n      Co-operative Group Ltd Manchester M60 0AG\n      Apple\n      LN11 0LT\n      M60 0AG\n      Lincolnshire\n      Greater Manchester\n      boscalid\n      0.05\n      2.0\n      0.025\n    \n    \n      3\n      0230/2016\n      2016-08-08\n      Bramley Apples\n      UK\n      Co-op\n      Northgate, Louth LN11 0LT\n      Co-op\n      Co-operative Group Ltd Manchester M60 0AG\n      Apple\n      LN11 0LT\n      M60 0AG\n      Lincolnshire\n      Greater Manchester\n      flonicamid (sum)\n      0.02\n      0.2\n      0.100\n    \n    \n      4\n      0230/2016\n      2016-08-08\n      Bramley Apples\n      UK\n      Co-op\n      Northgate, Louth LN11 0LT\n      Co-op\n      Co-operative Group Ltd Manchester M60 0AG\n      Apple\n      LN11 0LT\n      M60 0AG\n      Lincolnshire\n      Greater Manchester\n      pyraclostrobin\n      0.03\n      0.5\n      0.060\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      35155\n      2858/2020 Organic\n      2020-10-20\n      Organic Sweet Potatoes\n      Spain\n      Tesco\n      300 Beverley Way, New Malden KT3 4PJ\n      Tesco\n      Tesco Stores Ltd Welwyn Garden City AL7 1GA\n      Sweet_Potatoes_Q4_(BNA)\n      KT3 4PJ\n      AL7 1GA\n      Greater London\n      Hertfordshire\n      0\n      0.00\n      0.0\n      0.000\n    \n    \n      35156\n      0562/2020 Organic\n      2020-10-05\n      Organic Duchy Sweet Potatoes\n      Egypt\n      Waitrose\n      Mill Lane, Swindon SN1 7BX\n      Waitrose\n      Waitrose Ltd Doncastle Road, Bracknell, Berksh...\n      Sweet_Potatoes_Q4_(BNA)\n      SN1 7BX\n      RG12 8YA\n      Wiltshire\n      Berkshire\n      0\n      0.00\n      0.0\n      0.000\n    \n    \n      35157\n      0563/2020\n      2020-10-05\n      Sweet Potatoes\n      USA\n      Waitrose\n      Mill Lane, Swindon SN1 7BX\n      Waitrose\n      Waitrose Ltd Doncastle Road, Bracknell, Berksh...\n      Sweet_Potatoes_Q4_(BNA)\n      SN1 7BX\n      RG12 8YA\n      Wiltshire\n      Berkshire\n      0\n      0.00\n      0.0\n      0.000\n    \n    \n      35158\n      2601/2020\n      2020-10-14\n      Sweet Potatoes\n      USA\n      Waitrose\n      Ossington Way, Newark NG24 1FF\n      Waitrose\n      Waitrose Ltd Doncastle Road, Bracknell, Berksh...\n      Sweet_Potatoes_Q4_(BNA)\n      NG24 1FF\n      RG12 8YA\n      Nottinghamshire\n      Berkshire\n      0\n      0.00\n      0.0\n      0.000\n    \n    \n      35159\n      2601/2020\n      2020-10-14\n      Sweet Potatoes\n      USA\n      Waitrose\n      Ossington Way, Newark NG24 1FF\n      Waitrose\n      Waitrose Ltd Doncastle Road, Bracknell, Berksh...\n      Sweet_Potatoes_Q4_(BNA)\n      NG24 1FF\n      RG12 8YA\n      Nottinghamshire\n      Berkshire\n      0\n      0.00\n      0.0\n      0.000\n    \n  \n\n35160 rows × 17 columns"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#pandas-overview",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#pandas-overview",
    "title": "Presentation: Communicating code and data",
    "section": "pandas overview",
    "text": "pandas overview\n\nUsing pandas we can control various outputs\nBut these still need a format to display within\nAnd display functionality is not easy\n\nOr convert to a html file\ndf2.iloc[:500].to_html('df2_500.html')\nBut using a style sheet as shown in stack overflow by Parfait"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#ipydatagrid",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#ipydatagrid",
    "title": "Presentation: Communicating code and data",
    "section": "ipydatagrid",
    "text": "ipydatagrid\nhttps://github.com/bloomberg/ipydatagrid"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#itables-code",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#itables-code",
    "title": "Presentation: Communicating code and data",
    "section": "itables code",
    "text": "itables code\nfrom itables import init_notebook_mode\n\nimport itables\ninit_notebook_mode(all_interactive=True)\n\nitables.show(df2)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n    \n      \n      sample_id\n      date_of_sampling\n      description\n      country_of_origin\n      retail_outlet\n      address\n      brand_name\n      packer_/_manufacturer_/_importer\n      product\n      address_postcode\n      packer_postcode\n      address_area\n      packer_area\n      chem_name\n      amount_detected\n      mrl\n      amount_pc\n    \n  Loading... (need help?)"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#dash",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#dash",
    "title": "Presentation: Communicating code and data",
    "section": "Dash",
    "text": "Dash\nhttps://dash.plotly.com/datatable\n\nDownloaded 800,000 times per month, Dash is the original low-code framework for rapidly building data apps in Python, R, Julia, and F# (experimental).\n\nhttps://medium.com/plotly/introducing-jupyterdash-811f1f57c02e\n\n\nOSError: Address 'http://127.0.0.1:8050' already in use.\n    Try passing a different port to run_server."
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#streamlit",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#streamlit",
    "title": "Presentation: Communicating code and data",
    "section": "Streamlit",
    "text": "Streamlit\n\nA faster way to build and share data apps\n\n\nDash can be run within a notebook but is principally an app.\nStreamlit is a similar app.\nBut much easier to code.\n\nimport pandas as pd\nimport streamlit as st\nall_dfs = pd.read_csv(\"./data/combined_df.csv\")\nst.dataframe(all_dfs.head())\n# Or\nst.tables(all_dfs)"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#and-more",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#and-more",
    "title": "Presentation: Communicating code and data",
    "section": "And more",
    "text": "And more\nDataTables\n\nDataTables is a plug-in for the jQuery Javascript library. It is a highly flexible tool, built upon the foundations of progressive enhancement, that adds all of these advanced features to any HTML table.\n\nJupyter widgets\nIf you are looking for Jupyter widgets, have a look at (taken from https://mwouts.github.io/itables/references.html) - QGrid by Quantopian - IPyaggrid by Louis Raison and Olivier Borderies - IPySheet by QuantStack."
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#more-details-on-itables",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#more-details-on-itables",
    "title": "Presentation: Communicating code and data",
    "section": "More details on itables",
    "text": "More details on itables\nFrom my brief review I found itables the best package\n\nIt works\nIt gives lots of control of table output to be consistent with good-practice\n\nColumn width\nNumber formatting\nColumn alignment\n\nAlongside\n\nSearch\nColumn ordering\nHow many rows are shown\nScrolling options\netc"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#itables-code1",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#itables-code1",
    "title": "Presentation: Communicating code and data",
    "section": "itables code1",
    "text": "itables code1\ndef _indi_columnDefs(cols, format,col_width):\n    if 'num_format' in format:\n        if format['num_format'] == \"#,##0\":\n            format_str = \"',', '.', 0, ''\"\n        elif format['num_format'] ==  '0.0':\n            format_str = \"',', '.', 1, ''\"\n        else: \n            format_str = \"',', '.', 3, ''\"\n    else:\n        format_str = \"',', '.', 3, ''\"\n    columnDefs= {\n            \"targets\": cols,\n            \"className\":f\"dt-{format['align']}\",\n            \"render\": JavascriptCode(f\"$.fn.dataTable.render.number({format_str})\"),\n            \"width\": f\"{col_width}px\",\n        }\n    return columnDefs"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#itables-code2",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#itables-code2",
    "title": "Presentation: Communicating code and data",
    "section": "itables code2",
    "text": "itables code2\nstyle = {\n    \"general\": {\n        \"table_output\" :{\n            \"pct_cols\" : {\n                \"regex_defined\": [\"%\" , \"[Pp]ercentage\"],\n                \"format\": {\"num_format\": \"0.0\", \"align\": \"right\"}\n            },\n            \"total_cols\" : {\n                    \"regex_defined\": [\"[Tt]otal\" ],\n                    \"format\": { \"num_format\": \"0.0\", \"align\": \"center\"}\n            },\n            \"code_cols\" : {\n                \"regex_defined\": [\"[Cc]ode\"],\n                \"format\": { \"align\": \"left\"}\n            },\n            \"_else\" : {\n                \"regex_defined\": [],\n                \"format\": {\"num_format\": \"#,##0\", \"align\": \"right\"}\n            }\n        \n        },\n        \"col_width\": 130.0\n    }\n}"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#itables-code3",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#itables-code3",
    "title": "Presentation: Communicating code and data",
    "section": "itables code3",
    "text": "itables code3\n\ndef _create_style_col(style: dict, df: pd.DataFrame):\n    style_col = {}\n    colsAll=[]\n    for keys in style['general']['table_output'].keys():\n        if keys !='_else':\n            # print(\">>\",keys)\n            cols=[]\n            for i,column in enumerate(df.columns):\n                # print(column)\n                if [True for reg in style['general']['table_output'][keys]['regex_defined'] if re.search(reg,column)]:\n                    # print(i,column)\n                    cols.append(i)\n            # print(cols)\n            colsAll = colsAll + cols\n            style_col[keys]=cols\n\n    style_col['_else'] = [ i for i,column in enumerate(df.columns) if i not in colsAll]\n\n    return style_col"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#itables-code4",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#itables-code4",
    "title": "Presentation: Communicating code and data",
    "section": "itables code4",
    "text": "itables code4\ndef create_columnDefs(style: dict, df: pd.DataFrame):\n    \n    style_col = _create_style_col(style, df)\n    columnDefs = []\n    for keys in style['general']['table_output'].keys():\n        columnDefs.append( _indi_columnDefs(style_col[keys],\n                                            style['general']['table_output'][keys]['format'],\n                                            style['general']['col_width']) )\n    return columnDefs"
  },
  {
    "objectID": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#itables-code5",
    "href": "posts/Communicating_Code/PresentationOfCommunicatingCode.html#itables-code5",
    "title": "Presentation: Communicating code and data",
    "section": "itables code5",
    "text": "itables code5\n\ncolumnDefs= create_columnDefs(style, data)\nshow(\n    data,\n    # style=\"table-layout:auto;width:80%;float:left\",\n    classes=\"display\",\n    \n    # specify how many rows\n    lengthMenu = [25,100,-1],\n    # or to scroll through data\n    scrollX=True,\n    # scrollY=\"800px\", \n    scrollCollapse=True, \n    # paging=False,\n\n    style=f\"width:{style['general']['col_width']*10}px\",\n    autoWidth=False,\n\n    # add footer\n    # footer=True,\n    \n    columnDefs=columnDefs,\n    tags =f'<caption style=\"caption-side: Bottom\">File: {files[0]} and Tab: {ansa.value}</caption>'\n)"
  },
  {
    "objectID": "posts/GolfSwing/2021-12-01-GolfPos1FastAI.html#introduction",
    "href": "posts/GolfSwing/2021-12-01-GolfPos1FastAI.html#introduction",
    "title": "Body position recognition using FastAI",
    "section": "Introduction",
    "text": "Introduction\nThis is a first step in a project to analyse golf swings.\nIn this first step I try to identify different parts of the body and golf equipment during the golf swing. This step is of limited success for overall analysis but the steps used are useful for the lessons learnt.\nThis work uses deep learning to identify locations (vectors) on images and fitting by regression.\nIn this step I will use a dataSet found at  Git Hub GolfSwing and the paper of the work https://arxiv.org/abs/1903.06528.\nWhat this dataset/paper does is split the golf swing into a number of sequences based on the position of the body and golf club, e.g. start, golf club parallel to ground, striking the ball etc. We will call these the golf positions. These positions are shown below.\n\n\n\nvia GIPHY\n\n\nThe dataset includes a series of videos that have been characterised based on the different swing sequences.\n\nSteps in this page\n\nDownload the video dataset and the details of the frames of the different positions\nCreate images at the different positions from the videos\nClassify points on the images and a file for each image of these\nUpload data to GitHub and download on notebook for analysis\nUse deep learning to identify the positions on the images"
  },
  {
    "objectID": "posts/GolfSwing/2021-12-01-GolfPos1FastAI.html#use-the-video-analysis-dataset-to-create-images-of-golf-swings",
    "href": "posts/GolfSwing/2021-12-01-GolfPos1FastAI.html#use-the-video-analysis-dataset-to-create-images-of-golf-swings",
    "title": "Body position recognition using FastAI",
    "section": "Use the video analysis dataset to create images of golf swings",
    "text": "Use the video analysis dataset to create images of golf swings\nFirst I cloned the directory (https://github.com/wmcnally/golfdb) onto my local PC. I then need to identify which videos to use- I want the ones behind the golfer and preferably of lower frame rate.\nBelow are the names of the videos I selected\n\nimport numpy as np\nimport os\n\nuseVids=[1,3,5,7,13,24,43,46,48,71,77,81,83,89,93,242,681,1060]\nnp.shape(useVids)\n\n(18,)\n\n\nI now want to find the frames in each video that represent the selected positions.\nThese exist in a ‘.pkl’ file. So we open the file and then select the videos (rows) we want to use.\n\nimport pandas as pd\nimport pickle\n\nfile_path= cda + \"\\data\\\\golfDB.pkl\"\n\n\ndata = pickle.load(open(file_path,\"rb\"))\naa=[]\ni=0\nfor ii in useVids:\n    if i==0:\n        aa=data[ii==data.id]\n        \n    else:\n        aa=aa.append(data[ii==data.id])\n       \n    i=i+1\naa.reset_index(drop=True,inplace=True)\naa.tail()\n\n\nIn the DataFrame (aa) the details we want are just the ‘events’ so we know what frames to save as images from the videos\nFirst we create a function that takes a video location and details of the frames (or the selected golf positions) and then creates a new folder containing images of those frames.\nThis uses the library cv2 and a secondary check to normalise the positions if it is different from that given (this was useful in earlier versions but later ones the frame number matched that given by the aa dataFrame).\nThe function works by finding a frame rate then stepping through the video by adding the time per frame after each step. If the frame is at a position given by the input (from aa) it is saved as an image.\n\ndef createImages(fila,pos):\n    ''' \n    Given a video file location (fila) it will save as images to a folder\n    Given positions in video (pos) these images from the video are saved\n    pos is created based on positions of swings\n    '''\n    import cv2\n    import numpy as np\n    import os\n    \n    # create a video capture object\n    cap = cv2.VideoCapture(fila)\n    \n    # get details of the video clip\n    duration = cap.get(cv2.CAP_PROP_POS_MSEC)\n    \n    frame_count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n    fps = cap.get(cv2.CAP_PROP_FPS)\n    duration_seconds = frame_count / fps\n    print('duration is ',duration,'. frame_count is ',frame_count,'. fps is ',fps,'. duration sec is',duration_seconds)\n    \n    #alter pos based on frame count\n    posb4=pos\n    pos=(pos/(np.max(pos)/frame_count))\n    pos=np.array([int(nn) for nn in pos])\n    pos=pos[1:-2]#ignore first value and last two\n    \n    \n    # create a folder if it doesn't exist\n    folder = fila.split('\\\\')[-1].split('.')[0]\n    folder = '_images'+folder\n    print(folder)\n    try:\n        os.mkdir(folder)\n    except:\n        pass\n\n    \n    vidcap = cap\n    \n    # this function creates an image from part of a video and \n    # saves as a JPG file\n    def getFrame(sec,go):\n        vidcap.set(cv2.CAP_PROP_POS_MSEC,sec)\n        hasFrames,image = vidcap.read()\n        if hasFrames and go:\n            cv2.imwrite(os.path.join(folder,\"frame{:d}.jpg\".format(count)), image)     # save frame as JPG file\n        return hasFrames\n    \n    # goes through the video clip and steps through based on frame rate\n    sec = 0\n    frameRate = 1000/fps \n    count=1\n    go=0\n    success = True\n    while success:\n        count = count + 1\n        sec = sec + frameRate\n        #only saves images if at positions in pos\n        if count in pos:\n            go=1\n        else:\n            go=0\n        success = getFrame(sec,go)\n\n    print(\"{} images are extacted in {}.\".format(count,folder))\n\nAnd below I call the script for the videos I selected\n\nimport cv2\nfila = cda + '\\\\data\\\\videos_160\\\\'\nfor ii,aai in enumerate(aa.id):\n    fold = fila + str(aai)+'.mp4'\n    pos=aa.iloc[ii,7]\n    pos=pos-pos[0]\n    if ii>1:\n        cII(fold,pos)\n        cap = createImages.VideoCapture(fold)\n\nSo now we have a series of folders for each video with images given by the selected positions"
  },
  {
    "objectID": "posts/GolfSwing/2021-12-01-GolfPos1FastAI.html#manually-classify-points-on-the-images",
    "href": "posts/GolfSwing/2021-12-01-GolfPos1FastAI.html#manually-classify-points-on-the-images",
    "title": "Body position recognition using FastAI",
    "section": "Manually classify points on the images",
    "text": "Manually classify points on the images\nTo be able to perform analysis on the images they first need to be labelled.\nTo do this I decided to take the manual approach and classify the images myself. I decided to choose the following regions in each image: - The ball - The end of the golf club (clubhead) - The back wrist - the back elbow - the top of the head\nThis is done using the follwing function\n\ndef imDo(im):\n    \n    fig=plt.figure(figsize=(20, 15))\n    plt.imshow(im)\n\n    def tellme(s):\n        print(s)\n        plt.title(s, fontsize=16)\n\n    tellme('You will define golf swing, click to begin')\n\n    plt.waitforbuttonpress()\n\n    while True:\n        pts = []\n        while len(pts) < 5:\n            tellme('Select golf ball-golf club- wrist- elbow- head with mouse')\n            pts = np.asarray(plt.ginput(5, timeout=-1))\n            if len(pts) < 5:\n                tellme('Too few points, starting over')\n                time.sleep(1)  # Wait a second\n        \n        ph = plt.plot(pts[:, 0], pts[:, 1], marker='x',markersize=20,markeredgewidth=3)\n\n        tellme('Happy? Key click for yes, mouse click for no')\n\n        if plt.waitforbuttonpress():\n            break\n    plt.close(fig)\n    return pts\n\nBefore we can call this function we want to make sure the image appears as a new window\nAlso some imports\n\nimport fastbook\n\nfrom fastbook import *\nfrom fastai.vision.all import *\nimport matplotlib\n\ncda = os.getcwd()\n\nmatplotlib.use('TKAgg')\n\nNow for each image file created, the script below runs imDo which plots the image then asks the user to select 5 points on the image for classification.\nthese points are then save as txt file with the same name as the image file to be used later in modeling\n\n\nfoldOuta=cda+'//_dataSel//'\nlsa = os.listdir(foldOuta)\nlsa\nptsALL=[]\nfor ii,folds in enumerate(lsa):\n    if ii>0:\n        print(folds)\n        img_files = get_image_files(foldOuta+folds)\n        for fils in img_files:\n            im = PILImage.create(fils)\n            pts=imDo(im)\n            ptsALL.append(pts)\n            fnom=str(fils).split('\\\\')[-1].split('.')[0]\n            \n            np.savetxt(foldOuta+folds+'\\\\'+fnom+'.txt',pts)"
  },
  {
    "objectID": "posts/GolfSwing/2021-12-01-GolfPos1FastAI.html#upload-data-for-use-in-modeling",
    "href": "posts/GolfSwing/2021-12-01-GolfPos1FastAI.html#upload-data-for-use-in-modeling",
    "title": "Body position recognition using FastAI",
    "section": "Upload data for use in modeling",
    "text": "Upload data for use in modeling\nFastai has a function called untar_data that prepares images in a .tgz folder ready to use for analysis.\nA tgz file can be made by a Python script, but all the ones I tried produced an error, so instead I used\nTo create a tar file see https://opensource.com/article/17/7/how-unzip-targz-file\nOpen up a terminal go to the folder that contains the folder wanting to compress and then tar with the command line\ntar –create –verbose –file GC.tgz GolfComb\nI have then uploaded it to GitHub. Go to the file on Github open it and right click on ‘view raw’ and select copy link."
  },
  {
    "objectID": "posts/GolfSwing/2021-12-01-GolfPos1FastAI.html#model-the-data",
    "href": "posts/GolfSwing/2021-12-01-GolfPos1FastAI.html#model-the-data",
    "title": "Body position recognition using FastAI",
    "section": "Model the data",
    "text": "Model the data\nThe rest needs to be done with a GPU. I have done this with https://colab.research.google.com/ (free time is limited but details not published) and the code tab for a notebook on https://www.kaggle.com/ (36 h per month for free)\nFirst import the fastai stuff\n\n!pip install -Uqq fastbook\nimport fastbook\nfastbook.setup_book()\nfrom fastbook import *\n\n\nfrom fastai.vision.all import *\n\nimport os\nimport re\nimport numpy\n\nuntar the data and set the path\n\nurl='https://github.com/ThomasHSimm/GolfSwingTSimm/blob/main/_dataSel/GC.tgz?raw=true'\n\npath = untar_data(url)\n\n\nPath.BASE_PATH = path\n\nHave a look at the data\n\n(path/'Test').ls()\n\n\nA function to classify the points on the image\nLoads the text file for each image and returns a TensorPoint object of points on the image\n\ndef get_pointa_img(fileo):\n     \n    txtfile = str(fileo)[0:-4] + '.txt'\n    # print(txtfile)\n    pts=np.loadtxt(txtfile)\n    pp=pts[-1,:]\n    # print(pp)\n    return TensorPoint(pp)\n\nCreate a DataBlock\nThe DataBlock is the foundation of the model. It needs to know - the location of the images, - the label for the images (points on images in this case) - separation of data into test and validation sets (done automatically if not specified) - the type of data used blocks=(ImageBlock, PointBlock) - any resizing of images - any transforms (Data Augmentation)\n\nitem_tfms = [Resize(448, method='squish')]\nbiwi = DataBlock(\n    blocks=(ImageBlock, PointBlock),\n    get_items=get_image_files,\n    item_tfms=item_tfms,\n    get_y=get_pointa_img,\n    batch_tfms=[*aug_transforms(size=224, min_scale=0.75),\n                               Normalize.from_stats(*imagenet_stats)])\n\nNow create a DataLoaders object which has the path of the data and the batch size (here 30)\nBatch size is important to specify to avoid memory issues\n\ndls = biwi.dataloaders(path,30)\n\nNow create the learner\nPass it the dataLoaders, we’re doing transfer learning from resnet50 (imageNet trained model), what metrics we’ll use for loss, and the range in y values we want\n\nlearn = cnn_learner(dls, resnet50, metrics=partial(accuracy_multi, thresh=0.02),y_range=(-1,1))\n\nHave a look at the data. Can see the transforms\n\ndls.show_batch(max_n=8, figsize=(12,12))\n\n\n\n\nFind the best learning rate\n\n\nlearn.lr_find()\n\n\nTo fit the model we have a few options:\n\nlearn.fit(10,lr=4e-3)\nlearn.fit_one_cycle()\nlearn.fine_tune(10, base_lr=1e-3, freeze_epochs=7)\nlearn.fine_tune(15, lr)\n\nFastAI adds an extra 2 layers on the end of neural network these can then be fitted using fine_tune. It is recommended to do a few fits frozen before unfreezing. This is normally the best option for transfer learning.\nBut the other ones can be used. In general fit can be more unstable and lead to bigger losses, but can be useful if fine_tune is not bringing losses down.\nhttps://forums.fast.ai/t/fine-tune-vs-fit-one-cycle/66029/6\nfit_one_cycle = New Model\nfine_tuning = with Transfer Learning?\n\nI’d say yes but with a very strong but, only because it’s easy to fall into a trap that way. fine_tuning is geared towards transfer learning specifically, but you can also just do fit_one_cycle as well! (Or flat_cos).\n\nFor beginners it’s a great starting fit function (and advanced too), but also don’t forget that you can then build on what that function is doing. For instance, I wonder how modifying/adapting that function for Ranger/flat_cos would need to change!\n\nlearn.fine_tune(10, base_lr=1e-3, freeze_epochs=7)\n\n\n\nlearn.lr_find()\n\n\nSome more fitting, reducing the learning rate after steps\n\nlearn.fit(20,lr=1e-4)\n\n\nSome more fitting\nMixing fit with fine_tune and reducing learning rate seems to work best for reducing loss\nLoss here is:\ntrain_loss  valid_loss\n\n0.054042    0.008305"
  },
  {
    "objectID": "posts/GolfSwing/2021-12-01-GolfPos1FastAI.html#results",
    "href": "posts/GolfSwing/2021-12-01-GolfPos1FastAI.html#results",
    "title": "Body position recognition using FastAI",
    "section": "Results",
    "text": "Results\nLook at the results, pretty good for ~10 mins of 81 images of learning although doesn’t always get the top of the head.\n\nlearn.show_results()\n\n\n\n#save the model\nlearn.export(fname='headTry1.pkl')\n\nHowever, when this is generalised to other points, such as hands and clubhead, that are less static the results are poor.\nPresumably a combination of the low resolution of the images making it difficult to identify features and the lack of images.\n\nIncreasing the res of the images/videos improves the classification considerably.\nBut still not quite there, probably needs more labelling\ntrain_loss     valid_loss\n0.030079       0.031188"
  },
  {
    "objectID": "posts/GolfSwing/2021-12-16-WorkingWithVideos.html#intoduction",
    "href": "posts/GolfSwing/2021-12-16-WorkingWithVideos.html#intoduction",
    "title": "ThomasHSimm",
    "section": "Intoduction",
    "text": "Intoduction\nSome bits of python code to use videos\n\n#hide\nloca='C:\\\\Users\\\\44781\\\\Documents\\\\GitHub\\\\GolfSwingTSimm\\\\data\\\\golfDB.pkl'"
  },
  {
    "objectID": "posts/GolfSwing/2021-12-16-WorkingWithVideos.html#some-imports",
    "href": "posts/GolfSwing/2021-12-16-WorkingWithVideos.html#some-imports",
    "title": "ThomasHSimm",
    "section": "Some imports",
    "text": "Some imports\n\nimport pandas as pd\nimport os\nimport cv2\nimport numpy as np\n\ndf = pd.read_pickle(loca)\n\nidUSE = 6\n\ndf1=df.loc[idUSE]"
  },
  {
    "objectID": "posts/GolfSwing/2021-12-16-WorkingWithVideos.html#youtube-videos",
    "href": "posts/GolfSwing/2021-12-16-WorkingWithVideos.html#youtube-videos",
    "title": "ThomasHSimm",
    "section": "Youtube videos",
    "text": "Youtube videos\nCan have various issues so check https://pytube.io/en/latest/user/install.html if issues in use.\nI used !pip install pytube then after a week I was getting errors. So in terminal I did\n!pip install pytube then clone directory\ngit clone git://github.com/pytube/pytube.git and finally install\ncd pytube python -m pip install .\nImport the module, and here I’ll use the golf dataset from another post\n\nfrom pytube import YouTube\n\nytID=df1['youtube_id']\nyt = YouTube('http://youtube.com/watch?v='+ytID)\n\nThe youtube object contains several videos in different formats.\n\n#collapse-output\nyt.streams\n\n[<Stream: itag=\"17\" mime_type=\"video/3gpp\" res=\"144p\" fps=\"7fps\" vcodec=\"mp4v.20.3\" acodec=\"mp4a.40.2\" progressive=\"True\" type=\"video\">, <Stream: itag=\"18\" mime_type=\"video/mp4\" res=\"360p\" fps=\"30fps\" vcodec=\"avc1.42001E\" acodec=\"mp4a.40.2\" progressive=\"True\" type=\"video\">, <Stream: itag=\"22\" mime_type=\"video/mp4\" res=\"720p\" fps=\"30fps\" vcodec=\"avc1.64001F\" acodec=\"mp4a.40.2\" progressive=\"True\" type=\"video\">, <Stream: itag=\"137\" mime_type=\"video/mp4\" res=\"1080p\" fps=\"30fps\" vcodec=\"avc1.640028\" progressive=\"False\" type=\"video\">, <Stream: itag=\"248\" mime_type=\"video/webm\" res=\"1080p\" fps=\"30fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\">, <Stream: itag=\"136\" mime_type=\"video/mp4\" res=\"720p\" fps=\"30fps\" vcodec=\"avc1.4d401f\" progressive=\"False\" type=\"video\">, <Stream: itag=\"247\" mime_type=\"video/webm\" res=\"720p\" fps=\"30fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\">, <Stream: itag=\"135\" mime_type=\"video/mp4\" res=\"480p\" fps=\"30fps\" vcodec=\"avc1.4d401f\" progressive=\"False\" type=\"video\">, <Stream: itag=\"244\" mime_type=\"video/webm\" res=\"480p\" fps=\"30fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\">, <Stream: itag=\"134\" mime_type=\"video/mp4\" res=\"360p\" fps=\"30fps\" vcodec=\"avc1.4d401e\" progressive=\"False\" type=\"video\">, <Stream: itag=\"243\" mime_type=\"video/webm\" res=\"360p\" fps=\"30fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\">, <Stream: itag=\"133\" mime_type=\"video/mp4\" res=\"240p\" fps=\"30fps\" vcodec=\"avc1.4d4015\" progressive=\"False\" type=\"video\">, <Stream: itag=\"242\" mime_type=\"video/webm\" res=\"240p\" fps=\"30fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\">, <Stream: itag=\"160\" mime_type=\"video/mp4\" res=\"144p\" fps=\"30fps\" vcodec=\"avc1.4d400c\" progressive=\"False\" type=\"video\">, <Stream: itag=\"278\" mime_type=\"video/webm\" res=\"144p\" fps=\"30fps\" vcodec=\"vp9\" progressive=\"False\" type=\"video\">, <Stream: itag=\"139\" mime_type=\"audio/mp4\" abr=\"48kbps\" acodec=\"mp4a.40.5\" progressive=\"False\" type=\"audio\">, <Stream: itag=\"140\" mime_type=\"audio/mp4\" abr=\"128kbps\" acodec=\"mp4a.40.2\" progressive=\"False\" type=\"audio\">, <Stream: itag=\"249\" mime_type=\"audio/webm\" abr=\"50kbps\" acodec=\"opus\" progressive=\"False\" type=\"audio\">, <Stream: itag=\"250\" mime_type=\"audio/webm\" abr=\"70kbps\" acodec=\"opus\" progressive=\"False\" type=\"audio\">, <Stream: itag=\"251\" mime_type=\"audio/webm\" abr=\"160kbps\" acodec=\"opus\" progressive=\"False\" type=\"audio\">]\n\n\nWe can then filter the results based on criteria. By running yt.streams.filter?? we can see more on the code, as shown below\n\nSignature:\nyt.streams.filter(\n    fps=None,\n    res=None,\n    resolution=None,\n    mime_type=None,\n    type=None,\n    subtype=None,\n    file_extension=None,\n    abr=None,\n    bitrate=None,\n    video_codec=None,\n    audio_codec=None,\n    only_audio=None,\n    only_video=None,\n    progressive=None,\n    adaptive=None,\n    is_dash=None,\n    custom_filter_functions=None,\n)\n\n\nstreams= yt.streams.filter(file_extension='mp4',res='360p')\nstreams\n\n[<Stream: itag=\"18\" mime_type=\"video/mp4\" res=\"360p\" fps=\"30fps\" vcodec=\"avc1.42001E\" acodec=\"mp4a.40.2\" progressive=\"True\" type=\"video\">, <Stream: itag=\"134\" mime_type=\"video/mp4\" res=\"360p\" fps=\"30fps\" vcodec=\"avc1.4d401e\" progressive=\"False\" type=\"video\">]\n\n\nDownload the stream\n\nstreams.first().download(filename=ytID+'.mp4')\n\n'C:\\\\Users\\\\44781\\\\Documents\\\\GitHub\\\\THS_website\\\\_notebooks\\\\iPuVhnI8pJU.mp4'"
  },
  {
    "objectID": "posts/GolfSwing/2021-12-16-WorkingWithVideos.html#video-player",
    "href": "posts/GolfSwing/2021-12-16-WorkingWithVideos.html#video-player",
    "title": "ThomasHSimm",
    "section": "Video Player",
    "text": "Video Player\nTo play the videos I’ll be using VLC. There can be some issues with installing so I used the exe file from here https://get.videolan.org/vlc/3.0.11/win64/vlc-3.0.11-win64.exe\nAnother fix to install issues is inserting a line like os.add_dll_directory(\"Location\\anaconda3\\\\Lib\\\\site-packages\") into the script\nMore details of using VLC with Python can be found here https://bigl.es/tooling-tuesday-using-vlc-with-python/\nImporting vlc and creating a media object and then playing it.\n\nimport vlc\n\nvidLoc=ytID+'.mp4'\n\nmedia = vlc.MediaPlayer(vidLoc)\n\nmedia.play()\n\n0\n\n\n\n.play opens a player as shown above. Note there is no pause/play/rewind buttons and the player cannot be closed. The only way to close is to use the command media.stop() or restart the kernel\n\nmedia.stop()\n\nTo make it more like a standard video player the following can be used:\n\nmedia.play() to play\nmedia.pause() to pause\nmedia.set_rate(3) to increase speed of video\nmedia.get_time() and media.set_time(5) to get and set the time of video\nmedia.stop() to close the video\n\nIntsead of doing these on the command line it makes sense to have them as button presses.\nI’ll use easygui here as it’s quite easy and I’m not too bothered about the aesthetics or functionality. Because easygui is a bit clunky. https://bigl.es/tooling-tuesday-easygui/\nSo to use create a easygui buttonbox with different features to control the video. Once clicked the button will then close (can’t keep same button open with this) and open again.\n\nimport easygui \n\n\nwhile True:\n    choice = easygui.buttonbox(title=\"@Golf Media Player\",\n       choices=[\"Play\",\"<<\",\"<<<\",\">>\",\">>>\",\"Pause\",\"Stop\"])\n\n    if choice == \"Play\":\n        media.set_rate(1)\n        media.play()\n    elif choice == \"Pause\":\n        media.pause()\n    elif choice ==\">>\":\n        media.set_rate(2)\n    elif choice==\">>>\":\n        media.set_rate(4)\n    elif choice==\"<<\":\n        timo=media.get_time()\n        timo=timo-2.5*1000\n        if timo<0:\n            timo=0\n        media.set_time(timo)\n    elif choice==\"<<<\":\n        timo=media.get_time()\n        timo=timo-5*1000\n        if timo<0:\n            timo=0\n        media.set_time(timo)\n    else:\n        time_use = media.get_time()/1000\n        print('the time is {}'.format(time_use))\n        media.stop()\n        break\n\nthe time is 15.652"
  },
  {
    "objectID": "posts/GolfSwing/2021-12-16-WorkingWithVideos.html#extracting-images-from-the-video",
    "href": "posts/GolfSwing/2021-12-16-WorkingWithVideos.html#extracting-images-from-the-video",
    "title": "ThomasHSimm",
    "section": "Extracting images from the video",
    "text": "Extracting images from the video\nTo work with a video it is most often easier to convert it to an image.\nTo do this I’ll use cv2 https://pypi.org/project/opencv-python/\nFirst we create a cv2 object of the video with cap = cv2.VideoCapture(vidLoc) then success, image = cap.read() to get images as we scroll through the frames, as shown below.\n\n#collapse-output\ncap = cv2.VideoCapture(vidLoc)\n\nsuccess, image = cap.read()\nwhile success:\n    success, image = cap.read()\n    print(success)\n\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n\n\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nFalse\n\n\nSome useful features of the cap object that will be used are:\n\ncap.get(cv2.CAP_PROP_POS_MSEC) the duration of the video\ncap.get(cv2.CAP_PROP_FRAME_COUNT) the frame count\nframe_count / fps the duration in seconds (the previous two divided)\ncv2.CAP_PROP_FRAME_WIDTH) frame width\ncv2.CAP_PROP_FRAME_HEIGHT) frame height\ncv2.copyMakeBorder() to create an image object to save and select which area of the video to save\ncv2.imwrite(fnom, img) to save the frame as an image\n\nWe could instead use other ways as shown https://learnopencv.com/read-write-and-display-a-video-using-opencv-cpp-python/ to extract images but I’ll use the above\nThe below is modified from https://github.com/wmcnally/golfdb\nIt creates images at a given frame where the video is cropped based on the variable bbox (which is between 0 and 1)\nNote also dim=600 which adjusts the size of the output image\n\n         \ndef getImages(cap,bbox,frame_use):\n    \n    x = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH) * bbox[0])\n    y = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) * bbox[1])\n    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH) * bbox[2])\n    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT) * bbox[3])\n    \n    count = 0\n    success, image = cap.read()\n    while success:\n        count += 1\n        \n        if abs(count-frame_use)==0:\n                dim=600\n                crop_img = image[y:y + h, x:x + w]\n                crop_size = crop_img.shape[:2]\n                ratio = dim / max(crop_size)\n                new_size = tuple([int(x*ratio) for x in crop_size])\n                resized = cv2.resize(crop_img, (new_size[1], new_size[0]))\n                delta_w = dim - new_size[1]\n                delta_h = dim - new_size[0]\n                top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n                left, right = delta_w // 2, delta_w - (delta_w // 2)\n                b_img = cv2.copyMakeBorder(resized, top, bottom, left, right, cv2.BORDER_CONSTANT,\n                                           value=[0.406*255, 0.456*255, 0.485*255])  # ImageNet means (BGR)\n                cv2.imwrite(os.path.join(ytID+str(count)+\".jpg\"),b_img)\n                \n        elif count > frame_use:\n            break\n        success, image = cap.read()\n\n        \n\nAnd to run the above function\n\ncap = cv2.VideoCapture(vidLoc)\ngetImages(cap,df1['bbox'],100)\n\n\nevent_names = {\n    0: 'Address',\n    1: 'Toe-up',\n    2: 'Mid-backswing (arm parallel)',\n    3: 'Top',\n    4: 'Mid-downswing (arm parallel)',\n    5: 'Impact',\n    6: 'Mid-follow-through (shaft parallel)',\n    7: 'Finish'\n}\n_, img = cap.read()\n# cv2.imshow(event_names[0], img)\n\n\nimport matplotlib.pyplot as plt\n\nplt.imshow(img)\n\n<matplotlib.image.AxesImage at 0x23f1d3cc7f0>\n\n\n\n\n\n\nbbox\n\narray([0.1453125 , 0.00138889, 0.46796875, 0.99930556])"
  },
  {
    "objectID": "posts/GolfSwing/2022-02-26-GolfSwingPart2.html#overview",
    "href": "posts/GolfSwing/2022-02-26-GolfSwingPart2.html#overview",
    "title": "ThomasHSimm",
    "section": "Overview",
    "text": "Overview\nIn a previous part Part 1 a neural network model was used to find positions on the body during a golf swing. This work used images taken from videos of golf swing (analysed using the code below by the authors listed) because it is often easier to work with images rather than videos.\nBut to get images of the golf swing to analyse it can be useful to get them at different parts of the golf swing. This is what this part does.\nTaken from https://github.com/wmcnally/golfdb and shown in the paper here https://arxiv.org/abs/1903.06528\n[Ref Paper] McNally, William, et al. \"Golfdb: A video database for golf swing sequencing.\" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops. 2019\nThe code separates the golf swing into a number of different segments based on body and golf club positions.\nThis code can be run on kaggle here https://www.kaggle.com/thomassimm/golfdb-lessimports\nThe input is an mp3 file of a golf swing\nThe ouput is a series of images at different parts of the golf swing"
  },
  {
    "objectID": "posts/GolfSwing/2022-02-26-GolfSwingPart2.html#the-code",
    "href": "posts/GolfSwing/2022-02-26-GolfSwingPart2.html#the-code",
    "title": "ThomasHSimm",
    "section": "The Code",
    "text": "The Code\n\nSpecify the file to use\nAdd downloaded directory (not always necsessary) and specify the video file.\n\n!cp -r ../input/golfdb3/* ./\n\nstra='../input/golfdb3/test_video.mp4'\nstra='../input/golfdb2/golfdb/data/videos_160/1017.mp4'\nstra\n\n\n\nImports, classes and defs\nSome imports. Neural nets using Torch\n\nimport scipy.io\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\n# from eval import ToTensor, Normalize\n# from model import EventDetector\nimport numpy as np\nimport torch.nn.functional as F\nimport cv2\nfrom torch.autograd import Variable\n\nThe following classes and definitions are taken from the files in the GitHub directory\n\nclass SampleVideo(Dataset):\n    def __init__(self, path, input_size=160, transform=None):\n        self.path = path\n        self.input_size = input_size\n        self.transform = transform\n\n    def __len__(self):\n        return 1\n\n    def __getitem__(self, idx):\n        cap = cv2.VideoCapture(self.path)\n        frame_size = [cap.get(cv2.CAP_PROP_FRAME_HEIGHT), cap.get(cv2.CAP_PROP_FRAME_WIDTH)]\n        ratio = self.input_size / max(frame_size)\n        new_size = tuple([int(x * ratio) for x in frame_size])\n        delta_w = self.input_size - new_size[1]\n        delta_h = self.input_size - new_size[0]\n        top, bottom = delta_h // 2, delta_h - (delta_h // 2)\n        left, right = delta_w // 2, delta_w - (delta_w // 2)\n\n        # preprocess and return frames\n        images = []\n        for pos in range(int(cap.get(cv2.CAP_PROP_FRAME_COUNT))):\n            _, img = cap.read()\n            resized = cv2.resize(img, (new_size[1], new_size[0]))\n            b_img = cv2.copyMakeBorder(resized, top, bottom, left, right, cv2.BORDER_CONSTANT,\n                                       value=[0.406 * 255, 0.456 * 255, 0.485 * 255])  # ImageNet means (BGR)\n\n            b_img_rgb = cv2.cvtColor(b_img, cv2.COLOR_BGR2RGB)\n            images.append(b_img_rgb)\n        cap.release()\n        labels = np.zeros(len(images)) # only for compatibility with transforms\n        sample = {'images': np.asarray(images), 'labels': np.asarray(labels)}\n        if self.transform:\n            sample = self.transform(sample)\n        return sample\n\n\nclass ToTensor(object):\n    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n    def __call__(self, sample):\n        images, labels = sample['images'], sample['labels']\n        images = images.transpose((0, 3, 1, 2))\n        return {'images': torch.from_numpy(images).float().div(255.),\n                'labels': torch.from_numpy(labels).long()}\n\n\nclass Normalize(object):\n    def __init__(self, mean, std):\n        self.mean = torch.tensor(mean, dtype=torch.float32)\n        self.std = torch.tensor(std, dtype=torch.float32)\n\n    def __call__(self, sample):\n        images, labels = sample['images'], sample['labels']\n        images.sub_(self.mean[None, :, None, None]).div_(self.std[None, :, None, None])\n        return {'images': images, 'labels': labels}\n\n\nimport torch.nn as nn\nimport math\n\n\"\"\"\nhttps://github.com/tonylins/pytorch-mobilenet-v2\n\"\"\"\n\ndef conv_bn(inp, oup, stride):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\ndef conv_1x1_bn(inp, oup):\n    return nn.Sequential(\n        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n        nn.BatchNorm2d(oup),\n        nn.ReLU6(inplace=True)\n    )\n\n\nclass InvertedResidual(nn.Module):\n    def __init__(self, inp, oup, stride, expand_ratio):\n        super(InvertedResidual, self).__init__()\n        self.stride = stride\n        assert stride in [1, 2]\n\n        hidden_dim = round(inp * expand_ratio)\n        self.use_res_connect = self.stride == 1 and inp == oup\n\n        if expand_ratio == 1:\n            self.conv = nn.Sequential(\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n        else:\n            self.conv = nn.Sequential(\n                # pw\n                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # dw\n                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n                nn.BatchNorm2d(hidden_dim),\n                nn.ReLU6(inplace=True),\n                # pw-linear\n                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n                nn.BatchNorm2d(oup),\n            )\n\n    def forward(self, x):\n        if self.use_res_connect:\n            return x + self.conv(x)\n        else:\n            return self.conv(x)\n\n\nclass MobileNetV2(nn.Module):\n    def __init__(self, n_class=1000, input_size=224, width_mult=1.):\n        super(MobileNetV2, self).__init__()\n        block = InvertedResidual\n        min_depth = 16\n        input_channel = 32\n        last_channel = 1280\n        interverted_residual_setting = [\n            # t, c, n, s\n            [1, 16, 1, 1],\n            [6, 24, 2, 2],\n            [6, 32, 3, 2],\n            [6, 64, 4, 2],\n            [6, 96, 3, 1],\n            [6, 160, 3, 2],\n            [6, 320, 1, 1],\n        ]\n\n        # building first layer\n        assert input_size % 32 == 0\n        input_channel = int(input_channel * width_mult) if width_mult >= 1.0 else input_channel\n        self.last_channel = int(last_channel * width_mult) if width_mult > 1.0 else last_channel\n        self.features = [conv_bn(3, input_channel, 2)]\n        # building inverted residual blocks\n        for t, c, n, s in interverted_residual_setting:\n            output_channel = max(int(c * width_mult), min_depth)\n            for i in range(n):\n                if i == 0:\n                    self.features.append(block(input_channel, output_channel, s, expand_ratio=t))\n                else:\n                    self.features.append(block(input_channel, output_channel, 1, expand_ratio=t))\n                input_channel = output_channel\n        # building last several layers\n        self.features.append(conv_1x1_bn(input_channel, self.last_channel))\n        # make it nn.Sequential\n        self.features = nn.Sequential(*self.features)\n\n        # building classifier\n        self.classifier = nn.Sequential(\n            nn.Dropout(0.2),\n            nn.Linear(self.last_channel, n_class),\n        )\n\n        self._initialize_weights()\n\n    def forward(self, x):\n        x = self.features(x)\n        x = x.mean(3).mean(2)\n        x = self.classifier(x)\n        return x\n\n    def _initialize_weights(self):\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. / n))\n                if m.bias is not None:\n                    m.bias.data.zero_()\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n            elif isinstance(m, nn.Linear):\n                n = m.weight.size(1)\n                m.weight.data.normal_(0, 0.01)\n                m.bias.data.zero_()\n\n\nimport torch.nn as nn\nclass EventDetector(nn.Module):\n    def __init__(self, pretrain, width_mult, lstm_layers, lstm_hidden, bidirectional=True, dropout=True):\n        super(EventDetector, self).__init__()\n        self.width_mult = width_mult\n        self.lstm_layers = lstm_layers\n        self.lstm_hidden = lstm_hidden\n        self.bidirectional = bidirectional\n        self.dropout = dropout\n\n        net = MobileNetV2(width_mult=width_mult)\n        state_dict_mobilenet = torch.load('mobilenet_v2.pth.tar')\n        if pretrain:\n            net.load_state_dict(state_dict_mobilenet)\n\n        self.cnn = nn.Sequential(*list(net.children())[0][:19])\n        self.rnn = nn.LSTM(int(1280*width_mult if width_mult > 1.0 else 1280),\n                           self.lstm_hidden, self.lstm_layers,\n                           batch_first=True, bidirectional=bidirectional)\n        if self.bidirectional:\n            self.lin = nn.Linear(2*self.lstm_hidden, 9)\n        else:\n            self.lin = nn.Linear(self.lstm_hidden, 9)\n        if self.dropout:\n            self.drop = nn.Dropout(0.5)\n\n    def init_hidden(self, batch_size):\n        if self.bidirectional:\n            return (Variable(torch.zeros(2*self.lstm_layers, batch_size, self.lstm_hidden).cuda(), requires_grad=True),\n                    Variable(torch.zeros(2*self.lstm_layers, batch_size, self.lstm_hidden).cuda(), requires_grad=True))\n        else:\n            return (Variable(torch.zeros(self.lstm_layers, batch_size, self.lstm_hidden).cuda(), requires_grad=True),\n                    Variable(torch.zeros(self.lstm_layers, batch_size, self.lstm_hidden).cuda(), requires_grad=True))\n\n    def forward(self, x, lengths=None):\n        batch_size, timesteps, C, H, W = x.size()\n        self.hidden = self.init_hidden(batch_size)\n\n        # CNN forward\n        c_in = x.view(batch_size * timesteps, C, H, W)\n        c_out = self.cnn(c_in)\n        c_out = c_out.mean(3).mean(2)\n        if self.dropout:\n            c_out = self.drop(c_out)\n\n        # LSTM forward\n        r_in = c_out.view(batch_size, timesteps, -1)\n        r_out, states = self.rnn(r_in, self.hidden)\n        out = self.lin(r_out)\n        out = out.view(batch_size*timesteps,9)\n\n        return out\n\n\n\nRun the code\n\nseq_length=64\n\nds = SampleVideo(stra, transform=transforms.Compose([ToTensor(),\n                                Normalize([0.485, 0.456, 0.406],\n                                          [0.229, 0.224, 0.225])]))\n\ndl = DataLoader(ds, batch_size=1, shuffle=False, drop_last=False)\n\nmodel = EventDetector(pretrain=True,\n                      width_mult=1.,\n                      lstm_layers=1,\n                      lstm_hidden=256,\n                      bidirectional=True,\n                      dropout=False)\ntry:\n    save_dict = torch.load('models/swingnet_1800.pth.tar')\nexcept:\n    print(\"Model weights not found. Download model weights and place in 'models' folder. See README for instructions\")\n    \n    \ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint('Using device:', device)\nmodel.load_state_dict(save_dict['model_state_dict'])\nmodel.to(device)\nmodel.eval()\nprint(\"Loaded model weights\")\n\nprint('Testing...')\nfor sample in dl:\n    images = sample['images']\n    # full samples do not fit into GPU memory so evaluate sample in 'seq_length' batches\n    batch = 0\n    while batch * seq_length < images.shape[1]:\n        if (batch + 1) * seq_length > images.shape[1]:\n            image_batch = images[:, batch * seq_length:, :, :, :]\n        else:\n            image_batch = images[:, batch * seq_length:(batch + 1) * seq_length, :, :, :]\n        logits = model(image_batch.cuda())\n        if batch == 0:\n            probs = F.softmax(logits.data, dim=1).cpu().numpy()\n        else:\n            probs = np.append(probs, F.softmax(logits.data, dim=1).cpu().numpy(), 0)\n        batch += 1\n\n        \nevents = np.argmax(probs, axis=0)[:-1]\nprint('Predicted event frames: {}'.format(events))\n\n\nconfidence = []\nfor i, e in enumerate(events):\n    confidence.append(probs[e, i])\nprint('Confidence: {}'.format([np.round(c, 3) for c in confidence]))\n\nOutput:\nUsing device: cuda\nLoaded model weights\nTesting…\nPredicted event frames: [ 82 121 137 166 189 203 213 245]\nConfidence: [0.215, 0.376, 0.79, 0.767, 0.827, 0.968, 0.935, 0.247]"
  },
  {
    "objectID": "posts/GolfSwing/2022-02-26-GolfSwingPart2.html#plot-the-results",
    "href": "posts/GolfSwing/2022-02-26-GolfSwingPart2.html#plot-the-results",
    "title": "ThomasHSimm",
    "section": "Plot the results",
    "text": "Plot the results\n\nimport os\n##delte images\nlsa=os.listdir()\nfimg=[ ll for ll in lsa if ll.split('.')[-1]=='jpg']\n# print(fimg)\nimgs=[os.remove(ff) for ff in fimg]\n\nfimg=[ ll for ll in lsa if ll.split('.')[-1]=='jpg']\n\n\ndef createImages(fila,pos,nomS):\n    ''' \n    Given a video file location (fila) it will save as images to a folder\n    Given positions in video (pos) these images from the video are saved\n    pos is created based on positions of swings\n    '''\n    import cv2\n    cap = cv2.VideoCapture(fila)\n    eventNom=[0,1,2,3,4,5,6,7]\n    for i, e in enumerate(events):\n        cap.set(cv2.CAP_PROP_POS_FRAMES, e)\n        _, img = cap.read()\n        cv2.imwrite(os.path.join(os.getcwd(),'_'+ nomS+'_'+\"frame{:d}.jpg\".format(eventNom[i])), img)     # save frame as JPG file\n    \n    \nfila=stra\npos=events\ncreateImages(fila,pos,'10')\n\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nlsa=os.listdir()\nfimg=[ ll for ll in lsa if ll.split('.')[-1]=='jpg']\nfimg.sort()\n\nimgs=[mpimg.imread(ff) for ff in fimg]\n\n\ncap = cv2.VideoCapture(stra)\n\n\n# plt.subplot(4,2,1)\nf, axs = plt.subplots(4,2,figsize=(15,15))\nfor i, e in enumerate(events):\n    cap.set(cv2.CAP_PROP_POS_FRAMES, e)\n    _, img = cap.read()\n    plt.subplot(4,2,i+1)\n    plt.imshow(img)\n    plt.title(e)"
  },
  {
    "objectID": "posts/GolfSwing/2022-02-27-GolfSwingPart3.html#overview",
    "href": "posts/GolfSwing/2022-02-27-GolfSwingPart3.html#overview",
    "title": "ThomasHSimm",
    "section": "Overview",
    "text": "Overview\nIn a previous part Part 1 a neural network model was used to find positions on the body during a golf swing. The model was not particularly succesful, perhaps due to the lack of data (specific to the golf swing) that was used to train the model on.\nThis problem can be got around by using a model that has been pre-trained on human gestures. Several pre-trained models can be found here Pre-trained models. I tried a few and found the chose the model keypoint-rcnn-resnet50-fpn-coco-torch worked well with this data. Link to model and Paper of model.\nThe input to the model is taken from Part 2 which separated a golf video into a series of images of the swing.\nIn this page I use the model with both fiftyOne and as a streamlit app.\n\nyoutube: https://youtu.be/Q0BB0huWb6s https://youtu.be/Q0BB0huWb6s"
  },
  {
    "objectID": "posts/GolfSwing/2022-02-27-GolfSwingPart3.html#code",
    "href": "posts/GolfSwing/2022-02-27-GolfSwingPart3.html#code",
    "title": "ThomasHSimm",
    "section": "Code",
    "text": "Code\nThe code can be run on google colab here COCO50_1 (works best on google chrome)"
  },
  {
    "objectID": "posts/GolfSwing/2022-02-27-GolfSwingPart3.html#installs-and-imports",
    "href": "posts/GolfSwing/2022-02-27-GolfSwingPart3.html#installs-and-imports",
    "title": "ThomasHSimm",
    "section": "Installs and imports",
    "text": "Installs and imports\n\n!pip uninstall opencv_python_headless\n\n!pip install opencv-python-headless==4.5.4.60\n\n!pip install fiftyone\n\nimport fiftyone as fo"
  },
  {
    "objectID": "posts/GolfSwing/2022-02-27-GolfSwingPart3.html#upoad-some-images-to-the-workspace",
    "href": "posts/GolfSwing/2022-02-27-GolfSwingPart3.html#upoad-some-images-to-the-workspace",
    "title": "ThomasHSimm",
    "section": "Upoad some images to the workspace",
    "text": "Upoad some images to the workspace\nUntar and create a dataset object from them\nAnd look at them\nThe image files can be found here GC2.tgz\n\nimport tarfile\nmy_tar = tarfile.open('/content/GC2.tgz')\nmy_tar.extractall('/content/my_folder') # specify which folder to extract to\nmy_tar.close()\n\n\nimport fiftyone as fo\nimport fiftyone.zoo as foz\n\nimport fiftyone as fo\n\nname = \"my_folder\"\ndataset_dir = \"/content\"\n\n# Create the dataset\ndataset = fo.Dataset.from_dir(\n    dataset_dir=dataset_dir,\n    dataset_type=fo.types.ImageDirectory,\n    name=name,\n)\n\nsession = fo.launch_app(dataset)\n\n\nThis screen is interative and allows us to look at the images"
  },
  {
    "objectID": "posts/GolfSwing/2022-02-27-GolfSwingPart3.html#load-the-trained-model",
    "href": "posts/GolfSwing/2022-02-27-GolfSwingPart3.html#load-the-trained-model",
    "title": "ThomasHSimm",
    "section": "Load the trained model",
    "text": "Load the trained model\nApply the model to the dataset\nView the results\n\nmodel = foz.load_zoo_model(\"keypoint-rcnn-resnet50-fpn-coco-torch\")\n\n# label_types=[\"classification\", \"classifications\", \"detections\", \"instances\", \"segmentations\", \"keypoints\", \"polylines\", \"polygons\", \"scalar\"],\n\ndataset.apply_model(model, label_field=\"predictions\",label_types='predictions_keypoints')\n\nsession = fo.launch_app(dataset)\n\n\n\nyoutube: https://youtu.be/dkxtOBWD7Vw"
  },
  {
    "objectID": "posts/GolfSwing/2022-02-27-GolfSwingPart3.html#extract-data-from-the-model",
    "href": "posts/GolfSwing/2022-02-27-GolfSwingPart3.html#extract-data-from-the-model",
    "title": "ThomasHSimm",
    "section": "Extract data from the model",
    "text": "Extract data from the model\nWe might want to use the data from the model outside of fiftyOne.\nIn the following I extract the data so that it can be plotted.\n\ndef plotPredOne(i):\n  import numpy as np\n\n  import matplotlib.pyplot as plt\n  import matplotlib.image as mpimg\n\n  img = mpimg.imread(i['filepath'])\n\n  #need to take account of more than one person in image\n  points1 = np.array(i['predictions_keypoints']['keypoints'][0]['points'])\n  adjPts = np.shape(img)[0]\n  box1 = np.array(i['predictions_detections']['detections'][0]['bounding_box']) \n  box1=box1*adjPts\n  # Bboxes are in [top-left-x, top-left-y, width, height] format\n  box2=np.array([ \n      [box1[0], box1[1]],\n      [box1[0] +box1[2] ,box1[1] ],\n      [box1[0] +box1[2] ,box1[1] +box1[3]] ,\n      [box1[0]  ,box1[1] +box1[3]],\n      [box1[0], box1[1]]\n      ])\n \n  plt.figure()\n  plt.imshow(img)\n\n  plt.plot(points1[:,0]*adjPts,points1[:,1]*adjPts, '+k',markersize=10,linewidth=3)\n  plt.plot(box2[:,0],box2[:,1], '--og',markersize=10,linewidth=3)\n\n    #back of body\n  v=[4,6,12,14,16]\n  plt.plot(points1[v,0]*adjPts,points1[v,1]*adjPts, '-k<',markersize=10,linewidth=2)\n\n  #front of body\n  v=[0,5,11,13,15]\n  plt.plot(points1[v,0]*adjPts,points1[v,1]*adjPts, '-w>',markersize=10,linewidth=2)\n\n  vects = np.array([[ 5,6],#shoulders also 4?\n         [11,12], #hips\n         [13,14], #knees\n         [15,16],#heels\n         [7,8],#elbows\n         [9,10],#hands\n         ]) \n  mak='gcyrmb'\n  for iv,v in enumerate(vects):\n    plt.plot(points1[v,0]*adjPts,points1[v,1]*adjPts, '-'+mak[iv],markersize=10,linewidth=3)\n\n\nfor iii,i in enumerate(dataset):\n  \n  plotPredOne(i)"
  },
  {
    "objectID": "posts/GolfSwing/2022-02-27-GolfSwingPart3.html#convert-into-a-streamlit-app",
    "href": "posts/GolfSwing/2022-02-27-GolfSwingPart3.html#convert-into-a-streamlit-app",
    "title": "ThomasHSimm",
    "section": "Convert into a Streamlit App",
    "text": "Convert into a Streamlit App\nTo convert to a streamlit app I will use the PyTorch module rather than the fiftyOne.\nI will also keep it simple by loading only 3 images- start of swing, top of backswing and at impact- and modeling these at the start of the load part of the app.\nThe app will then just plot the images as shown above.\n\nImports and give the app a title\n\nimport streamlit as st\nimport torch\nimport torchvision\nfrom torchvision import transforms\nfrom PIL import Image\nimport tarfile\nimport os\nfrom torchvision.io import read_image\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nst.title('Golf Swing')\n\n\n\nLoading the data & applying the model\nCreate a function to load data and model the data\nload_data(choi)\nImages are loaded from Gc2.tgz\nmy_tar = tarfile.open(cda2+'/GC2.tgz')\nThe particular model to use is loaded\nmodel = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=True)\nmodel.eval()\nImages are loaded and converted to a tensor\nnumber_img = Image.open(cda2+'/images/'+image_filename)\nconvert_tensor = transforms.ToTensor()\nAnd predictions are made\npredictions=model(imgTens)\nIn the main body the function is called\ndata_load_state = st.text('Loading data...')\npredictions,imgLocAll,cda2=load_data(1)\ndata_load_state.text(\"Loaded data (using st.cache)\")\n\n# So only have to do this when app launches\n@st.cache()\n\n# the function 'choi' is the video file to use\ndef load_data(choi):\n\n    # the images are in the GC2.tgz file- this needs to be untarred first\n    cda = os.getcwd()\n    cda2=cda\n    my_tar = tarfile.open(cda2+'/GC2.tgz')\n    my_tar.extractall(cda2) # specify which folder to extract to\n    my_tar.close()\n\n    # Create a variable of the image names and which video they are part of\n    imgAll=[]\n    vidAll=[]\n    i=0\n    last1=' '\n    for xx in os.listdir(cda2+'/images/'):\n        if xx[-1]=='g':\n            imgAll = np.append(imgAll, xx)\n            if xx.split('_')[1]!=last1:\n                i=i+1\n            vidAll=np.append(vidAll,i)\n            last1=xx.split('_')[1]\n\n    vidAllUnq=np.unique(vidAll)\n    \n    # Load the model to be used\n    model = torchvision.models.detection.keypointrcnn_resnet50_fpn(pretrained=True)\n    model.eval()\n    \n    # Select the images to be used\n    imgs = imgAll[vidAll==vidAllUnq[choi]]\n\n    # make sure in correct order\n    aa=[int(xx.split('_')[-1].split('e')[1].split('.')[0]) for xx in imgs]\n    ind=sorted(range(len(aa)), key=lambda k: aa[k])\n    imgs=imgs[ind]\n\n    # create tensor of images to be used- here 3 (images) X width X height\n    imgTens=[]\n    imgLocAll=[]\n    \n    # Just use the start, top and impact of swing\n    iiUse=[0,3,5]\n    for ii,image_filename in enumerate(imgs):\n    #             print(cda2+'images/'+image_filename)\n        if ii in iiUse:\n            number_img = Image.open(cda2+'/images/'+image_filename)\n            convert_tensor = transforms.ToTensor()\n            number_img=convert_tensor(number_img)\n            imgTens.append(number_img)\n            imgLocAll.append(image_filename)\n\n    # Make the predictions\n    predictions=model(imgTens)\n    \n    return predictions,imgLocAll,cda2\n\n# Outside the function, the load function is called\ndata_load_state = st.text('Loading data...')\npredictions,imgLocAll,cda2=load_data(1)\ndata_load_state.text(\"Loaded data (using st.cache)\")\n\n\n\nStreamlit user interface\nUser selects the images from this box:\nchoice=imgLocAll\nimgSEL = st.sidebar.selectbox(     'Select how to search',      choice)\nDisplay to user what swing it is:\nSwingPos=['Start','Back','Through']\nSwingPos[numSEL]\nAnd at the end of the file the figure is displayed in streamlit with the following command:\nst.pyplot(fig)\n\n\nThe plot part\nExtract the data from the model about different parts of the body:\npoints1=np.array([x.detach().numpy()[0:2] for x in predictions[numSEL]['keypoints'][0]])\nThe plot lines plot different parts of the body, the following plot the back of the body\nv=[4,6,12,14,16]\nplt.plot(points1[v,0]*adjPts,points1[v,1]*adjPts, '-w<',markersize=10,linewidth=2)\n\n# load the images so can be plotted\nimg = mpimg.imread(cda2+'/images/'+imgSEL)\n\n# the image selected\nnumSEL=[oo for oo,x in enumerate(choice) if x==imgSEL][0]\n\n# get data from model as a numpy array - here want keypoints other info is also available\npoints1=np.array([x.detach().numpy()[0:2] for x in predictions[numSEL]['keypoints'][0]])\n\n# create a plot\nfig=plt.figure(figsize=(7,7))\nplt.imshow(img)\n\n# Plot across back and front of body\nadjPts=1\n#back of body\nv=[4,6,12,14,16]\nplt.plot(points1[v,0]*adjPts,points1[v,1]*adjPts, '-w<',markersize=10,linewidth=2)\n\n#front of body\nv=[0,5,11,13,15]\nplt.plot(points1[v,0]*adjPts,points1[v,1]*adjPts, '-k>',markersize=10,linewidth=2)\n\n# Plot over lines on body\nvects = np.array([[ 5,6],#shoulders also 4?\n     [11,12], #hips\n     [13,14], #knees\n     [15,16],#heels\n     [7,8],#elbows\n     [9,10],#hands\n     ]) \nmak='gcyrmb'\nfor iv,v in enumerate(vects):\n    plt.plot(points1[v,0]*adjPts,points1[v,1]*adjPts, '-'+mak[iv],markersize=10,linewidth=3)\n\nLEG=['Back','Front','Shoulders','Hips','Knees','Heels','Elbows','Hands']\nplt.legend(LEG)\nfor x in points1:\n    plt.plot(x[0],x[1],'+b')\n\n\n\nRequirements.txt\nFinally streamlit needs a requirements text in the GitHub repository\n\ntorch\ntorchvision\nPillow\nmatplotlib\nnumpy"
  },
  {
    "objectID": "posts/GolfSwing/2022-02-27-GolfSwingPart3.html#the-streamlit-app",
    "href": "posts/GolfSwing/2022-02-27-GolfSwingPart3.html#the-streamlit-app",
    "title": "ThomasHSimm",
    "section": "The Streamlit App",
    "text": "The Streamlit App\nStreamlit App\nGitHub page\n\nyoutube: https://youtu.be/Q0BB0huWb6s"
  },
  {
    "objectID": "posts/GolfSwing/GolfSwingPart4.html#modifications-to-code-for-separating-swing-video",
    "href": "posts/GolfSwing/GolfSwingPart4.html#modifications-to-code-for-separating-swing-video",
    "title": "ThomasHSimm",
    "section": "Modifications to code for separating swing video",
    "text": "Modifications to code for separating swing video\n\n- Imported files are not saved to a location\nThis means can’t pass ‘path’ when creating the Dataset. So that line is removed from the __init__\nIn theory the video could then be loaded with cap = cv2.VideoCapture(self) but this doesn’t work as openCV requires a file. So a get around for this is create a temp file https://discuss.streamlit.io/t/how-to-access-uploaded-video-in-streamlit-by-open-cv/5831/4\nf = st.file_uploader(\"Upload file\")\ntfile = tempfile.NamedTemporaryFile(delete=False)\ntfile.write(f.read())\nvf = cv.VideoCapture(tfile.name)\nFor the same reason the image files are not saved as a file\n\n\n- Video load issue\nIf the video is loaded a second time ret, img = cv2.VideoCapture there are problems receieving the video i.e. ret=False.\nA get around used was to copy the imported file\nuploaded_filesCOPY = copy.copy( uploaded_files )\n\n\n- Using on a CPU instead of a GPU\nThe code needed modifying slightly to allow it to work using a CPU. Although it does have the following line, a few more changes were needed\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nAdding a description in the loads\ntorch.load('mobilenet_v2.pth.tar',map_location=torch.device('cpu'))\nRemoving the cdu() part at end of e.g. variables\nVariable(torch.zeros(2*self.lstm_layers, batch_size, self.lstm_hidden).cuda()\n# to     \nVariable(torch.zeros(2*self.lstm_layers, batch_size, self.lstm_hidden)"
  },
  {
    "objectID": "posts/olympics/2022-07-20-OlympicsSQL-HeightWeightAge.html#background",
    "href": "posts/olympics/2022-07-20-OlympicsSQL-HeightWeightAge.html#background",
    "title": "Olympics data with SQL and pandas- height weight and age",
    "section": "Background",
    "text": "Background\nI did some initial plots on the changes in the characteristics of athletes given in the data, height, weight and age, of athletes attending the Olympics by year (see below).\nFrom these plots I was really intrigued as to what may be the cause of these changes.\nMainly what was happening between 1960 and 1980 were there seemed to be changes in each of the parameters?\nMy initial thought was this could be related to some combination of - a switch from amateurs to professionals - the Cold War between USA and USSR - an after effect of WWII\n\n\n\n\nOlympic Background\nThroughout much of the Olympic’s history there has been tension around professionals and amateur athletes. The games were intended for amateur athletes, and those who played sport professionally were banned or even had their medals stripped.\nThe reasoning behind amateurism was based on how sport was seen by the aristrocracy and greatly influenced Pierre de Coubertin, who is thought of as the father of the Olympic games:\n\"There was also a prevailing concept of fairness, in which practising or training was considered tantamount to cheating.[2] Those who practised a sport professionally were considered to have an unfair advantage over those who practised it merely as a hobby.[2]\"\nThe Soviet Union, who competed from 1952-1988, entered teams of athletes who were all nominally students, soldiers, or working in a profession, but all of whom were in reality paid by the state to train on a full-time basis.[3] The situation greatly disadvantaged American and Western European athletes, and was a major factor in the decline of American medal hauls in the 1970s and 1980s. However, workarounds in Western countries also allowed individuals to focus full-time on sport while passing the amateur rules.[4]\nThis abuse of amateur rules by the Eastern Bloc nations prompted the IOC to shift away from pure amateurism.The rules were steadily relaxed from 1972, amounting only to technicalities and lip service, until being completely abandoned in the 1990s\n\nWikipedia Olympic Games And Amateurism\nEassom 1994, pp. 120–123\n“The Role of Sports in The Soviet Union – Guided History”. blogs.bu.edu.\nDegrees of Difficulty: How Women’s Gymnastics Rose to Prominence and Fell from Grace”, by Georgia Cervin"
  },
  {
    "objectID": "posts/olympics/2022-07-20-OlympicsSQL-HeightWeightAge.html#load-data-and-libraries",
    "href": "posts/olympics/2022-07-20-OlympicsSQL-HeightWeightAge.html#load-data-and-libraries",
    "title": "Olympics data with SQL and pandas- height weight and age",
    "section": "Load data and libraries",
    "text": "Load data and libraries\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pandasql import sqldf\nimport copy\nimport numpy as np\nimport scipy.stats\n\n\ndf_F_S =pd.read_csv('athlete_F_S')\ndf_F_W=pd.read_csv('athlete_F_W')\ndf_M_S=pd.read_csv('athlete_M_S')\ndf_M_W=pd.read_csv('athlete_M_W')\n\ndf_all_athletes= pd.read_csv('all_athletes')\ndf_country= pd.read_csv('country')\ndf_event= pd.read_csv('event')\n# df_games= pd.read_csv('games')\n# df_population= pd.read_csv('population')\n\n# df_country = df_country.groupby('NOC').max()\n# df_country.head(10)"
  },
  {
    "objectID": "posts/olympics/2022-07-20-OlympicsSQL-HeightWeightAge.html#what-are-the-best-weight-height-age",
    "href": "posts/olympics/2022-07-20-OlympicsSQL-HeightWeightAge.html#what-are-the-best-weight-height-age",
    "title": "Olympics data with SQL and pandas- height weight and age",
    "section": "What are the best Weight, Height, Age?",
    "text": "What are the best Weight, Height, Age?\nObviously this will depend on event. But if we average across events what are - the best of values of these? - and how do these change with time?\nTo get this figure the methodology is fairly simple, in the SQL query: - We take the average of weight, height, age across years and medal type - Because we want a simple binary answer (medal or not) we create a variable called medal which is 1 if they got a medal and 0 otherwise - We then group on this and take an average - The two function below are just so we can plot for avg_weight, avg_height and avg_age without repeating the same steps\nhere Male summer athletes are used but the result for female summer athletes show the same trend\n\nmedalQ=sqldf('\\\n    SELECT                                 \\\n        Year,                              \\\n        medal,                             \\\n        AVG(avg_weight)    AS avg_weight,  \\\n        AVG(avg_height)    AS avg_height,  \\\n        AVG(avg_age)       AS avg_age      \\\n    FROM                                   \\\n         (SELECT                           \\\n         Year,                             \\\n         MAX(Medal_Gold,Medal_Silver,Medal_Bronze)\\\n                           AS medal,       \\\n         avg(Weight)       AS avg_weight,  \\\n         AVG(Height)       AS avg_height,  \\\n         AVG(age)          AS avg_age      \\\n         from df_M_S                       \\\n         group by                          \\\n             Year,                         \\\n             Medal_Gold,Medal_Silver,Medal_Bronze           \\\n         order by Year asc) A              \\\n     GROUP BY Year, medal;',locals())  \nmedalQ.head()\n\n\n\n\n\n  \n    \n      \n      Year\n      medal\n      avg_weight\n      avg_height\n      avg_age\n    \n  \n  \n    \n      0\n      1896\n      0\n      70.444444\n      169.916667\n      23.896552\n    \n    \n      1\n      1896\n      1\n      71.551282\n      175.217949\n      23.211671\n    \n    \n      2\n      1900\n      0\n      76.971429\n      175.054545\n      29.428571\n    \n    \n      3\n      1900\n      1\n      72.711355\n      178.202932\n      28.454139\n    \n    \n      4\n      1904\n      0\n      71.742424\n      175.131579\n      26.752080\n    \n  \n\n\n\n\n\ndef modname(string):\n    string=''.join([string[0].upper(),string[1:].lower()])\n    string=string.replace('_',' ')\n    return string\n    \ndef plotMedal(plotchoi,medalQ):\n    plt.subplots(figsize=(8,5))\n    plt.plot(medalQ[medalQ.medal==1].Year,medalQ[medalQ.medal==1][plotchoi],'g*-')\n\n    plt.plot(medalQ[medalQ.medal==0].Year,medalQ[medalQ.medal==0][plotchoi],'rv--')\n\n    plt.legend(['Medal','No medal'])\n    plt.grid(True)\n    plt.xlabel('Year')\n    plt.ylabel(modname(plotchoi))\n\n\nplotchoi='avg_height'\nplotMedal(plotchoi,medalQ)\n\nplotchoi='avg_weight'\nplotMedal(plotchoi,medalQ)\n\nplotchoi='avg_age'\nplotMedal(plotchoi,medalQ)\nplt.ylim([18,40])\n\n(18.0, 40.0)\n\n\n\n\n\n\n\n\n\n\n\n\nAnswer: What is the best weight, height and age?\n\nAge, height and weight of athletes change with year\nAfter the initial years (> ~1930)\n\nAthletes who get more medals have greater height and weight\nWhereas, the age is indistinguishable"
  },
  {
    "objectID": "posts/olympics/2022-07-20-OlympicsSQL-HeightWeightAge.html#the-change-in-athletes-based-on-weight-height-and-age",
    "href": "posts/olympics/2022-07-20-OlympicsSQL-HeightWeightAge.html#the-change-in-athletes-based-on-weight-height-and-age",
    "title": "Olympics data with SQL and pandas- height weight and age",
    "section": "The change in athletes based on weight, height and age",
    "text": "The change in athletes based on weight, height and age\nHere the methodology used to produce the figures in the background section is presented.\nThe method is fairly simple, we just use each athlete table and then GROUP BY year and take the averages\n\ndf_F=\\\n     sqldf('SELECT                              \\\n              Year,                             \\\n               avg(Height) AS avg_height,       \\\n               avg(Weight) AS avg_weight,       \\\n               avg(Age)    AS avg_age           \\\n            FROM                                \\\n                df_F_S AS d                     \\\n            GROUP BY                            \\\n               Year                             \\\n            ORDER BY                            \\\n                Year asc;',locals())\ndf_M=\\\n     sqldf('SELECT                              \\\n              Year,                             \\\n               avg(Height) AS avg_height,       \\\n               avg(Weight) AS avg_weight,       \\\n               avg(Age)    AS avg_age           \\\n            FROM                                \\\n                df_M_S AS d                     \\\n            GROUP BY                            \\\n               Year                             \\\n            ORDER BY                            \\\n                Year asc;',locals())\n               \ndf_Fw=\\\n     sqldf('SELECT                              \\\n              Year,                             \\\n               avg(Height) AS avg_height,       \\\n               avg(Weight) AS avg_weight,       \\\n               avg(Age)    AS avg_age           \\\n            FROM                                \\\n                df_F_W AS d                     \\\n            GROUP BY                            \\\n               Year                             \\\n            ORDER BY                            \\\n                Year asc;',locals())\ndf_Mw=\\\n     sqldf('SELECT                              \\\n              Year,                             \\\n               avg(Height) AS avg_height,       \\\n               avg(Weight) AS avg_weight,       \\\n               avg(Age)    AS avg_age           \\\n            FROM                                \\\n                df_M_W AS d                     \\\n            GROUP BY                            \\\n               Year                             \\\n            ORDER BY                            \\\n                Year asc;',locals())\n               \ndf_F.head()                    \n\n\n\n\n\n  \n    \n      \n      Year\n      avg_height\n      avg_weight\n      avg_age\n    \n  \n  \n    \n      0\n      1900\n      NaN\n      NaN\n      29.791667\n    \n    \n      1\n      1904\n      NaN\n      NaN\n      50.230769\n    \n    \n      2\n      1906\n      NaN\n      NaN\n      23.500000\n    \n    \n      3\n      1908\n      NaN\n      NaN\n      33.897436\n    \n    \n      4\n      1912\n      NaN\n      NaN\n      22.379310\n    \n  \n\n\n\n\n\ndef yrplot(df_F,df_M,df_Fw,df_Mw,whatplot= 'avg_weight'): \n    cola=['r>','b<','mo','cs']\n\n    plt.subplots(figsize=(6,4))\n    plt.plot(df_F.Year,df_F[whatplot],cola[0],markersize=10)\n\n    plt.plot(df_M.Year,df_M[whatplot],cola[1],markersize=10)\n\n    plt.plot(df_Fw.Year,df_Fw[whatplot],cola[2])\n    plt.plot(df_Mw.Year,df_Mw[whatplot],cola[3])\n\n    def doPlot(df_F,avgNo,whatplot,col,lw):\n        bb = df_F.Year.rolling(avgNo).mean()\n        cc = df_F[whatplot]\n        cc = cc.rolling(avgNo).mean()\n        plt.plot(bb,cc,col,linewidth=lw)\n\n    doPlot(df_F,3,whatplot,'r-',4)\n    doPlot(df_M,3,whatplot,'b-',4)\n\n    doPlot(df_Fw,3,whatplot,'m--',2)\n    doPlot(df_Mw,3,whatplot,'c--',2)\n\n    plt.legend(['Female Summer','Male Summer','Female Winter','Male Winter'])\n    plt.ylabel(modname(whatplot),fontsize=14)\n    plt.xlabel('Year',fontsize=14)\n    plt.xlim([1890, 2020])\n    plt.grid(True)\n\n\nyrplot(df_F,df_M,df_Fw,df_Mw,whatplot= 'avg_weight')\nyrplot(df_F,df_M,df_Fw,df_Mw,whatplot= 'avg_height')\nyrplot(df_F,df_M,df_Fw,df_Mw,whatplot= 'avg_age')\nplt.ylim([18,35]);\n\n\n\n\n\n\n\n\n\n\n\nAre the changes due to changes in athletes or changes in the Olympics?\n\nJoin athlete table df_M_S with event table df_event\n\nUse this to get a list of events and the year they occur\n\nGroup this table\n\nTo get which event meet criteria of a minimum date, maximum date and having occured so many times\n\n\n(select\nEvent_id\n……\nAND max(year)>1990) usea\n\nGroup the table of events with the athlete table df_M_S\nTake the average over the events and year\nTake this average just over year\n\nThis stops changes due to changes in number of athletes in a particular event\n\n\nThere are 15-20 events included in the results below. Although, a relatively low figure this still represents a lot of athletes. Furthermore, when we split these events into 2 the same trends we find in all the data are seen in the two splits. The absolute values can differ but the min./max. values seem to fairly consistent. But obviously more exploration would be beneficial if this is led by theories or experts in the areas. Without this we could spend forever looking for trends.\n\n# --find events that have occured more than a set amount within a range of dates\n# -- i.e. events that can focus on to see results of changes with time\n\ndef do_same_event(df_M_S,df_F_S,df_event,athlete_df_name='df_M_S',counta='17',yr_start='1900',yr_end='1990'):\n    tempa= sqldf('                               \\\n    SELECT                                   \\\n        Year,                                \\\n        avg(wgt)            AS avg_weight,   \\\n        avg(hgt)            AS avg_height,   \\\n        avg(aga)            AS avg_age       \\\n    FROM                                     \\\n        (SELECT                              \\\n        Year,                                \\\n        AVG(weight)         AS wgt,          \\\n        AVG(height)         AS hgt,          \\\n        AVG(age)            AS aga           \\\n        FROM                                 \\\n        (SELECT                              \\\n          Event_id                           \\\n        FROM                                 \\\n            (SELECT                          \\\n            E.Event_id,                      \\\n            Year,                            \\\n            count(*)        AS counta        \\\n            FROM                             \\\n                {0}         AS A             \\\n            LEFT JOIN                        \\\n                df_event    AS E             \\\n            ON                               \\\n                E.event_id = A.event_id      \\\n            GROUP BY                         \\\n                E.Event_id,                  \\\n                Year                         \\\n            ORDER BY year asc) AA            \\\n        GROUP BY AA.Event_id                 \\\n        HAVING COUNT(*) >{1}                 \\\n        AND MIN(year)<{2}                    \\\n        AND MAX(year)>{3}                    \\\n        ORDER BY event_id asc                \\\n        LIMIT 80) usea                       \\\n        LEFT JOIN                            \\\n            {0}              AS a            \\\n        ON                                   \\\n            usea.event_id = a.event_id       \\\n        GROUP BY                             \\\n            year,                            \\\n            usea.event_id                    \\\n        ORDER BY year asc) two               \\\n        GROUP BY year;'.format(athlete_df_name,counta,yr_start,yr_end),locals())\n    return tempa\n\n\ntempaM = do_same_event(df_M_S,df_F_S,df_event,'df_M_S')\ntempaF = do_same_event(df_M_S,df_F_S,df_event,'df_F_S',counta='12',yr_start='1945',yr_end='1990')\n\ntempaF.head()\n\n\n\n\n\n  \n    \n      \n      Year\n      avg_weight\n      avg_height\n      avg_age\n    \n  \n  \n    \n      0\n      1900\n      NaN\n      NaN\n      25.250000\n    \n    \n      1\n      1906\n      NaN\n      NaN\n      23.500000\n    \n    \n      2\n      1908\n      NaN\n      NaN\n      31.200000\n    \n    \n      3\n      1912\n      NaN\n      NaN\n      21.759259\n    \n    \n      4\n      1920\n      NaN\n      160.145833\n      21.737132\n    \n  \n\n\n\n\n\ndef modname(string):\n    string=''.join([string[0].upper(),string[1:].lower()])\n    string=string.replace('_',' ')\n    return string\n\ndef yrplot1(df_F,df_M,whatplot): \n    cola=['r>','b<','mo','cs']\n\n    fig,ax=plt.subplots(figsize=(8,5))\n    ax.plot(df_F.Year,df_F[whatplot],cola[0],markersize=10)\n    ax2=ax.twinx()\n    ax2.plot(df_M.Year,df_M[whatplot],cola[1],markersize=10)\n\n#     plt.plot(df_Fw.Year,df_Fw[whatplot],cola[2])\n#     plt.plot(df_Mw.Year,df_Mw[whatplot],cola[3])\n\n\n    def doPlot(df_F,avgNo,whatplot,col,lw,xx):\n        bb = df_F.Year.rolling(avgNo).mean()\n        cc = df_F[whatplot]\n        cc = cc.rolling(avgNo).mean()\n        xx.plot(bb,cc,col,linewidth=lw)\n\n    doPlot(df_F,3,whatplot,'r-',4,ax)\n    doPlot(df_M,3,whatplot,'b-',4,ax2)\n\n#     doPlot(df_Fw,3,whatplot,'m--',2)\n#     doPlot(df_Mw,3,whatplot,'c--',2)\n\n    fig.legend(['Female Summer','-','Male Summer','-'],loc='upper center')#,'Female Winter','Male Winter'])\n    ax.set_ylabel('Female ' + modname(whatplot),fontsize=14)\n    ax2.set_ylabel('Male ' + modname(whatplot),fontsize=14)\n    ax.set_xlabel('Year',fontsize=14)\n    plt.xlim([1890, 2020])\n    ax2.grid(True)\n    \n\n\nyrplot1(tempaF,tempaM,whatplot= 'avg_weight')\nyrplot1(tempaF,tempaM,whatplot= 'avg_height')\nyrplot1(tempaF,tempaM,whatplot= 'avg_age')\n# no limit"
  },
  {
    "objectID": "posts/olympics/2022-07-20-OlympicsSQL-HeightWeightAge.html#the-cold-war",
    "href": "posts/olympics/2022-07-20-OlympicsSQL-HeightWeightAge.html#the-cold-war",
    "title": "Olympics data with SQL and pandas- height weight and age",
    "section": "The Cold War",
    "text": "The Cold War\n\ndef yrplot(df__,whatplot= 'avg_weight'): \n    \n    countries=['EST', 'EUN' ,'ROW', 'USA' ,'WES']\n    \n#     df__.NOCSMALL.unique()\n#     countries=np.sort(countries)\n    print(countries)\n    cola=['>','o','+','*','<']\n    colur=[[1,0.6,.6],[1,0,0],[.5,.5,.5],[0,0,1],[.6,.6,1]]\n#     ['EST' 'EUN' 'ROW' 'USA' 'WES']\n#     'EST','USA','WES','ROW','EUN'\n\n    fig,ax1=plt.subplots(figsize=(8,5))\n    \n    for i,country in enumerate(countries):\n        if country!='ROW':\n            ax1.plot(df__[df__.NOCSMALL==country].Year,\\\n                 df__[df__.NOCSMALL==country][whatplot],\\\n                 marker=cola[i],linestyle='None',color=colur[i]\\\n                 ,markersize=10)\n\n\n    def doPlot(df_F,avgNo,whatplot,country,col,lw,ax1):\n        bb = df_F[df__.NOCSMALL==country].Year.rolling(avgNo).mean()\n        cc = df_F[df__.NOCSMALL==country][whatplot]\n        cc = cc.rolling(avgNo).mean()\n        ax1.plot(bb,cc,linewidth=lw,color=col)\n        return ax1\n\n    for i,country in enumerate(countries):\n        if country!='ROW':\n            ax1=doPlot(df__,avgNo=3,whatplot=whatplot,country=country,col=colur[i],lw=3,ax1=ax1)\n    \n    lega = ['East Europe','Russia','USA','West Europe']\n    plt.legend(lega)\n    plt.grid(True)\n    plt.ylabel(modname(whatplot))\n    \n    return ax1\n\n\ndef get_df_USA_USSR(df_M_S,df_F_S,nameDF):\n    USA_USSR=sqldf(\\\n           'SELECT                            \\\n              Year,                           \\\n              NOCSMALL,                       \\\n              AVG(avg_height)  AS avg_height, \\\n              AVG(avg_weight)  AS avg_weight, \\\n              AVG(avg_age)     AS avg_age,    \\\n              SUM(number_of_athletes) AS number_of_athletes         \\\n           FROM                                    \\\n               (SELECT                             \\\n                   Year,                           \\\n                   AVG(avg_height)  AS avg_height, \\\n                   AVG(avg_weight)  AS avg_weight, \\\n                   AVG(avg_age)     AS avg_age,    \\\n                   SUM(num_ath) AS number_of_athletes,        \\\n                   CASE                                \\\n                       WHEN NOC IN (\"FRA\",\"ESP\",\"ITA\",\"POR\",\"GBR\",\"IRL\",\"NED\",\"BEL\",\"DEN\",\"SUI\") THEN \"WES\"\\\n                       WHEN NOC IN (\"POL\",\"ROU\",\"UKR\",\"LAT\",\"BUL\",\"HUN\",\"LTU\",\"LAT\",\"BLR\",\"ALB\",\"SVK\",\"AUT\",\"EST\",\"BIH\",\"BOH\") THEN \"EST\"\\\n                       WHEN NOC=\"USA\" THEN \"USA\"       \\\n                       WHEN NOC=\"EUN\" THEN \"EUN\"       \\\n                       ELSE \"ROW\"\\\n                   END AS NOCSMALL,                     \\\n                   NOC\\\n               FROM                               \\\n                  (SELECT                         \\\n                  Year,                           \\\n                  NOC,                            \\\n                  avg(Height) AS avg_height,      \\\n                  avg(Weight) AS avg_weight,      \\\n                  avg(Age)    AS avg_age,         \\\n                  count(*)    AS num_ath          \\\n                  FROM                            \\\n                    {} AS d                       \\\n                  GROUP BY                        \\\n                    Year,NOC                      \\\n                  ORDER BY                        \\\n                    Year asc) A                   \\\n              GROUP BY                            \\\n                Year,NOC) B                       \\\n           GROUP BY Year, NOCSMALL ;'.format(nameDF),locals())\n    return USA_USSR\n\n\ndef do_USA_USSR(df_M_S,df_F_S,men_women,whatplot):\n    if men_women=='men':\n        nameDF='df_M_S'\n    elif men_women=='women':\n        nameDF='df_F_S'\n    \n        \n    USA_USSR_F = get_df_USA_USSR(df_M_S,df_F_S,nameDF)\n    ax1=yrplot(USA_USSR_F,whatplot)\n    \n    return ax1\n    \n    \n\n\ndo_USA_USSR(df_M_S,df_F_S,'men','avg_weight')\nplt.ylim([65, 90])\n\n['EST', 'EUN', 'ROW', 'USA', 'WES']\n\n\n(65.0, 90.0)\n\n\n\n\n\n\ndo_USA_USSR(df_M_S,df_F_S,'women','avg_weight')\nplt.ylim([50, 70])\n\n['EST', 'EUN', 'ROW', 'USA', 'WES']\n\n\n(50.0, 70.0)\n\n\n\n\n\n\ndo_USA_USSR(df_M_S,df_F_S,'men','avg_height')\n\n['EST', 'EUN', 'ROW', 'USA', 'WES']\n\n\n<AxesSubplot:ylabel='Avg height'>\n\n\n\n\n\n\ndo_USA_USSR(df_M_S,df_F_S,'women','avg_height')\n# plt.ylim([65, 90])\n\n['EST', 'EUN', 'ROW', 'USA', 'WES']\n\n\n<AxesSubplot:ylabel='Avg height'>\n\n\n\n\n\n\ndo_USA_USSR(df_M_S,df_F_S,'men','avg_age')\n# plt.ylim([65, 90])\n\n['EST', 'EUN', 'ROW', 'USA', 'WES']\n\n\n<AxesSubplot:ylabel='Avg age'>\n\n\n\n\n\n\ndo_USA_USSR(df_M_S,df_F_S,'women','avg_age')\nplt.ylim([18, 35])\n\n['EST', 'EUN', 'ROW', 'USA', 'WES']\n\n\n(18.0, 35.0)\n\n\n\n\n\n\ndef number_of_athletes_USA_USSR(df_F_S,df_M_S):\n    testa2=sqldf('\\\n        SELECT                                 \\\n        Year,                              \\\n        NOCSMALL,                          \\\n        count(*) AS number_of_athletes,     \\\n        \"F\" AS Sex                          \\\n    FROM                                   \\\n         (SELECT                           \\\n         athlete_ID,                        \\\n         Year,                             \\\n         CASE                                \\\n            WHEN NOC IN (\"FRA\",\"ESP\",\"ITA\",\"POR\",\"GBR\",\"IRL\",\"NED\",\"BEL\",\"DEN\",\"SUI\") THEN \"WES\"\\\n            WHEN NOC IN (\"POL\",\"ROU\",\"UKR\",\"LAT\",\"BUL\",\"HUN\",\"LTU\",\"LAT\",\"BLR\",\"ALB\",\"SVK\",\"AUT\",\"EST\",\"BIH\",\"BOH\") THEN \"EST\"\\\n            WHEN NOC=\"USA\" THEN \"USA\"       \\\n            WHEN NOC=\"EUN\" THEN \"EUN\"       \\\n            ELSE \"ROW\"\\\n            END AS NOCSMALL                \\\n         from df_F_S                       \\\n         group by athlete_ID,Year               \\\n         order by Year asc) A              \\\n     GROUP BY Year, NOCSMALL               \\\n     UNION ALL                                 \\\n     SELECT                                \\\n        Year,                              \\\n        NOCSMALL,                          \\\n        count(*) AS number_of_athletes,     \\\n        \"M\" AS Sex                          \\\n    FROM                                   \\\n         (SELECT                           \\\n         athlete_ID,                        \\\n         Year,                             \\\n         CASE                                \\\n            WHEN NOC IN (\"FRA\",\"ESP\",\"ITA\",\"POR\",\"GBR\",\"IRL\",\"NED\",\"BEL\",\"DEN\",\"SUI\") THEN \"WES\"\\\n            WHEN NOC IN (\"POL\",\"ROU\",\"UKR\",\"LAT\",\"BUL\",\"HUN\",\"LTU\",\"LAT\",\"BLR\",\"ALB\",\"SVK\",\"AUT\",\"EST\",\"BIH\",\"BOH\") THEN \"EST\"\\\n            WHEN NOC=\"USA\" THEN \"USA\"       \\\n            WHEN NOC=\"EUN\" THEN \"EUN\"       \\\n            ELSE \"ROW\"\\\n            END AS NOCSMALL                \\\n         from df_M_S                       \\\n         group by athlete_ID,Year          \\\n         order by Year asc) A              \\\n     GROUP BY Year, NOCSMALL;',locals()  )\n    return testa2\n\ndef number_of_medals_USA_USSR(df_F_S,df_M_S):\n    testa2=sqldf('\\\n        SELECT                                 \\\n            COUNT(*) AS number_of_medals,\\\n            Year, Sex, NOCSMALL\\\n        FROM \\\n        (SELECT NOCSMALL,Year,Sex,COUNT(*) AS counta\\\n        FROM                                   \\\n         (SELECT                           \\\n         athlete_ID,                        \\\n         event_ID,                          \\\n         \"F\"   AS Sex,                      \\\n         Medal_Gold,Medal_Silver,Medal_Bronze,\\\n         Year,                             \\\n         CASE                                \\\n            WHEN NOC IN (\"FRA\",\"ESP\",\"ITA\",\"POR\",\"GBR\",\"IRL\",\"NED\",\"BEL\",\"DEN\",\"SUI\") THEN \"WES\"\\\n            WHEN NOC IN (\"POL\",\"ROU\",\"UKR\",\"LAT\",\"BUL\",\"HUN\",\"LTU\",\"LAT\",\"BLR\",\"ALB\",\"SVK\",\"AUT\",\"EST\",\"BIH\",\"BOH\") THEN \"EST\"\\\n            WHEN NOC=\"USA\" THEN \"USA\"       \\\n            WHEN NOC=\"EUN\" THEN \"EUN\"       \\\n            ELSE \"ROW\"\\\n            END AS NOCSMALL                \\\n         from df_F_S                       \\\n         WHERE Medal_Gold=1 OR Medal_Silver=1 OR Medal_Bronze=1\\\n         UNION ALL                                 \\\n         SELECT                           \\\n         athlete_ID,                        \\\n         event_ID,                          \\\n         \"M\" AS Sex,                       \\\n         Medal_Gold,Medal_Silver,Medal_Bronze,\\\n         Year,                             \\\n         CASE                                \\\n            WHEN NOC IN (\"FRA\",\"ESP\",\"ITA\",\"POR\",\"GBR\",\"IRL\",\"NED\",\"BEL\",\"DEN\",\"SUI\") THEN \"WES\"\\\n            WHEN NOC IN (\"POL\",\"ROU\",\"UKR\",\"LAT\",\"BUL\",\"HUN\",\"LTU\",\"LAT\",\"BLR\",\"ALB\",\"SVK\",\"AUT\",\"EST\",\"BIH\",\"BOH\") THEN \"EST\"\\\n            WHEN NOC=\"USA\" THEN \"USA\"       \\\n            WHEN NOC=\"EUN\" THEN \"EUN\"       \\\n            ELSE \"ROW\"\\\n            END AS NOCSMALL                \\\n         from df_M_S                       \\\n         WHERE Medal_Gold=1 OR Medal_Silver=1 OR Medal_Bronze=1\\\n         order by Year asc) A\\\n     GROUP BY \\\n         Year, NOCSMALL,event_id,Medal_Gold,Medal_Silver,Medal_Bronze)  AS B\\\n GROUP BY Year, NOCSMALL, Sex\\\n                 ;',locals()  )                                       \n    return testa2\n\n\nUSA_USSR_medals=number_of_medals_USA_USSR(df_F_S,df_M_S)\nUSA_USSR_athletes=number_of_athletes_USA_USSR(df_F_S,df_M_S)\n\n\nUSA_USSR_medals.head()\n\n\n\n\n\n  \n    \n      \n      number_of_medals\n      Year\n      Sex\n      NOCSMALL\n    \n  \n  \n    \n      0\n      11\n      1896\n      M\n      EST\n    \n    \n      1\n      61\n      1896\n      M\n      ROW\n    \n    \n      2\n      19\n      1896\n      M\n      USA\n    \n    \n      3\n      29\n      1896\n      M\n      WES\n    \n    \n      4\n      2\n      1900\n      F\n      EST\n    \n  \n\n\n\n\n\nyrplot(USA_USSR_medals[USA_USSR_medals.Sex=='F'],whatplot= 'number_of_medals')\n\n['EST', 'EUN', 'ROW', 'USA', 'WES']\n\n\n<AxesSubplot:ylabel='Number of medals'>\n\n\n\n\n\n\nyrplot(USA_USSR_medals[USA_USSR_medals.Sex=='M'],whatplot= 'number_of_medals')\n\n['EST', 'EUN', 'ROW', 'USA', 'WES']\n\n\n<AxesSubplot:ylabel='Number of medals'>\n\n\n\n\n\n\nyrplot(USA_USSR_athletes[USA_USSR_athletes.Sex=='F'],whatplot= 'number_of_athletes')\n\n['EST', 'EUN', 'ROW', 'USA', 'WES']\n\n\n<AxesSubplot:ylabel='Number of athletes'>\n\n\n\n\n\n\nyrplot(USA_USSR_athletes[USA_USSR_athletes.Sex=='M'],whatplot= 'number_of_athletes')\n\n['EST', 'EUN', 'ROW', 'USA', 'WES']\n\n\n<AxesSubplot:ylabel='Number of athletes'>"
  },
  {
    "objectID": "posts/olympics/2022-07-29-OlympicsSQL_createCountryDF.html#overview",
    "href": "posts/olympics/2022-07-29-OlympicsSQL_createCountryDF.html#overview",
    "title": "Olympics data with SQL and pandas- creating a country table",
    "section": "Overview",
    "text": "Overview\nThe country table needed extra analysis so I seperated it from the rest of the analysis.\nIt also requires importing some new data, which I will add here too\n\nimport pandas as pd\nfrom pandasql import sqldf\nimport matplotlib.pyplot as plt\nimport re \n\n\ndf= pd.read_csv(\"athlete_events.csv\")\ndf2=pd.read_csv(\"noc_regions.csv\")\n\nThe next line is just to add a unique ID for athletes when the data is split up later\n\ndf= df.reset_index()\ndf.rename(columns={'index':'event_athlete_ID','ID':'athlete_ID'},inplace=True)\n\n\ndf.head()\n\n\n\n\n\n  \n    \n      \n      event_athlete_ID\n      athlete_ID\n      Name\n      Sex\n      Age\n      Height\n      Weight\n      Team\n      NOC\n      Games\n      Year\n      Season\n      City\n      Sport\n      Event\n      Medal\n    \n  \n  \n    \n      0\n      0\n      1\n      A Dijiang\n      M\n      24.0\n      180.0\n      80.0\n      China\n      CHN\n      1992 Summer\n      1992\n      Summer\n      Barcelona\n      Basketball\n      Basketball Men's Basketball\n      NaN\n    \n    \n      1\n      1\n      2\n      A Lamusi\n      M\n      23.0\n      170.0\n      60.0\n      China\n      CHN\n      2012 Summer\n      2012\n      Summer\n      London\n      Judo\n      Judo Men's Extra-Lightweight\n      NaN\n    \n    \n      2\n      2\n      3\n      Gunnar Nielsen Aaby\n      M\n      24.0\n      NaN\n      NaN\n      Denmark\n      DEN\n      1920 Summer\n      1920\n      Summer\n      Antwerpen\n      Football\n      Football Men's Football\n      NaN\n    \n    \n      3\n      3\n      4\n      Edgar Lindenau Aabye\n      M\n      34.0\n      NaN\n      NaN\n      Denmark/Sweden\n      DEN\n      1900 Summer\n      1900\n      Summer\n      Paris\n      Tug-Of-War\n      Tug-Of-War Men's Tug-Of-War\n      Gold\n    \n    \n      4\n      4\n      5\n      Christine Jacoba Aaftink\n      F\n      21.0\n      185.0\n      82.0\n      Netherlands\n      NED\n      1988 Winter\n      1988\n      Winter\n      Calgary\n      Speed Skating\n      Speed Skating Women's 500 metres\n      NaN\n    \n  \n\n\n\n\n\ndf2.head()\n\n\n\n\n\n  \n    \n      \n      NOC\n      region\n      notes\n    \n  \n  \n    \n      0\n      AFG\n      Afghanistan\n      NaN\n    \n    \n      1\n      AHO\n      Curacao\n      Netherlands Antilles\n    \n    \n      2\n      ALB\n      Albania\n      NaN\n    \n    \n      3\n      ALG\n      Algeria\n      NaN\n    \n    \n      4\n      AND\n      Andorra\n      NaN\n    \n  \n\n\n\n\n\nWhat do we use to value to identify a nation\n\nprint('There are {} unique teams and {} unique NOCs in df.\\n     \\\nAnd {} unique NOC values, {} unique regions and {} unique notes in df2.'.format( \\\n    len(pd.unique(df.Team)),len(pd.unique(df.NOC)), \n    len(pd.unique(df2.region)),len(pd.unique(df2.region)),len(pd.unique(df2.notes)) ))\n\n\nThere are 1184 unique teams and 230 unique NOCs in df.\n     And 207 unique NOC values, 207 unique regions and 22 unique notes in df2.\n\n\n1184 Seems a lot of teams to consider, it may be best to stick with using NOC as a unique identifier for a country. We can then probably use regions as the name of the country.\nLet’s have a look at the values of teams,NOC and regions\n\nsqldf(\"SELECT                                \\\n         NOC,                                \\\n         Team,                               \\\n         count(*)                            \\\n       FROM                                  \\\n         df                                  \\\n       GROUP BY                              \\\n         Team, NOC                           \\\n       ORDER BY team DESC                    \\\n       LIMIT 30;\",locals())\n\n\n\n\n\n  \n    \n      \n      NOC\n      Team\n      count(*)\n    \n  \n  \n    \n      0\n      FIN\n      rn-2\n      5\n    \n    \n      1\n      BEL\n      Zut\n      3\n    \n    \n      2\n      ZIM\n      Zimbabwe\n      309\n    \n    \n      3\n      GRE\n      Zefyros\n      2\n    \n    \n      4\n      ZAM\n      Zambia\n      183\n    \n    \n      5\n      YUG\n      Yugoslavia-2\n      10\n    \n    \n      6\n      YUG\n      Yugoslavia-1\n      10\n    \n    \n      7\n      YUG\n      Yugoslavia\n      2558\n    \n    \n      8\n      SUI\n      Ylliam VIII\n      5\n    \n    \n      9\n      SUI\n      Ylliam VII\n      6\n    \n    \n      10\n      SUI\n      Ylliam II\n      5\n    \n    \n      11\n      GBR\n      Yeoman XII\n      3\n    \n    \n      12\n      GBR\n      Yeoman VII\n      3\n    \n    \n      13\n      RSA\n      Yeoman V\n      3\n    \n    \n      14\n      BAH\n      Yeoman\n      4\n    \n    \n      15\n      YEM\n      Yemen\n      32\n    \n    \n      16\n      MYA\n      Yangon\n      2\n    \n    \n      17\n      MEX\n      Xolotl\n      3\n    \n    \n      18\n      FIN\n      Xantippa\n      3\n    \n    \n      19\n      GBR\n      Wolseley-Siddeley-1\n      4\n    \n    \n      20\n      CAN\n      Winnipeg Shamrocks-1\n      12\n    \n    \n      21\n      CAN\n      Windor\n      2\n    \n    \n      22\n      NED\n      Willem-Six\n      3\n    \n    \n      23\n      ARG\n      Wiking\n      5\n    \n    \n      24\n      USA\n      Widgeon\n      2\n    \n    \n      25\n      FRA\n      Whitini Star\n      1\n    \n    \n      26\n      DEN\n      White Lady\n      3\n    \n    \n      27\n      JPN\n      Whisper\n      1\n    \n    \n      28\n      CAN\n      Whirlaway\n      2\n    \n    \n      29\n      USA\n      Western Rowing Club-3\n      6\n    \n  \n\n\n\n\nTeam in df seems to not reflect the country very well. e.g. Whisper is not a country but JPN probably represents Japan.\nSo the use of NOC seems to make sense\nNow let us consider the NOC, region and notes variables\n\nsqldf(\"SELECT                                \\\n         NOC,                                \\\n         Region,                             \\\n         Notes,                              \\\n         count(*)                            \\\n       FROM                                  \\\n         df2                                 \\\n       GROUP BY                              \\\n         NOC, Region, Notes                  \\\n       ORDER BY Region DESC                  \\\n       LIMIT 30;\",locals())\n\n\n\n\n\n  \n    \n      \n      NOC\n      region\n      notes\n      count(*)\n    \n  \n  \n    \n      0\n      RHO\n      Zimbabwe\n      None\n      1\n    \n    \n      1\n      ZIM\n      Zimbabwe\n      None\n      1\n    \n    \n      2\n      ZAM\n      Zambia\n      None\n      1\n    \n    \n      3\n      YAR\n      Yemen\n      North Yemen\n      1\n    \n    \n      4\n      YEM\n      Yemen\n      None\n      1\n    \n    \n      5\n      YMD\n      Yemen\n      South Yemen\n      1\n    \n    \n      6\n      ISV\n      Virgin Islands, US\n      Virgin Islands\n      1\n    \n    \n      7\n      IVB\n      Virgin Islands, British\n      None\n      1\n    \n    \n      8\n      VIE\n      Vietnam\n      None\n      1\n    \n    \n      9\n      VNM\n      Vietnam\n      None\n      1\n    \n    \n      10\n      VEN\n      Venezuela\n      None\n      1\n    \n    \n      11\n      VAN\n      Vanuatu\n      None\n      1\n    \n    \n      12\n      UZB\n      Uzbekistan\n      None\n      1\n    \n    \n      13\n      URU\n      Uruguay\n      None\n      1\n    \n    \n      14\n      UAE\n      United Arab Emirates\n      None\n      1\n    \n    \n      15\n      UKR\n      Ukraine\n      None\n      1\n    \n    \n      16\n      UGA\n      Uganda\n      None\n      1\n    \n    \n      17\n      USA\n      USA\n      None\n      1\n    \n    \n      18\n      GBR\n      UK\n      None\n      1\n    \n    \n      19\n      TKM\n      Turkmenistan\n      None\n      1\n    \n    \n      20\n      TUR\n      Turkey\n      None\n      1\n    \n    \n      21\n      TUN\n      Tunisia\n      None\n      1\n    \n    \n      22\n      TTO\n      Trinidad\n      Trinidad and Tobago\n      1\n    \n    \n      23\n      WIF\n      Trinidad\n      West Indies Federation\n      1\n    \n    \n      24\n      TGA\n      Tonga\n      None\n      1\n    \n    \n      25\n      TOG\n      Togo\n      None\n      1\n    \n    \n      26\n      TLS\n      Timor-Leste\n      None\n      1\n    \n    \n      27\n      THA\n      Thailand\n      None\n      1\n    \n    \n      28\n      TAN\n      Tanzania\n      None\n      1\n    \n    \n      29\n      TJK\n      Tajikistan\n      None\n      1\n    \n  \n\n\n\n\nNOC doesn’t look unique enough for us. For example, - Zimbabwe is RHO and ZIM and this probably just reflects a name change in the country - Yemen has 3 NOC values, presumably reflecting the unification that took place in 1990. Most of these differences are not that important to what I am looking at, where it would be better to have a broader description of a nation\nSo what I will do is - use the region tag as a unique identifier of a country - replace multiple NOC values of a single country with a single NOC value\nThe first step is to identify which regions have multiple NOC values. We can do this again in SQL by creating a new table\n\nd1=sqldf(\"SELECT                               \\\n            NOC,                               \\\n            region,                            \\\n            notes,                             \\\n            count(*)                           \\\n         FROM                                  \\\n            df2                                \\\n         GROUP BY                              \\\n            region                             \\\n         HAVING COUNT(*)>1                     \\\n         ORDER BY count(*) DESC;\",locals())\nd1\n\n\n\n\n\n  \n    \n      \n      NOC\n      region\n      notes\n      count(*)\n    \n  \n  \n    \n      0\n      FRG\n      Germany\n      None\n      4\n    \n    \n      1\n      YAR\n      Yemen\n      North Yemen\n      3\n    \n    \n      2\n      SCG\n      Serbia\n      Serbia and Montenegro\n      3\n    \n    \n      3\n      EUN\n      Russia\n      None\n      3\n    \n    \n      4\n      MAL\n      Malaysia\n      None\n      3\n    \n    \n      5\n      BOH\n      Czech Republic\n      Bohemia\n      3\n    \n    \n      6\n      ROT\n      None\n      Refugee Olympic Team\n      3\n    \n    \n      7\n      RHO\n      Zimbabwe\n      None\n      2\n    \n    \n      8\n      VIE\n      Vietnam\n      None\n      2\n    \n    \n      9\n      TTO\n      Trinidad\n      Trinidad and Tobago\n      2\n    \n    \n      10\n      SYR\n      Syria\n      None\n      2\n    \n    \n      11\n      CRT\n      Greece\n      Crete\n      2\n    \n    \n      12\n      CHN\n      China\n      None\n      2\n    \n    \n      13\n      CAN\n      Canada\n      None\n      2\n    \n    \n      14\n      ANZ\n      Australia\n      Australasia\n      2\n    \n  \n\n\n\n\nWe can then create a table to get - the country, - the new NOC value for each country - all the NOC values that correspond to that country\n\nd2=sqldf(\"SELECT                            \\\n            d1.NOC      AS new_NOC,         \\\n            df2.NOC     AS orig_NOC,        \\\n            df2.region,                     \\\n            df2.notes                       \\\n          FROM                              \\\n            d1                              \\\n          LEFT JOIN                         \\\n            df2                             \\\n          ON                                \\\n            d1.region=df2.region            \\\n          ORDER BY                          \\\n            df2.region DESC;\",locals())\nd2\n\n\n\n\n\n  \n    \n      \n      new_NOC\n      orig_NOC\n      region\n      notes\n    \n  \n  \n    \n      0\n      RHO\n      RHO\n      Zimbabwe\n      None\n    \n    \n      1\n      RHO\n      ZIM\n      Zimbabwe\n      None\n    \n    \n      2\n      YAR\n      YAR\n      Yemen\n      North Yemen\n    \n    \n      3\n      YAR\n      YEM\n      Yemen\n      None\n    \n    \n      4\n      YAR\n      YMD\n      Yemen\n      South Yemen\n    \n    \n      5\n      VIE\n      VIE\n      Vietnam\n      None\n    \n    \n      6\n      VIE\n      VNM\n      Vietnam\n      None\n    \n    \n      7\n      TTO\n      TTO\n      Trinidad\n      Trinidad and Tobago\n    \n    \n      8\n      TTO\n      WIF\n      Trinidad\n      West Indies Federation\n    \n    \n      9\n      SYR\n      SYR\n      Syria\n      None\n    \n    \n      10\n      SYR\n      UAR\n      Syria\n      United Arab Republic\n    \n    \n      11\n      SCG\n      SCG\n      Serbia\n      Serbia and Montenegro\n    \n    \n      12\n      SCG\n      SRB\n      Serbia\n      None\n    \n    \n      13\n      SCG\n      YUG\n      Serbia\n      Yugoslavia\n    \n    \n      14\n      EUN\n      EUN\n      Russia\n      None\n    \n    \n      15\n      EUN\n      RUS\n      Russia\n      None\n    \n    \n      16\n      EUN\n      URS\n      Russia\n      None\n    \n    \n      17\n      MAL\n      MAL\n      Malaysia\n      None\n    \n    \n      18\n      MAL\n      MAS\n      Malaysia\n      None\n    \n    \n      19\n      MAL\n      NBO\n      Malaysia\n      North Borneo\n    \n    \n      20\n      CRT\n      CRT\n      Greece\n      Crete\n    \n    \n      21\n      CRT\n      GRE\n      Greece\n      None\n    \n    \n      22\n      FRG\n      FRG\n      Germany\n      None\n    \n    \n      23\n      FRG\n      GDR\n      Germany\n      None\n    \n    \n      24\n      FRG\n      GER\n      Germany\n      None\n    \n    \n      25\n      FRG\n      SAA\n      Germany\n      None\n    \n    \n      26\n      BOH\n      BOH\n      Czech Republic\n      Bohemia\n    \n    \n      27\n      BOH\n      CZE\n      Czech Republic\n      None\n    \n    \n      28\n      BOH\n      TCH\n      Czech Republic\n      None\n    \n    \n      29\n      CHN\n      CHN\n      China\n      None\n    \n    \n      30\n      CHN\n      HKG\n      China\n      Hong Kong\n    \n    \n      31\n      CAN\n      CAN\n      Canada\n      None\n    \n    \n      32\n      CAN\n      NFL\n      Canada\n      Newfoundland\n    \n    \n      33\n      ANZ\n      ANZ\n      Australia\n      Australasia\n    \n    \n      34\n      ANZ\n      AUS\n      Australia\n      None\n    \n    \n      35\n      ROT\n      None\n      None\n      None\n    \n  \n\n\n\n\nAnd finally replace the values of NOC in df and df2 with the new values for countries with duplicate values\nEasier to do this with Python\n\nfor i,old_NOC in enumerate(d2.orig_NOC):\n    df.loc[df.NOC==old_NOC,'NOC']=d2.loc[i,'new_NOC']\n    df2.loc[df2.NOC==old_NOC,'NOC']=d2.loc[i,'new_NOC']\n    \n\n\n\nGDP data\nWikipedia was used to get data on population and GDP of different countries. The data was imported using Excel’s capability to give it a url to obtain the table, and saved as different tabs in the file CountryData.xlsx. For GDP I selected the World Bank Estimate.\n\nGDP data\nPopulation data\n\nWebsite urls correct as of 23/7/22\n\ndf_GDP = pd.read_excel('CountryData.xlsx',sheet_name=2)\n\ndf_GDP=df_GDP.drop(columns=['IMF[1][12] Estimate','IMF[1][12] Year','United Nations[13] Estimate','United Nations[13] Year','World Bank[14][15] Year'])\ndf_GDP=df_GDP.rename(columns={'World Bank[14][15] Estimate':'GDP','Country/Territory':'Country','UN Region':'Continent'},errors='raise')\n\ndf_GDP.head(10)\n\n\n\n\n\n  \n    \n      \n      Country\n      Continent\n      GDP\n    \n  \n  \n    \n      0\n      World\n      -\n      84,705,567\n    \n    \n      1\n      United States\n      Americas\n      20,936,600\n    \n    \n      2\n      China\n      Asia\n      14,722,731\n    \n    \n      3\n      Japan\n      Asia\n      4,975,415\n    \n    \n      4\n      Germany\n      Europe\n      3,806,060\n    \n    \n      5\n      India\n      Asia\n      2,622,984\n    \n    \n      6\n      United Kingdom\n      Europe\n      2,707,744\n    \n    \n      7\n      France\n      Europe\n      2,603,004\n    \n    \n      8\n      Canada\n      Americas\n      1,643,408\n    \n    \n      9\n      Italy\n      Europe\n      1,886,445\n    \n  \n\n\n\n\nmake column GDP an integer, and remove the comma\n\ndf_GDP['GDP']=[x.replace(',','') for x in df_GDP['GDP']]\n\ndf_GDP['GDP']=pd.to_numeric(df_GDP['GDP'],errors='coerce').fillna(0).astype('int')\n\nWe now need to match the names of countries from the data we have found on the internet with the Olympics data.\nA good example of this is United Kingdom, which can be named several ways (with slightly different meanings) including: UK, Great Britain, Great Britain, Great Britain and Northern Ireland, GB.\nThe function changeDF_country takes the imported dataframes and df2 as inputs and outputs the imported dataframes with corrected NOC and country values\n\nChecks whether country in imported df matches region in df\n\n\nIf so country doesn’t need to be changed and can provide matching NOC value\n\n\nIf not above then see if df2.notes matches the country\n\n\nIf so can return country and NOC matching the notes that match\n\n\nIf not aboves, try some name changes, like United States to USA\n\n\nThen return the matching NOC and country and NOC matching values\n\n\nNone of the above\n\n\nThen return country given and give NOC a name to show there was no match.\nPrint out the name of the country where no match was found. To see if missing anything big or obvious\n\n\ndef getNation(region_to_check):\n       \n    if region_to_check=='United States':\n        region_out = 'USA'\n    elif bool(re.search(r'Germany', region_to_check)):\n        region_out='Germany'\n    elif region_to_check=='United Kingdom':\n        region_out = 'UK'\n    elif region_to_check=='Soviet Union':\n        region_out='Russia'\n    else:\n        region_out=region_to_check\n        print('nothing found for {}'.format(region_out))\n    \n    return region_out\n\ndef changeDF_country(df__,df2):\n    xALL,nocALL=[],[]\n    for i in range(len(df__)):\n        country_check = df__.loc[i,'Country']\n#         print(i,country_check)\n        boolCountry=df2.region==country_check\n        x=df2[boolCountry].region\n        \n        try:\n            x=str(x.iloc[0])\n            df2.loc[boolCountry,'region']=x\n        except:\n            if len(x)<1:\n                boolCountry=df2.notes==df__.loc[i,'Country']\n                x=df2[boolCountry].region\n                try:\n                    x=str(x.iloc[0])\n                    df2.loc[boolCountry,'region']=x\n                except:\n                    if len(x)<1:\n                        country_check=getNation(df__.loc[i,'Country'])\n                        boolCountry=df2.region==country_check\n                        x=df2[boolCountry].region\n                        try:\n                            x=str(x.iloc[0])\n                            df2.loc[boolCountry,'region']=x\n                        except:\n                            x=country_check\n        xALL.append(x)\n        try:\n            nocALL.append(df2[boolCountry].NOC.iloc[0])\n        except:\n            nocALL.append('---')\n            \n#         print('x= ',x)\n#         print('-------------------')\n#         if i==42:\n#             break\n#     print(nocALL)\n    try:\n        df__.insert(1,'NOC',nocALL)\n        df__.insert(1,'Nation',xALL)#,'Nation',xALL})\n    except:\n        print('done allready')\n    return df__\n        \n\n\ndf_GDP=changeDF_country(df_GDP,df2)\n\nnothing found for World\nnothing found for DR Congo\nnothing found for Bolivia\nnothing found for Macau\nnothing found for Congo\nnothing found for North Macedonia\nnothing found for New Caledonia\nnothing found for French Polynesia\nnothing found for Eswatini\nnothing found for Greenland\nnothing found for Curaçao\nnothing found for East Timor\nnothing found for Zanzibar\nnothing found for British Virgin Islands\nnothing found for Northern Mariana Islands\nnothing found for Saint Kitts and Nevis\nnothing found for Saint Vincent and the Grenadines\nnothing found for Sint Maarten\nnothing found for São Tomé and Príncipe\nnothing found for Anguilla\nnothing found for Montserrat\n\n\n\n\nPopulation table\nThe same thing as above but for the population data\n\ndf_population = pd.read_excel('CountryData.xlsx',sheet_name=0)\ndf_population.head(5)\n\n\n\n\n\n  \n    \n      \n      Rank\n      Country / Dependency\n      UN Region\n      Population\n      Percentage of the world\n      Date\n      Source (official or from the United Nations)\n      Notes\n      Column9\n    \n  \n  \n    \n      0\n      –\n      World\n      NaN\n      7965207000\n      1.0000\n      2022-07-20\n      UN projection[2]\n      NaN\n      NaN\n    \n    \n      1\n      1\n      China\n      Asia\n      1412600000\n      0.1770\n      2021-12-31\n      National annual estimate\n      The population figure refers to mainland China...\n      NaN\n    \n    \n      2\n      2\n      India\n      Asia\n      1373761000\n      0.1720\n      2022-03-01\n      Annual national estimate\n      The figure includes the population of Indian-a...\n      NaN\n    \n    \n      3\n      3\n      United States\n      Americas\n      332906919\n      0.0418\n      2022-07-20\n      National population clock\n      The figure includes the 50 states and the Dist...\n      NaN\n    \n    \n      4\n      4\n      Indonesia\n      Asia\n      272248500\n      0.0342\n      2021-07-01\n      National annual estimate\n      NaN\n      NaN\n    \n  \n\n\n\n\n\n\ndf_population.drop(columns=['Percentage of the world','Date','Source (official or from the United Nations)','Column9','Notes'],inplace=True)\ndf_population=df_population.rename(columns={'Country / Dependency':'Country','UN Region':'Continent'},errors='raise')\n\ndf_population\n\n\n\n\n\n  \n    \n      \n      Rank\n      Country\n      Continent\n      Population\n    \n  \n  \n    \n      0\n      –\n      World\n      NaN\n      7965207000\n    \n    \n      1\n      1\n      China\n      Asia\n      1412600000\n    \n    \n      2\n      2\n      India\n      Asia\n      1373761000\n    \n    \n      3\n      3\n      United States\n      Americas\n      332906919\n    \n    \n      4\n      4\n      Indonesia\n      Asia\n      272248500\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      237\n      –\n      Niue\n      Oceania\n      1549\n    \n    \n      238\n      –\n      Tokelau (New Zealand)\n      Oceania\n      1501\n    \n    \n      239\n      195\n      Vatican City\n      Europe\n      825\n    \n    \n      240\n      –\n      Cocos (Keeling) Islands (Australia)\n      Oceania\n      573\n    \n    \n      241\n      –\n      Pitcairn Islands (United Kingdom)\n      Oceania\n      40\n    \n  \n\n242 rows × 4 columns\n\n\n\n\ndf_population=changeDF_country(df_population,df2)\n\nnothing found for World\nnothing found for DR Congo\nnothing found for Bolivia\nnothing found for Hong Kong (China)\nnothing found for Congo\nnothing found for Puerto Rico (United States)\nnothing found for North Macedonia\nnothing found for East Timor\nnothing found for Eswatini\nnothing found for Macau (China)\nnothing found for Western Sahara\nnothing found for Northern Cyprus\nnothing found for Transnistria\nnothing found for French Polynesia (France)\nnothing found for New Caledonia (France)\nnothing found for Abkhazia\nnothing found for São Tomé and Príncipe\nnothing found for Guam (United States)\nnothing found for Curaçao (Netherlands)\nnothing found for Artsakh\nnothing found for Aruba (Netherlands)\nnothing found for Saint Vincent and the Grenadines\nnothing found for Jersey (British Crown Dependency)\nnothing found for U.S. Virgin Islands (United States)\nnothing found for Isle of Man (British Crown Dependency)\nnothing found for Cayman Islands (United Kingdom)\nnothing found for Bermuda (United Kingdom)\nnothing found for Guernsey (British Crown Dependency)\nnothing found for Greenland (Denmark)\nnothing found for Saint Kitts and Nevis\nnothing found for Faroe Islands (Denmark)\nnothing found for South Ossetia\nnothing found for American Samoa (United States)\nnothing found for Northern Mariana Islands (United States)\nnothing found for Turks and Caicos Islands (United Kingdom)\nnothing found for Sint Maarten (Netherlands)\nnothing found for Gibraltar (United Kingdom)\nnothing found for Saint Martin (France)\nnothing found for Åland (Finland)\nnothing found for British Virgin Islands (United Kingdom)\nnothing found for Anguilla (United Kingdom)\nnothing found for Wallis and Futuna (France)\nnothing found for Saint Barthélemy (France)\nnothing found for Saint Helena, Ascension and Tristan da Cunha (United Kingdom)\nnothing found for Saint Pierre and Miquelon (France)\nnothing found for Montserrat (United Kingdom)\nnothing found for Falkland Islands (United Kingdom)\nnothing found for Christmas Island (Australia)\nnothing found for Norfolk Island (Australia)\nnothing found for Niue\nnothing found for Tokelau (New Zealand)\nnothing found for Vatican City\nnothing found for Cocos (Keeling) Islands (Australia)\nnothing found for Pitcairn Islands (United Kingdom)\n\n\n\n\nCombine everything for one country table\nNow we merge all data sets together. I’ll use pandas but this is the same as a SQL left outer join. Because we want to keep all the NOC values in df2 we start with that then join each one after\n\ndf_country=[]\ndf_country=df2.merge(df_population,left_on='NOC',right_on='NOC')\n\ndf_country=df_country.merge(df_GDP,left_on='Nation',right_on='Nation')\ndf_country.drop(columns=['Continent_y','Country_x','Country_y','NOC_y','Rank','notes','region'],inplace=True)\ndf_country=df_country.rename(columns={'NOC_x':'NOC','Continent_x':'Continent'},errors='raise')\n\n# We have multiple versions of Nation so we just take the max one\ndf_country = df_country.groupby('NOC').max()\n\n\ndf_country.to_csv('country')\n\ndf.to_csv(\"athlete_events.csv\")\ndf2.to_csv(\"noc_regions.csv\")"
  },
  {
    "objectID": "posts/olympics/2022-07-29-OlympicsSQL_createDFs.html#introduction",
    "href": "posts/olympics/2022-07-29-OlympicsSQL_createDFs.html#introduction",
    "title": "Olympics data with SQL and pandas- create the tables",
    "section": "Introduction",
    "text": "Introduction\nTwo csv files (representing two different tables) were imported to databricks.\nThe main table (athlete_events) consists of 270,000 rows, whereas the unique names in the table are 135,000, or around half the total.\n\nLots of columns and lots that are objects,\n\nso we want to refine this by reducing columns and making it an integer or something smaller than object if possible\n\nThere are some NaN values, particularly for height/weight at earlier games and also for medals\nAn athlete can be represented in several rows if they do multiple events or at different games (e.g. Christine Jacoba Aaftink). So we may want a seperate ID that incorporate the athlete and the event/games that is unique\nThe TEAM, NOC we only want one identifier and a seperate table for countries\n\nThe first step was to split the table up. - First the users are split up based on whether they are male or female and whether they are in the summer or winter games. So split into 4. - Secondly not all data is needed for these athletes table, so instead of 15 columns this is reduced to 9 - Thirdly, the size of these athlete table is reduced by replacing several variables from string to int to reduce the size. Since for example, there is only a limited number of events.\nAn entity relationship diagram (ERD) of the tables described above was developed as shown below.\nThose highlighted in blue and light blue would require additional data, the darkness of blue representing how much new data is needed.\nLucid Chart was used to produce the ERD\n\nN.B. Most of these are steps not really necessary for this dataset, but I wanted to practice SQL (and pandas). If this was a real world problem I would weigh up the benefits of the splitting in terms of my time and computation to see if it was really necessary.\n\nCreating a country table\nIn a separate page (https://thomashsimm.com/sql/pandas/python/olympics/2022/07/29/OlympicsSQL_createCountryDF.html) I show how I created the country table.\nI also made some slight changes to the two main DataFrames df2 and df. Basically just to change the country label and add unique athlete and athlete + event ids\ndf= df.reset_index()\ndf.rename(columns={'index':'event_athlete_ID','ID':'athlete_ID'},inplace=True)\nThe main part is to get rid of some duplicate NOC values, mostly correct but will not work in some regards e.g. China and Hong Kong.\n\n# This gets region (or countries) which are repeated with different NOC values\nd1=sqldf(\"SELECT                              \\\n       NOC, region,notes,count(*)             \\\n       FROM df2                               \\\n       GROUP BY region                        \\\n       HAVING COUNT(*)>1                      \\\n       ORDER BY count(*) DESC;\",locals())\n\n# this then creates a table with one NOC per region and the original NOC values\n# we'll use the new NOC (one per region) a the new index\nd2=sqldf(\"SELECT                              \\\n       d1.NOC as new_NOC,df2.NOC orig_NOC,df2.region,df2.notes             \\\n       FROM d1                                \\\n       LEFT JOIN df2                          \\\n       ON d1.region=df2.region                \\\n      ORDER BY df2.region DESC;\",locals())\n\n\n# then replace the regions with several NOC values with the new one\nfor i,old_NOC in enumerate(d2.orig_NOC):\n    df.loc[df.NOC==old_NOC,'NOC']=d2.loc[i,'new_NOC']\n    df2.loc[df2.NOC==old_NOC,'NOC']=d2.loc[i,'new_NOC']\n\n\ndf= pd.read_csv(\"athlete_events.csv\")\ntry:\n    df.drop(columns='Unnamed: 0',inplace=True)\nexcept:\n    pass\ndf2=pd.read_csv(\"noc_regions.csv\")"
  },
  {
    "objectID": "posts/olympics/2022-07-29-OlympicsSQL_createDFs.html#look-at-the-data",
    "href": "posts/olympics/2022-07-29-OlympicsSQL_createDFs.html#look-at-the-data",
    "title": "Olympics data with SQL and pandas- create the tables",
    "section": "Look at the data",
    "text": "Look at the data\n\n# !pip install pandasql\nimport pandas as pd\nfrom pandasql import sqldf\nimport matplotlib.pyplot as plt\nimport re \n\ndf\n\n\n\n\n\n  \n    \n      \n      event_athlete_ID\n      athlete_ID\n      Name\n      Sex\n      Age\n      Height\n      Weight\n      Team\n      NOC\n      Games\n      Year\n      Season\n      City\n      Sport\n      Event\n      Medal\n    \n  \n  \n    \n      0\n      0\n      1\n      A Dijiang\n      M\n      24.0\n      180.0\n      80.0\n      China\n      CHN\n      1992 Summer\n      1992\n      Summer\n      Barcelona\n      Basketball\n      Basketball Men's Basketball\n      NaN\n    \n    \n      1\n      1\n      2\n      A Lamusi\n      M\n      23.0\n      170.0\n      60.0\n      China\n      CHN\n      2012 Summer\n      2012\n      Summer\n      London\n      Judo\n      Judo Men's Extra-Lightweight\n      NaN\n    \n    \n      2\n      2\n      3\n      Gunnar Nielsen Aaby\n      M\n      24.0\n      NaN\n      NaN\n      Denmark\n      DEN\n      1920 Summer\n      1920\n      Summer\n      Antwerpen\n      Football\n      Football Men's Football\n      NaN\n    \n    \n      3\n      3\n      4\n      Edgar Lindenau Aabye\n      M\n      34.0\n      NaN\n      NaN\n      Denmark/Sweden\n      DEN\n      1900 Summer\n      1900\n      Summer\n      Paris\n      Tug-Of-War\n      Tug-Of-War Men's Tug-Of-War\n      Gold\n    \n    \n      4\n      4\n      5\n      Christine Jacoba Aaftink\n      F\n      21.0\n      185.0\n      82.0\n      Netherlands\n      NED\n      1988 Winter\n      1988\n      Winter\n      Calgary\n      Speed Skating\n      Speed Skating Women's 500 metres\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      271111\n      271111\n      135569\n      Andrzej ya\n      M\n      29.0\n      179.0\n      89.0\n      Poland-1\n      POL\n      1976 Winter\n      1976\n      Winter\n      Innsbruck\n      Luge\n      Luge Mixed (Men)'s Doubles\n      NaN\n    \n    \n      271112\n      271112\n      135570\n      Piotr ya\n      M\n      27.0\n      176.0\n      59.0\n      Poland\n      POL\n      2014 Winter\n      2014\n      Winter\n      Sochi\n      Ski Jumping\n      Ski Jumping Men's Large Hill, Individual\n      NaN\n    \n    \n      271113\n      271113\n      135570\n      Piotr ya\n      M\n      27.0\n      176.0\n      59.0\n      Poland\n      POL\n      2014 Winter\n      2014\n      Winter\n      Sochi\n      Ski Jumping\n      Ski Jumping Men's Large Hill, Team\n      NaN\n    \n    \n      271114\n      271114\n      135571\n      Tomasz Ireneusz ya\n      M\n      30.0\n      185.0\n      96.0\n      Poland\n      POL\n      1998 Winter\n      1998\n      Winter\n      Nagano\n      Bobsleigh\n      Bobsleigh Men's Four\n      NaN\n    \n    \n      271115\n      271115\n      135571\n      Tomasz Ireneusz ya\n      M\n      34.0\n      185.0\n      96.0\n      Poland\n      POL\n      2002 Winter\n      2002\n      Winter\n      Salt Lake City\n      Bobsleigh\n      Bobsleigh Men's Four\n      NaN\n    \n  \n\n271116 rows × 16 columns\n\n\n\n\ndf.dtypes\n\nevent_athlete_ID      int64\nathlete_ID            int64\nName                 object\nSex                  object\nAge                 float64\nHeight              float64\nWeight              float64\nTeam                 object\nNOC                  object\nGames                object\nYear                  int64\nSeason               object\nCity                 object\nSport                object\nEvent                object\nMedal                object\ndtype: object\n\n\n\ndf.describe()\n\n\n\n\n\n  \n    \n      \n      event_athlete_ID\n      athlete_ID\n      Age\n      Height\n      Weight\n      Year\n    \n  \n  \n    \n      count\n      271116.000000\n      271116.000000\n      261642.000000\n      210945.000000\n      208241.000000\n      271116.000000\n    \n    \n      mean\n      135557.500000\n      68248.954396\n      25.556898\n      175.338970\n      70.702393\n      1978.378480\n    \n    \n      std\n      78264.592128\n      39022.286345\n      6.393561\n      10.518462\n      14.348020\n      29.877632\n    \n    \n      min\n      0.000000\n      1.000000\n      10.000000\n      127.000000\n      25.000000\n      1896.000000\n    \n    \n      25%\n      67778.750000\n      34643.000000\n      21.000000\n      168.000000\n      60.000000\n      1960.000000\n    \n    \n      50%\n      135557.500000\n      68205.000000\n      24.000000\n      175.000000\n      70.000000\n      1988.000000\n    \n    \n      75%\n      203336.250000\n      102097.250000\n      28.000000\n      183.000000\n      79.000000\n      2002.000000\n    \n    \n      max\n      271115.000000\n      135571.000000\n      97.000000\n      226.000000\n      214.000000\n      2016.000000"
  },
  {
    "objectID": "posts/olympics/2022-07-29-OlympicsSQL_createDFs.html#create-all_athletes-table",
    "href": "posts/olympics/2022-07-29-OlympicsSQL_createDFs.html#create-all_athletes-table",
    "title": "Olympics data with SQL and pandas- create the tables",
    "section": "Create all_athletes table",
    "text": "Create all_athletes table\nBecause we are splitting the athlete data based on Summer/Winter and Male/Female we need a folder to be able to join or access different parts of the individual athlete tables.\n\ndf= df.reset_index()\ndf.rename(columns={'index':'event_athlete_ID','ID':'athlete_ID'},inplace=True)\ndf.head(10)\n\n\n\n\n\n  \n    \n      \n      event_athlete_ID\n      event_athlete_ID\n      athlete_ID\n      Name\n      Sex\n      Age\n      Height\n      Weight\n      Team\n      NOC\n      Games\n      Year\n      Season\n      City\n      Sport\n      Event\n      Medal\n    \n  \n  \n    \n      0\n      0\n      0\n      1\n      A Dijiang\n      M\n      24.0\n      180.0\n      80.0\n      China\n      CHN\n      1992 Summer\n      1992\n      Summer\n      Barcelona\n      Basketball\n      Basketball Men's Basketball\n      NaN\n    \n    \n      1\n      1\n      1\n      2\n      A Lamusi\n      M\n      23.0\n      170.0\n      60.0\n      China\n      CHN\n      2012 Summer\n      2012\n      Summer\n      London\n      Judo\n      Judo Men's Extra-Lightweight\n      NaN\n    \n    \n      2\n      2\n      2\n      3\n      Gunnar Nielsen Aaby\n      M\n      24.0\n      NaN\n      NaN\n      Denmark\n      DEN\n      1920 Summer\n      1920\n      Summer\n      Antwerpen\n      Football\n      Football Men's Football\n      NaN\n    \n    \n      3\n      3\n      3\n      4\n      Edgar Lindenau Aabye\n      M\n      34.0\n      NaN\n      NaN\n      Denmark/Sweden\n      DEN\n      1900 Summer\n      1900\n      Summer\n      Paris\n      Tug-Of-War\n      Tug-Of-War Men's Tug-Of-War\n      Gold\n    \n    \n      4\n      4\n      4\n      5\n      Christine Jacoba Aaftink\n      F\n      21.0\n      185.0\n      82.0\n      Netherlands\n      NED\n      1988 Winter\n      1988\n      Winter\n      Calgary\n      Speed Skating\n      Speed Skating Women's 500 metres\n      NaN\n    \n    \n      5\n      5\n      5\n      5\n      Christine Jacoba Aaftink\n      F\n      21.0\n      185.0\n      82.0\n      Netherlands\n      NED\n      1988 Winter\n      1988\n      Winter\n      Calgary\n      Speed Skating\n      Speed Skating Women's 1,000 metres\n      NaN\n    \n    \n      6\n      6\n      6\n      5\n      Christine Jacoba Aaftink\n      F\n      25.0\n      185.0\n      82.0\n      Netherlands\n      NED\n      1992 Winter\n      1992\n      Winter\n      Albertville\n      Speed Skating\n      Speed Skating Women's 500 metres\n      NaN\n    \n    \n      7\n      7\n      7\n      5\n      Christine Jacoba Aaftink\n      F\n      25.0\n      185.0\n      82.0\n      Netherlands\n      NED\n      1992 Winter\n      1992\n      Winter\n      Albertville\n      Speed Skating\n      Speed Skating Women's 1,000 metres\n      NaN\n    \n    \n      8\n      8\n      8\n      5\n      Christine Jacoba Aaftink\n      F\n      27.0\n      185.0\n      82.0\n      Netherlands\n      NED\n      1994 Winter\n      1994\n      Winter\n      Lillehammer\n      Speed Skating\n      Speed Skating Women's 500 metres\n      NaN\n    \n    \n      9\n      9\n      9\n      5\n      Christine Jacoba Aaftink\n      F\n      27.0\n      185.0\n      82.0\n      Netherlands\n      NED\n      1994 Winter\n      1994\n      Winter\n      Lillehammer\n      Speed Skating\n      Speed Skating Women's 1,000 metres\n      NaN\n    \n  \n\n\n\n\n\ndf_all_athletes=df[['event_athlete_ID','athlete_ID','Name','Sex','Season']]\ndf_all_athletes                    \n\n\n\n\n\n  \n    \n      \n      event_athlete_ID\n      event_athlete_ID\n      athlete_ID\n      Name\n      Sex\n      Season\n    \n  \n  \n    \n      0\n      0\n      0\n      1\n      A Dijiang\n      M\n      Summer\n    \n    \n      1\n      1\n      1\n      2\n      A Lamusi\n      M\n      Summer\n    \n    \n      2\n      2\n      2\n      3\n      Gunnar Nielsen Aaby\n      M\n      Summer\n    \n    \n      3\n      3\n      3\n      4\n      Edgar Lindenau Aabye\n      M\n      Summer\n    \n    \n      4\n      4\n      4\n      5\n      Christine Jacoba Aaftink\n      F\n      Winter\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      271111\n      271111\n      271111\n      135569\n      Andrzej ya\n      M\n      Winter\n    \n    \n      271112\n      271112\n      271112\n      135570\n      Piotr ya\n      M\n      Winter\n    \n    \n      271113\n      271113\n      271113\n      135570\n      Piotr ya\n      M\n      Winter\n    \n    \n      271114\n      271114\n      271114\n      135571\n      Tomasz Ireneusz ya\n      M\n      Winter\n    \n    \n      271115\n      271115\n      271115\n      135571\n      Tomasz Ireneusz ya\n      M\n      Winter\n    \n  \n\n271116 rows × 6 columns"
  },
  {
    "objectID": "posts/olympics/2022-07-29-OlympicsSQL_createDFs.html#creating-an-events-table",
    "href": "posts/olympics/2022-07-29-OlympicsSQL_createDFs.html#creating-an-events-table",
    "title": "Olympics data with SQL and pandas- create the tables",
    "section": "Creating an Events Table",
    "text": "Creating an Events Table\nIn this table the individual events are displayed. e.g. 100m Mens Sprint Athletics or Womens Football\n\nprint(\"There are {} unique sports and {} unique events \".format(len(pd.unique(df.Sport)), len(pd.unique(df.Event))))\n\nThere are 66 unique sports and 765 unique events \n\n\nBecause of the way that events are named they won’t be duplicated, e.g. 400m breaststroke swimming will be different from 400m athletics running because the name is prefixed with Athletics Women, Swimming Men etc\nInstead of using one hot encoding (get_dummies for pandas as done with medals) we want a different number for each unique event in one column. To do this we can use factorize\naa=pd.factorize(df.ColumnCheck)\nwill give us a variable where - aa[0] is a list of numbers of length of rows in df, where each value represents a different event - aa[1] is then a list of the events of length of the unique events, aa[1][0] is event = 0, aa[1][1] is event = 1 etc - so below aa[1][0] = ‘Basketball Men’s Basketball’ and each row in the df with this event will have a 0 in aa[0]\n\nevent_details=pd.factorize(df.Event)\nevent_details[1][0:100], event_details[0][1:10]\n\n(Index(['Basketball Men's Basketball', 'Judo Men's Extra-Lightweight',\n        'Football Men's Football', 'Tug-Of-War Men's Tug-Of-War',\n        'Speed Skating Women's 500 metres',\n        'Speed Skating Women's 1,000 metres',\n        'Cross Country Skiing Men's 10 kilometres',\n        'Cross Country Skiing Men's 50 kilometres',\n        'Cross Country Skiing Men's 10/15 kilometres Pursuit',\n        'Cross Country Skiing Men's 4 x 10 kilometres Relay',\n        'Cross Country Skiing Men's 30 kilometres',\n        'Athletics Women's 100 metres',\n        'Athletics Women's 4 x 100 metres Relay', 'Ice Hockey Men's Ice Hockey',\n        'Swimming Men's 400 metres Freestyle', 'Badminton Men's Singles',\n        'Sailing Women's Windsurfer', 'Biathlon Women's 7.5 kilometres Sprint',\n        'Swimming Men's 200 metres Breaststroke',\n        'Swimming Men's 400 metres Breaststroke',\n        'Gymnastics Men's Individual All-Around',\n        'Gymnastics Men's Team All-Around', 'Gymnastics Men's Floor Exercise',\n        'Gymnastics Men's Horse Vault', 'Gymnastics Men's Parallel Bars',\n        'Gymnastics Men's Horizontal Bar', 'Gymnastics Men's Rings',\n        'Gymnastics Men's Pommelled Horse', 'Athletics Men's Shot Put',\n        'Art Competitions Mixed Sculpturing, Unknown Event',\n        'Alpine Skiing Men's Downhill', 'Alpine Skiing Men's Super G',\n        'Alpine Skiing Men's Giant Slalom', 'Alpine Skiing Men's Slalom',\n        'Alpine Skiing Men's Combined', 'Handball Women's Handball',\n        'Weightlifting Women's Super-Heavyweight',\n        'Wrestling Men's Light-Heavyweight, Greco-Roman',\n        'Speed Skating Men's 500 metres', 'Speed Skating Men's 1,500 metres',\n        'Gymnastics Men's Team All-Around, Free System', 'Luge Women's Singles',\n        'Water Polo Men's Water Polo', 'Sailing Mixed Three Person Keelboat',\n        'Hockey Women's Hockey', 'Rowing Men's Lightweight Double Sculls',\n        'Athletics Men's Pole Vault', 'Athletics Men's High Jump',\n        'Sailing Men's Two Person Dinghy', 'Athletics Men's 1,500 metres',\n        'Bobsleigh Men's Four', 'Swimming Men's 100 metres Butterfly',\n        'Swimming Men's 200 metres Butterfly',\n        'Swimming Men's 4 x 100 metres Medley Relay',\n        'Football Women's Football', 'Fencing Men's Foil, Individual',\n        'Fencing Men's epee, Individual', 'Fencing Men's epee, Team',\n        'Speed Skating Men's 5,000 metres', 'Speed Skating Men's 10,000 metres',\n        'Sailing Mixed 8 metres', 'Equestrianism Mixed Jumping, Individual',\n        'Cross Country Skiing Men's 15 kilometres',\n        'Shooting Men's Small-Bore Rifle, Prone, 50 metres',\n        'Shooting Men's Rapid-Fire Pistol, 25 metres', 'Shooting Men's Trap',\n        'Athletics Men's 4 x 100 metres Relay', 'Athletics Men's Long Jump',\n        'Boxing Men's Light-Welterweight', 'Athletics Women's Javelin Throw',\n        'Wrestling Men's Heavyweight, Freestyle', 'Taekwondo Men's Flyweight',\n        'Boxing Men's Heavyweight', 'Athletics Men's 5,000 metres',\n        'Cycling Men's Road Race, Individual', 'Cycling Men's Road Race, Team',\n        'Weightlifting Men's Lightweight', 'Weightlifting Men's Middleweight',\n        'Rowing Men's Coxless Pairs', 'Judo Men's Half-Middleweight',\n        'Taekwondo Women's Flyweight', 'Boxing Men's Flyweight',\n        'Basketball Women's Basketball', 'Diving Men's Platform',\n        'Canoeing Men's Canadian Doubles, 500 metres',\n        'Canoeing Men's Canadian Doubles, 1,000 metres',\n        'Canoeing Men's Kayak Fours, 1,000 metres', 'Handball Men's Handball',\n        'Rowing Women's Coxless Pairs', 'Boxing Men's Middleweight',\n        'Judo Men's Lightweight', 'Boxing Men's Featherweight',\n        'Tennis Men's Doubles', 'Shooting Mixed Skeet',\n        'Wrestling Men's Featherweight, Freestyle',\n        'Sailing Mixed Two Person Heavyweight Dinghy',\n        'Athletics Women's Shot Put', 'Rowing Men's Coxed Eights',\n        'Cycling Women's Sprint', 'Cycling Women's 500 metres Time Trial'],\n       dtype='object'),\n array([1, 2, 3, 4, 5, 4, 5, 4, 5], dtype=int64))\n\n\n\ndf.insert(2,'event_id',event_details[0])\ndf.head(10)\n\n\n\n\n\n  \n    \n      \n      event_athlete_ID\n      event_athlete_ID\n      event_id\n      athlete_ID\n      Name\n      Sex\n      Age\n      Height\n      Weight\n      Team\n      NOC\n      Games\n      Year\n      Season\n      City\n      Sport\n      Event\n      Medal\n    \n  \n  \n    \n      0\n      0\n      0\n      0\n      1\n      A Dijiang\n      M\n      24.0\n      180.0\n      80.0\n      China\n      CHN\n      1992 Summer\n      1992\n      Summer\n      Barcelona\n      Basketball\n      Basketball Men's Basketball\n      NaN\n    \n    \n      1\n      1\n      1\n      1\n      2\n      A Lamusi\n      M\n      23.0\n      170.0\n      60.0\n      China\n      CHN\n      2012 Summer\n      2012\n      Summer\n      London\n      Judo\n      Judo Men's Extra-Lightweight\n      NaN\n    \n    \n      2\n      2\n      2\n      2\n      3\n      Gunnar Nielsen Aaby\n      M\n      24.0\n      NaN\n      NaN\n      Denmark\n      DEN\n      1920 Summer\n      1920\n      Summer\n      Antwerpen\n      Football\n      Football Men's Football\n      NaN\n    \n    \n      3\n      3\n      3\n      3\n      4\n      Edgar Lindenau Aabye\n      M\n      34.0\n      NaN\n      NaN\n      Denmark/Sweden\n      DEN\n      1900 Summer\n      1900\n      Summer\n      Paris\n      Tug-Of-War\n      Tug-Of-War Men's Tug-Of-War\n      Gold\n    \n    \n      4\n      4\n      4\n      4\n      5\n      Christine Jacoba Aaftink\n      F\n      21.0\n      185.0\n      82.0\n      Netherlands\n      NED\n      1988 Winter\n      1988\n      Winter\n      Calgary\n      Speed Skating\n      Speed Skating Women's 500 metres\n      NaN\n    \n    \n      5\n      5\n      5\n      5\n      5\n      Christine Jacoba Aaftink\n      F\n      21.0\n      185.0\n      82.0\n      Netherlands\n      NED\n      1988 Winter\n      1988\n      Winter\n      Calgary\n      Speed Skating\n      Speed Skating Women's 1,000 metres\n      NaN\n    \n    \n      6\n      6\n      6\n      4\n      5\n      Christine Jacoba Aaftink\n      F\n      25.0\n      185.0\n      82.0\n      Netherlands\n      NED\n      1992 Winter\n      1992\n      Winter\n      Albertville\n      Speed Skating\n      Speed Skating Women's 500 metres\n      NaN\n    \n    \n      7\n      7\n      7\n      5\n      5\n      Christine Jacoba Aaftink\n      F\n      25.0\n      185.0\n      82.0\n      Netherlands\n      NED\n      1992 Winter\n      1992\n      Winter\n      Albertville\n      Speed Skating\n      Speed Skating Women's 1,000 metres\n      NaN\n    \n    \n      8\n      8\n      8\n      4\n      5\n      Christine Jacoba Aaftink\n      F\n      27.0\n      185.0\n      82.0\n      Netherlands\n      NED\n      1994 Winter\n      1994\n      Winter\n      Lillehammer\n      Speed Skating\n      Speed Skating Women's 500 metres\n      NaN\n    \n    \n      9\n      9\n      9\n      5\n      5\n      Christine Jacoba Aaftink\n      F\n      27.0\n      185.0\n      82.0\n      Netherlands\n      NED\n      1994 Winter\n      1994\n      Winter\n      Lillehammer\n      Speed Skating\n      Speed Skating Women's 1,000 metres\n      NaN\n    \n  \n\n\n\n\n\nevent_details=pd.factorize(df.Event)\ndf_event = pd.DataFrame(event_details[1])\ndf_event\n\n\n\n\n\n  \n    \n      \n      0\n    \n  \n  \n    \n      0\n      Basketball Men's Basketball\n    \n    \n      1\n      Judo Men's Extra-Lightweight\n    \n    \n      2\n      Football Men's Football\n    \n    \n      3\n      Tug-Of-War Men's Tug-Of-War\n    \n    \n      4\n      Speed Skating Women's 500 metres\n    \n    \n      ...\n      ...\n    \n    \n      760\n      Weightlifting Men's All-Around Dumbbell Contest\n    \n    \n      761\n      Archery Men's Au Chapelet, 33 metres\n    \n    \n      762\n      Archery Men's Au Cordon Dore, 33 metres\n    \n    \n      763\n      Archery Men's Target Archery, 28 metres, Indiv...\n    \n    \n      764\n      Aeronautics Mixed Aeronautics\n    \n  \n\n765 rows × 1 columns\n\n\n\n\n\ndf_event = df[['Sport','Event','Sex','Season']]\n\nevent_details=pd.factorize(df.Sport)\ndf_event.insert(0,'sport_id',event_details[0])\n\nevent_details=pd.factorize(df.Event)\ndf_event.insert(0,'event_id',event_details[0])\n\n\ndf_event\n\n\n\n\n\n  \n    \n      \n      event_id\n      sport_id\n      Sport\n      Event\n      Sex\n      Season\n    \n  \n  \n    \n      0\n      0\n      0\n      Basketball\n      Basketball Men's Basketball\n      M\n      Summer\n    \n    \n      1\n      1\n      1\n      Judo\n      Judo Men's Extra-Lightweight\n      M\n      Summer\n    \n    \n      2\n      2\n      2\n      Football\n      Football Men's Football\n      M\n      Summer\n    \n    \n      3\n      3\n      3\n      Tug-Of-War\n      Tug-Of-War Men's Tug-Of-War\n      M\n      Summer\n    \n    \n      4\n      4\n      4\n      Speed Skating\n      Speed Skating Women's 500 metres\n      F\n      Winter\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      271111\n      461\n      18\n      Luge\n      Luge Mixed (Men)'s Doubles\n      M\n      Winter\n    \n    \n      271112\n      418\n      48\n      Ski Jumping\n      Ski Jumping Men's Large Hill, Individual\n      M\n      Winter\n    \n    \n      271113\n      419\n      48\n      Ski Jumping\n      Ski Jumping Men's Large Hill, Team\n      M\n      Winter\n    \n    \n      271114\n      50\n      22\n      Bobsleigh\n      Bobsleigh Men's Four\n      M\n      Winter\n    \n    \n      271115\n      50\n      22\n      Bobsleigh\n      Bobsleigh Men's Four\n      M\n      Winter\n    \n  \n\n271116 rows × 6 columns\n\n\n\n\ndf_event = df_event.drop_duplicates().reset_index(drop=True)\ndf_event#[df_event.Sex=='F'].head(30)\n\n\n\n\n\n  \n    \n      \n      event_id\n      sport_id\n      Sport\n      Event\n      Sex\n      Season\n    \n  \n  \n    \n      0\n      0\n      0\n      Basketball\n      Basketball Men's Basketball\n      M\n      Summer\n    \n    \n      1\n      1\n      1\n      Judo\n      Judo Men's Extra-Lightweight\n      M\n      Summer\n    \n    \n      2\n      2\n      2\n      Football\n      Football Men's Football\n      M\n      Summer\n    \n    \n      3\n      3\n      3\n      Tug-Of-War\n      Tug-Of-War Men's Tug-Of-War\n      M\n      Summer\n    \n    \n      4\n      4\n      4\n      Speed Skating\n      Speed Skating Women's 500 metres\n      F\n      Winter\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      824\n      221\n      10\n      Sailing\n      Sailing Mixed 7 metres\n      F\n      Summer\n    \n    \n      825\n      333\n      10\n      Sailing\n      Sailing Mixed 6 metres\n      F\n      Summer\n    \n    \n      826\n      764\n      65\n      Aeronautics\n      Aeronautics Mixed Aeronautics\n      M\n      Summer\n    \n    \n      827\n      677\n      13\n      Art Competitions\n      Art Competitions Mixed Sculpturing, Medals And...\n      F\n      Summer\n    \n    \n      828\n      648\n      13\n      Art Competitions\n      Art Competitions Mixed Unknown Event\n      F\n      Summer\n    \n  \n\n829 rows × 6 columns\n\n\n\n\nAn additional columns in event_table\nLets add a column representing if the sport is a team sport or individual. We can’t do this on the unique members of a team in that event because the team can have multiple members in an individual event. Instead, we can look for how many people took the gold medal. Should work for most circumstances as the gold shouldn’t be shared- so if 2 people won gold it should represent a team sport of 2 people.\nThis is a little convoluted so I’ll do it in two steps in SQL. One way to calculate and one to join this new table back in with df_event\n\ndf_event_temp=sqldf(\"SELECT                   \\\n      event_id,event,num_athletes    \\\n      FROM                                    \\\n      (SELECT                                 \\\n          event,                              \\\n          event_id,                           \\\n          COUNT(*)    as num_athletes         \\\n      FROM df                                 \\\n      WHERE Medal='Gold'                      \\\n      GROUP BY Team, event_id, Games          \\\n      ORDER BY event_id                       \\\n      )                                       \\\n      GROUP BY event_id                       \\\n                    ;\",   locals())                               \n\ndf_event_temp\n\n\n\n\n\n  \n    \n      \n      event_id\n      event\n      num_athletes\n    \n  \n  \n    \n      0\n      0\n      Basketball Men's Basketball\n      12\n    \n    \n      1\n      1\n      Judo Men's Extra-Lightweight\n      1\n    \n    \n      2\n      2\n      Football Men's Football\n      16\n    \n    \n      3\n      3\n      Tug-Of-War Men's Tug-Of-War\n      6\n    \n    \n      4\n      4\n      Speed Skating Women's 500 metres\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      745\n      760\n      Weightlifting Men's All-Around Dumbbell Contest\n      1\n    \n    \n      746\n      761\n      Archery Men's Au Chapelet, 33 metres\n      1\n    \n    \n      747\n      762\n      Archery Men's Au Cordon Dore, 33 metres\n      1\n    \n    \n      748\n      763\n      Archery Men's Target Archery, 28 metres, Indiv...\n      1\n    \n    \n      749\n      764\n      Aeronautics Mixed Aeronautics\n      1\n    \n  \n\n750 rows × 3 columns\n\n\n\n\ndf_event_temp      = sqldf(\"SELECT                                \\\n                      d.event_id,                            \\\n                      d.sport_id,                            \\\n                      d.Sport,                               \\\n                      d.Event,                               \\\n                      d.Sex,                                 \\\n                      d.Season,                              \\\n                      t.num_athletes                         \\\n                FROM df_event  as d                          \\\n                LEFT JOIN                                    \\\n                    df_event_temp  as t                      \\\n                ON                                           \\\n                    t.event_id=d.event_id                    \\\n                ORDER BY d.event_id                          \\\n                      ;\",locals())\n\n\n\n# df_event_temp[((df_event_temp.Sex=='F') & (df_event_temp.Season=='Winter'))].head(30)\n\ndf_event=df_event_temp\n\nAnd make the last column int not float (CAST didn’t seem to work perhaps due to NaN values?)\n\ndf_event.fillna(0,inplace=True)\ndf_event = df_event.astype({'num_athletes':'int'})\n\n\n# sanity check\ndf_event[((df_event['num_athletes']==4) & (df_event['Season']=='Winter'))]\n\n\n\n\n\n  \n    \n      \n      event_id\n      sport_id\n      Sport\n      Event\n      Sex\n      Season\n      num_athletes\n    \n  \n  \n    \n      9\n      9\n      5\n      Cross Country Skiing\n      Cross Country Skiing Men's 4 x 10 kilometres R...\n      M\n      Winter\n      4\n    \n    \n      53\n      50\n      22\n      Bobsleigh\n      Bobsleigh Men's Four\n      M\n      Winter\n      4\n    \n    \n      215\n      204\n      40\n      Nordic Combined\n      Nordic Combined Men's Team\n      M\n      Winter\n      4\n    \n    \n      238\n      226\n      11\n      Biathlon\n      Biathlon Men's 4 x 7.5 kilometres Relay\n      M\n      Winter\n      4\n    \n    \n      274\n      259\n      11\n      Biathlon\n      Biathlon Mixed 2 x 6 kilometres and 2 x 7.5 ki...\n      M\n      Winter\n      4\n    \n    \n      275\n      259\n      11\n      Biathlon\n      Biathlon Mixed 2 x 6 kilometres and 2 x 7.5 ki...\n      F\n      Winter\n      4\n    \n    \n      330\n      310\n      11\n      Biathlon\n      Biathlon Women's 4 x 7.5 kilometres Relay\n      F\n      Winter\n      4\n    \n    \n      451\n      419\n      48\n      Ski Jumping\n      Ski Jumping Men's Large Hill, Team\n      M\n      Winter\n      4\n    \n    \n      470\n      435\n      52\n      Short Track Speed Skating\n      Short Track Speed Skating Men's 5,000 metres R...\n      M\n      Winter\n      4\n    \n    \n      478\n      443\n      11\n      Biathlon\n      Biathlon Women's 4 x 6 kilometres Relay\n      F\n      Winter\n      4\n    \n    \n      504\n      466\n      5\n      Cross Country Skiing\n      Cross Country Skiing Women's 4 x 5 kilometres ...\n      F\n      Winter\n      4\n    \n    \n      510\n      472\n      22\n      Bobsleigh\n      Bobsleigh Men's Four/Five\n      M\n      Winter\n      4\n    \n    \n      533\n      492\n      52\n      Short Track Speed Skating\n      Short Track Speed Skating Women's 3,000 metres...\n      F\n      Winter\n      4\n    \n    \n      615\n      572\n      18\n      Luge\n      Luge Mixed Team Relay\n      M\n      Winter\n      4\n    \n    \n      616\n      572\n      18\n      Luge\n      Luge Mixed Team Relay\n      F\n      Winter\n      4\n    \n    \n      640\n      594\n      59\n      Military Ski Patrol\n      Military Ski Patrol Men's Military Ski Patrol\n      M\n      Winter\n      4\n    \n  \n\n\n\n\n\ndf[df.event_id==572][['Sport','Event','Sex']].tail(6),df_event[df_event.event_id==572]\n\n(       Sport                  Event Sex\n 255927  Luge  Luge Mixed Team Relay   M\n 258882  Luge  Luge Mixed Team Relay   M\n 262369  Luge  Luge Mixed Team Relay   F\n 267369  Luge  Luge Mixed Team Relay   M\n 268477  Luge  Luge Mixed Team Relay   M\n 270261  Luge  Luge Mixed Team Relay   M,\n      event_id  sport_id Sport                  Event Sex  Season  num_athletes\n 615       572        18  Luge  Luge Mixed Team Relay   M  Winter             4\n 616       572        18  Luge  Luge Mixed Team Relay   F  Winter             4)"
  },
  {
    "objectID": "posts/olympics/2022-07-29-OlympicsSQL_createDFs.html#add-a-games-table",
    "href": "posts/olympics/2022-07-29-OlympicsSQL_createDFs.html#add-a-games-table",
    "title": "Olympics data with SQL and pandas- create the tables",
    "section": "Add a Games table",
    "text": "Add a Games table\nThe games table is used to give information about a particular Olympic games including - Where it was staged - Is it a summer or winter games - The year it was staged\nUsing the same methodology as before we want to - create a unique id for the games - replace this in the athlete tables - add a new table with the unique id and additional information about the particular games\nThis is more complex than the events table because we want to add additional data about the cities where the games were held. This data is obtained from wikipedia as before. So some of the methodology used in creating the country table is used here.\nThere’s a strange thing that there are two summer games in one year\n\ndf=df.sort_values(by=['Year','City'])\n\n\n# event_details=pd.factorize(pd.lib.fast_zip([df.Games, df.City]))\ntuples = df[['Games', 'City']].apply(tuple, axis=1)\nevent_details = pd.factorize( tuples )\nevent_details[1][0:10], event_details[0][1:10],len(event_details[1]),len(event_details[0])\n\n(Index([   ('1896 Summer', 'Athina'),     ('1900 Summer', 'Paris'),\n        ('1904 Summer', 'St. Louis'),    ('1906 Summer', 'Athina'),\n           ('1908 Summer', 'London'), ('1912 Summer', 'Stockholm'),\n        ('1920 Summer', 'Antwerpen'),  ('1924 Winter', 'Chamonix'),\n            ('1924 Summer', 'Paris'), ('1928 Summer', 'Amsterdam')],\n       dtype='object'),\n array([0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=int64),\n 52,\n 271116)\n\n\n\ndf.insert(3,'games_id',event_details[0])\ndf\n\n\n\n\n\n  \n    \n      \n      event_athlete_ID\n      event_athlete_ID\n      event_id\n      games_id\n      athlete_ID\n      Name\n      Sex\n      Age\n      Height\n      Weight\n      Team\n      NOC\n      Games\n      Year\n      Season\n      City\n      Sport\n      Event\n      Medal\n    \n  \n  \n    \n      3079\n      3079\n      3079\n      191\n      0\n      1724\n      Aristidis Akratopoulos\n      M\n      NaN\n      NaN\n      NaN\n      Greece\n      CRT\n      1896 Summer\n      1896\n      Summer\n      Athina\n      Tennis\n      Tennis Men's Singles\n      NaN\n    \n    \n      3080\n      3080\n      3080\n      92\n      0\n      1724\n      Aristidis Akratopoulos\n      M\n      NaN\n      NaN\n      NaN\n      Greece-3\n      CRT\n      1896 Summer\n      1896\n      Summer\n      Athina\n      Tennis\n      Tennis Men's Doubles\n      NaN\n    \n    \n      3081\n      3081\n      3081\n      191\n      0\n      1725\n      Konstantinos \"Kostas\" Akratopoulos\n      M\n      NaN\n      NaN\n      NaN\n      Greece\n      CRT\n      1896 Summer\n      1896\n      Summer\n      Athina\n      Tennis\n      Tennis Men's Singles\n      NaN\n    \n    \n      3082\n      3082\n      3082\n      92\n      0\n      1725\n      Konstantinos \"Kostas\" Akratopoulos\n      M\n      NaN\n      NaN\n      NaN\n      Greece-3\n      CRT\n      1896 Summer\n      1896\n      Summer\n      Athina\n      Tennis\n      Tennis Men's Doubles\n      NaN\n    \n    \n      7348\n      7348\n      7348\n      100\n      0\n      4113\n      Anastasios Andreou\n      M\n      NaN\n      NaN\n      NaN\n      Greece\n      CRT\n      1896 Summer\n      1896\n      Summer\n      Athina\n      Athletics\n      Athletics Men's 110 metres Hurdles\n      NaN\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      271024\n      271024\n      271024\n      15\n      51\n      135528\n      Marc Zwiebler\n      M\n      32.0\n      181.0\n      75.0\n      Germany\n      FRG\n      2016 Summer\n      2016\n      Summer\n      Rio de Janeiro\n      Badminton\n      Badminton Men's Singles\n      NaN\n    \n    \n      271053\n      271053\n      271053\n      11\n      51\n      135547\n      Viktoriya Viktorovna Zyabkina\n      F\n      23.0\n      174.0\n      62.0\n      Kazakhstan\n      KAZ\n      2016 Summer\n      2016\n      Summer\n      Rio de Janeiro\n      Athletics\n      Athletics Women's 100 metres\n      NaN\n    \n    \n      271054\n      271054\n      271054\n      174\n      51\n      135547\n      Viktoriya Viktorovna Zyabkina\n      F\n      23.0\n      174.0\n      62.0\n      Kazakhstan\n      KAZ\n      2016 Summer\n      2016\n      Summer\n      Rio de Janeiro\n      Athletics\n      Athletics Women's 200 metres\n      NaN\n    \n    \n      271055\n      271055\n      271055\n      12\n      51\n      135547\n      Viktoriya Viktorovna Zyabkina\n      F\n      23.0\n      174.0\n      62.0\n      Kazakhstan\n      KAZ\n      2016 Summer\n      2016\n      Summer\n      Rio de Janeiro\n      Athletics\n      Athletics Women's 4 x 100 metres Relay\n      NaN\n    \n    \n      271110\n      271110\n      271110\n      82\n      51\n      135568\n      Olga Igorevna Zyuzkova\n      F\n      33.0\n      171.0\n      69.0\n      Belarus\n      BLR\n      2016 Summer\n      2016\n      Summer\n      Rio de Janeiro\n      Basketball\n      Basketball Women's Basketball\n      NaN\n    \n  \n\n271116 rows × 19 columns\n\n\n\n\ndf[['games_id','Games','City']].groupby(['games_id','Games']).count()\n\n\n\n\n\n  \n    \n      \n      \n      City\n    \n    \n      games_id\n      Games\n      \n    \n  \n  \n    \n      0\n      1896 Summer\n      380\n    \n    \n      1\n      1900 Summer\n      1936\n    \n    \n      2\n      1904 Summer\n      1301\n    \n    \n      3\n      1906 Summer\n      1733\n    \n    \n      4\n      1908 Summer\n      3101\n    \n    \n      5\n      1912 Summer\n      4040\n    \n    \n      6\n      1920 Summer\n      4292\n    \n    \n      7\n      1924 Winter\n      460\n    \n    \n      8\n      1924 Summer\n      5233\n    \n    \n      9\n      1928 Summer\n      4992\n    \n    \n      10\n      1928 Winter\n      582\n    \n    \n      11\n      1932 Winter\n      352\n    \n    \n      12\n      1932 Summer\n      2969\n    \n    \n      13\n      1936 Summer\n      6506\n    \n    \n      14\n      1936 Winter\n      895\n    \n    \n      15\n      1948 Summer\n      6405\n    \n    \n      16\n      1948 Winter\n      1075\n    \n    \n      17\n      1952 Summer\n      8270\n    \n    \n      18\n      1952 Winter\n      1088\n    \n    \n      19\n      1956 Winter\n      1307\n    \n    \n      20\n      1956 Summer\n      4829\n    \n    \n      21\n      1956 Summer\n      298\n    \n    \n      22\n      1960 Summer\n      8119\n    \n    \n      23\n      1960 Winter\n      1116\n    \n    \n      24\n      1964 Winter\n      1778\n    \n    \n      25\n      1964 Summer\n      7702\n    \n    \n      26\n      1968 Winter\n      1891\n    \n    \n      27\n      1968 Summer\n      8588\n    \n    \n      28\n      1972 Summer\n      10304\n    \n    \n      29\n      1972 Winter\n      1655\n    \n    \n      30\n      1976 Winter\n      1861\n    \n    \n      31\n      1976 Summer\n      8641\n    \n    \n      32\n      1980 Winter\n      1746\n    \n    \n      33\n      1980 Summer\n      7191\n    \n    \n      34\n      1984 Summer\n      9454\n    \n    \n      35\n      1984 Winter\n      2134\n    \n    \n      36\n      1988 Winter\n      2639\n    \n    \n      37\n      1988 Summer\n      12037\n    \n    \n      38\n      1992 Winter\n      3436\n    \n    \n      39\n      1992 Summer\n      12977\n    \n    \n      40\n      1994 Winter\n      3160\n    \n    \n      41\n      1996 Summer\n      13780\n    \n    \n      42\n      1998 Winter\n      3605\n    \n    \n      43\n      2000 Summer\n      13821\n    \n    \n      44\n      2002 Winter\n      4109\n    \n    \n      45\n      2004 Summer\n      13443\n    \n    \n      46\n      2006 Winter\n      4382\n    \n    \n      47\n      2008 Summer\n      13602\n    \n    \n      48\n      2010 Winter\n      4402\n    \n    \n      49\n      2012 Summer\n      12920\n    \n    \n      50\n      2014 Winter\n      4891\n    \n    \n      51\n      2016 Summer\n      13688\n    \n  \n\n\n\n\n\n# Load the games table \ndf_games = pd.read_excel('CitiesOlympics.xlsx',sheet_name=0)\n\n# then sort by year and city like did with df, reset the index\ndf_games=df_games.sort_values(by=['Year','City']).reset_index(drop=True)\n# and replace games_id with new ordered index\ndf_games['games_id']=df_games.index\n\n## sanity check to see if the two tables for games match\n# sanity\ndtemp=df[['games_id','Games','City','Year']].groupby(['games_id','Games','City']).count()\ndtemp.reset_index(inplace=True)\n\npd.concat([df_games[['City','Year','Summer']], dtemp ], axis=1)\n\n# \n\n\n\n\n\n  \n    \n      \n      City\n      Year\n      Summer\n      games_id\n      Games\n      City\n      Year\n    \n  \n  \n    \n      0\n      Athens\n      1896\n      1\n      0\n      1896 Summer\n      Athina\n      380\n    \n    \n      1\n      Paris\n      1900\n      1\n      1\n      1900 Summer\n      Paris\n      1936\n    \n    \n      2\n      St. Louis\n      1904\n      1\n      2\n      1904 Summer\n      St. Louis\n      1301\n    \n    \n      3\n      Athens\n      1906\n      1\n      3\n      1906 Summer\n      Athina\n      1733\n    \n    \n      4\n      London\n      1908\n      1\n      4\n      1908 Summer\n      London\n      3101\n    \n    \n      5\n      Stockholm\n      1912\n      1\n      5\n      1912 Summer\n      Stockholm\n      4040\n    \n    \n      6\n      Antwerp\n      1920\n      1\n      6\n      1920 Summer\n      Antwerpen\n      4292\n    \n    \n      7\n      Chamonix\n      1924\n      0\n      7\n      1924 Winter\n      Chamonix\n      460\n    \n    \n      8\n      Paris\n      1924\n      1\n      8\n      1924 Summer\n      Paris\n      5233\n    \n    \n      9\n      Amsterdam\n      1928\n      1\n      9\n      1928 Summer\n      Amsterdam\n      4992\n    \n    \n      10\n      St. Moritz\n      1928\n      0\n      10\n      1928 Winter\n      Sankt Moritz\n      582\n    \n    \n      11\n      Lake Placid\n      1932\n      0\n      11\n      1932 Winter\n      Lake Placid\n      352\n    \n    \n      12\n      Los Angeles\n      1932\n      1\n      12\n      1932 Summer\n      Los Angeles\n      2969\n    \n    \n      13\n      Berlin\n      1936\n      1\n      13\n      1936 Summer\n      Berlin\n      6506\n    \n    \n      14\n      Garmisch-Partenkirchen\n      1936\n      0\n      14\n      1936 Winter\n      Garmisch-Partenkirchen\n      895\n    \n    \n      15\n      London\n      1948\n      1\n      15\n      1948 Summer\n      London\n      6405\n    \n    \n      16\n      St. Moritz\n      1948\n      0\n      16\n      1948 Winter\n      Sankt Moritz\n      1075\n    \n    \n      17\n      Helsinki\n      1952\n      1\n      17\n      1952 Summer\n      Helsinki\n      8270\n    \n    \n      18\n      Oslo\n      1952\n      0\n      18\n      1952 Winter\n      Oslo\n      1088\n    \n    \n      19\n      Cortina d'Ampezzo\n      1956\n      0\n      19\n      1956 Winter\n      Cortina d'Ampezzo\n      1307\n    \n    \n      20\n      Melbourne\n      1956\n      1\n      20\n      1956 Summer\n      Melbourne\n      4829\n    \n    \n      21\n      Stockholm\n      1956\n      0\n      21\n      1956 Summer\n      Stockholm\n      298\n    \n    \n      22\n      Rome\n      1960\n      1\n      22\n      1960 Summer\n      Roma\n      8119\n    \n    \n      23\n      Squaw Valley\n      1960\n      0\n      23\n      1960 Winter\n      Squaw Valley\n      1116\n    \n    \n      24\n      Innsbruck\n      1964\n      0\n      24\n      1964 Winter\n      Innsbruck\n      1778\n    \n    \n      25\n      Tokyo\n      1964\n      1\n      25\n      1964 Summer\n      Tokyo\n      7702\n    \n    \n      26\n      Grenoble\n      1968\n      0\n      26\n      1968 Winter\n      Grenoble\n      1891\n    \n    \n      27\n      Mexico City\n      1968\n      1\n      27\n      1968 Summer\n      Mexico City\n      8588\n    \n    \n      28\n      Munich\n      1972\n      1\n      28\n      1972 Summer\n      Munich\n      10304\n    \n    \n      29\n      Sapporo\n      1972\n      0\n      29\n      1972 Winter\n      Sapporo\n      1655\n    \n    \n      30\n      Innsbruck\n      1976\n      0\n      30\n      1976 Winter\n      Innsbruck\n      1861\n    \n    \n      31\n      Montreal\n      1976\n      1\n      31\n      1976 Summer\n      Montreal\n      8641\n    \n    \n      32\n      Lake Placid\n      1980\n      0\n      32\n      1980 Winter\n      Lake Placid\n      1746\n    \n    \n      33\n      Moscow\n      1980\n      1\n      33\n      1980 Summer\n      Moskva\n      7191\n    \n    \n      34\n      Los Angeles\n      1984\n      1\n      34\n      1984 Summer\n      Los Angeles\n      9454\n    \n    \n      35\n      Sarajevo\n      1984\n      0\n      35\n      1984 Winter\n      Sarajevo\n      2134\n    \n    \n      36\n      Calgary\n      1988\n      0\n      36\n      1988 Winter\n      Calgary\n      2639\n    \n    \n      37\n      Seoul\n      1988\n      1\n      37\n      1988 Summer\n      Seoul\n      12037\n    \n    \n      38\n      Albertville\n      1992\n      0\n      38\n      1992 Winter\n      Albertville\n      3436\n    \n    \n      39\n      Barcelona\n      1992\n      1\n      39\n      1992 Summer\n      Barcelona\n      12977\n    \n    \n      40\n      Lillehammer\n      1994\n      0\n      40\n      1994 Winter\n      Lillehammer\n      3160\n    \n    \n      41\n      Atlanta\n      1996\n      1\n      41\n      1996 Summer\n      Atlanta\n      13780\n    \n    \n      42\n      Nagano\n      1998\n      0\n      42\n      1998 Winter\n      Nagano\n      3605\n    \n    \n      43\n      Sydney\n      2000\n      1\n      43\n      2000 Summer\n      Sydney\n      13821\n    \n    \n      44\n      Salt Lake City\n      2002\n      0\n      44\n      2002 Winter\n      Salt Lake City\n      4109\n    \n    \n      45\n      Athens\n      2004\n      1\n      45\n      2004 Summer\n      Athina\n      13443\n    \n    \n      46\n      Turin\n      2006\n      0\n      46\n      2006 Winter\n      Torino\n      4382\n    \n    \n      47\n      Beijing\n      2008\n      1\n      47\n      2008 Summer\n      Beijing\n      13602\n    \n    \n      48\n      Vancouver\n      2010\n      0\n      48\n      2010 Winter\n      Vancouver\n      4402\n    \n    \n      49\n      London\n      2012\n      1\n      49\n      2012 Summer\n      London\n      12920\n    \n    \n      50\n      Sochi\n      2014\n      0\n      50\n      2014 Winter\n      Sochi\n      4891\n    \n    \n      51\n      Rio de Janeiro\n      2016\n      1\n      51\n      2016 Summer\n      Rio de Janeiro\n      13688\n    \n  \n\n\n\n\n\nimport re\n\n# starts with a digit (at start of string) or '.'- goes on for undefinable length  \nregex_pattern=r'^[\\d|.]*'\n# starts with a comma then spaces then digits or '.'\nregex_pattern2=',\\s*[\\d|.|]*'\n\ntest_string = df_games.iloc[1,-2]\nprint(test_string)\na=re.search(regex_pattern2, test_string)\n\nfor i in range(len(df_games)):\n#     print(df_games.iloc[i,-2])\n    try:\n        df_games.iloc[i,-1]= re.search(regex_pattern2,df_games.iloc[i,-2])[0][2:] \n        df_games.iloc[i,-2]= re.search(regex_pattern,df_games.iloc[i,-2])[0] \n    except:\n        pass\n\n48.8566° N, 2.3522°\n\n\n\ndef getNation(region_to_check):\n    import re    \n    if region_to_check=='United States':\n        region_out = 'USA'\n    elif bool(re.search(r'Germany', region_to_check)):\n        region_out='Germany'\n    elif region_to_check=='United Kingdom':\n        region_out = 'UK'\n    elif region_to_check=='Soviet Union':\n        region_out='Russia'\n    else:\n        region_out=region_to_check\n        print('nothing found for {}'.format(region_out))\n    \n    return region_out\n\nfor i in range(len(df_games)):\n    x=df2[df2.region==df_games.iloc[i,2]].region\n    if len(x)<1:\n        x=df2[df2.notes==df_games.iloc[i,2]].region\n        if len(x)<1:\n            x=getNation(df_games.iloc[i,2])\n#     \n    try:\n        df_games.iloc[i,2]=str(x.iloc[0]) \n    except:\n        df_games.iloc[i,2]=str(x) \n#     print(x)\n#     if len(x)<1:\n#         print('----------------------------------')\n        \n        \n\n\ndf_games.iloc[10:20,:]\n\n\n\n\n\n  \n    \n      \n      games_id\n      City\n      Country\n      Year\n      Region\n      Summer\n      Winter\n      Latitude\n      Longitude\n    \n  \n  \n    \n      10\n      10\n      St. Moritz\n      Switzerland\n      1928\n      Europe\n      0\n      1\n      46.4908\n      9.8355\n    \n    \n      11\n      11\n      Lake Placid\n      USA\n      1932\n      North America\n      0\n      1\n      27.2931\n      81.3629\n    \n    \n      12\n      12\n      Los Angeles\n      USA\n      1932\n      North America\n      1\n      0\n      34.0522\n      118.2437\n    \n    \n      13\n      13\n      Berlin\n      Germany\n      1936\n      Europe\n      1\n      0\n      52.5200\n      13.4050\n    \n    \n      14\n      14\n      Garmisch-Partenkirchen\n      Germany\n      1936\n      Europe\n      0\n      1\n      47.4919\n      11.0948\n    \n    \n      15\n      15\n      London\n      UK\n      1948\n      Europe\n      1\n      0\n      51.5072\n      0.1276\n    \n    \n      16\n      16\n      St. Moritz\n      Switzerland\n      1948\n      Europe\n      0\n      1\n      46.4908\n      9.8355\n    \n    \n      17\n      17\n      Helsinki\n      Finland\n      1952\n      Europe\n      1\n      0\n      60.1699\n      24.9384\n    \n    \n      18\n      18\n      Oslo\n      Norway\n      1952\n      Europe\n      0\n      1\n      59.9139\n      10.7522\n    \n    \n      19\n      19\n      Cortina d'Ampezzo\n      Italy\n      1956\n      Europe\n      0\n      1\n      46.5405\n      12.1357\n    \n  \n\n\n\n\n\ndf.groupby(['games_id','Year','Games','City']).count().reset_index()\n\n\n\n\n\n  \n    \n      \n      games_id\n      Year\n      Games\n      City\n      event_athlete_ID\n      event_athlete_ID\n      event_id\n      athlete_ID\n      Name\n      Sex\n      Age\n      Height\n      Weight\n      Team\n      NOC\n      Season\n      Sport\n      Event\n      Medal\n    \n  \n  \n    \n      0\n      0\n      1896\n      1896 Summer\n      Athina\n      380\n      380\n      380\n      380\n      380\n      380\n      217\n      46\n      49\n      380\n      380\n      380\n      380\n      380\n      143\n    \n    \n      1\n      1\n      1900\n      1900 Summer\n      Paris\n      1936\n      1936\n      1936\n      1936\n      1936\n      1936\n      1146\n      116\n      79\n      1936\n      1936\n      1936\n      1936\n      1936\n      604\n    \n    \n      2\n      2\n      1904\n      1904 Summer\n      St. Louis\n      1301\n      1301\n      1301\n      1301\n      1301\n      1301\n      1027\n      213\n      147\n      1301\n      1301\n      1301\n      1301\n      1301\n      486\n    \n    \n      3\n      3\n      1906\n      1906 Summer\n      Athina\n      1733\n      1733\n      1733\n      1733\n      1733\n      1733\n      990\n      257\n      205\n      1733\n      1733\n      1733\n      1733\n      1733\n      458\n    \n    \n      4\n      4\n      1908\n      1908 Summer\n      London\n      3101\n      3101\n      3101\n      3101\n      3101\n      3101\n      2452\n      475\n      483\n      3101\n      3101\n      3101\n      3101\n      3101\n      831\n    \n    \n      5\n      5\n      1912\n      1912 Summer\n      Stockholm\n      4040\n      4040\n      4040\n      4040\n      4040\n      4040\n      3884\n      721\n      596\n      4040\n      4040\n      4040\n      4040\n      4040\n      941\n    \n    \n      6\n      6\n      1920\n      1920 Summer\n      Antwerpen\n      4292\n      4292\n      4292\n      4292\n      4292\n      4292\n      3447\n      767\n      471\n      4292\n      4292\n      4292\n      4292\n      4292\n      1308\n    \n    \n      7\n      7\n      1924\n      1924 Winter\n      Chamonix\n      460\n      460\n      460\n      460\n      460\n      460\n      403\n      89\n      41\n      460\n      460\n      460\n      460\n      460\n      130\n    \n    \n      8\n      8\n      1924\n      1924 Summer\n      Paris\n      5233\n      5233\n      5233\n      5233\n      5233\n      5233\n      4148\n      885\n      649\n      5233\n      5233\n      5233\n      5233\n      5233\n      832\n    \n    \n      9\n      9\n      1928\n      1928 Summer\n      Amsterdam\n      4992\n      4992\n      4992\n      4992\n      4992\n      4992\n      4119\n      853\n      670\n      4992\n      4992\n      4992\n      4992\n      4992\n      734\n    \n    \n      10\n      10\n      1928\n      1928 Winter\n      Sankt Moritz\n      582\n      582\n      582\n      582\n      582\n      582\n      492\n      122\n      48\n      582\n      582\n      582\n      582\n      582\n      89\n    \n    \n      11\n      11\n      1932\n      1932 Winter\n      Lake Placid\n      352\n      352\n      352\n      352\n      352\n      352\n      329\n      196\n      55\n      352\n      352\n      352\n      352\n      352\n      92\n    \n    \n      12\n      12\n      1932\n      1932 Summer\n      Los Angeles\n      2969\n      2969\n      2969\n      2969\n      2969\n      2969\n      2662\n      1017\n      495\n      2969\n      2969\n      2969\n      2969\n      2969\n      647\n    \n    \n      13\n      13\n      1936\n      1936 Summer\n      Berlin\n      6506\n      6506\n      6506\n      6506\n      6506\n      6506\n      6304\n      1056\n      909\n      6506\n      6506\n      6506\n      6506\n      6506\n      917\n    \n    \n      14\n      14\n      1936\n      1936 Winter\n      Garmisch-Partenkirchen\n      895\n      895\n      895\n      895\n      895\n      895\n      884\n      136\n      78\n      895\n      895\n      895\n      895\n      895\n      108\n    \n    \n      15\n      15\n      1948\n      1948 Summer\n      London\n      6405\n      6405\n      6405\n      6405\n      6405\n      6405\n      5233\n      1053\n      1040\n      6405\n      6405\n      6405\n      6405\n      6405\n      852\n    \n    \n      16\n      16\n      1948\n      1948 Winter\n      Sankt Moritz\n      1075\n      1075\n      1075\n      1075\n      1075\n      1075\n      1071\n      116\n      111\n      1075\n      1075\n      1075\n      1075\n      1075\n      135\n    \n    \n      17\n      17\n      1952\n      1952 Summer\n      Helsinki\n      8270\n      8270\n      8270\n      8270\n      8270\n      8270\n      7993\n      2038\n      2038\n      8270\n      8270\n      8270\n      8270\n      8270\n      897\n    \n    \n      18\n      18\n      1952\n      1952 Winter\n      Oslo\n      1088\n      1088\n      1088\n      1088\n      1088\n      1088\n      1088\n      150\n      149\n      1088\n      1088\n      1088\n      1088\n      1088\n      136\n    \n    \n      19\n      19\n      1956\n      1956 Winter\n      Cortina d'Ampezzo\n      1307\n      1307\n      1307\n      1307\n      1307\n      1307\n      1282\n      344\n      342\n      1307\n      1307\n      1307\n      1307\n      1307\n      150\n    \n    \n      20\n      20\n      1956\n      1956 Summer\n      Melbourne\n      4829\n      4829\n      4829\n      4829\n      4829\n      4829\n      4256\n      2234\n      2232\n      4829\n      4829\n      4829\n      4829\n      4829\n      857\n    \n    \n      21\n      21\n      1956\n      1956 Summer\n      Stockholm\n      298\n      298\n      298\n      298\n      298\n      298\n      258\n      108\n      106\n      298\n      298\n      298\n      298\n      298\n      36\n    \n    \n      22\n      22\n      1960\n      1960 Summer\n      Roma\n      8119\n      8119\n      8119\n      8119\n      8119\n      8119\n      7906\n      7738\n      7675\n      8119\n      8119\n      8119\n      8119\n      8119\n      911\n    \n    \n      23\n      23\n      1960\n      1960 Winter\n      Squaw Valley\n      1116\n      1116\n      1116\n      1116\n      1116\n      1116\n      1108\n      536\n      512\n      1116\n      1116\n      1116\n      1116\n      1116\n      147\n    \n    \n      24\n      24\n      1964\n      1964 Winter\n      Innsbruck\n      1778\n      1778\n      1778\n      1778\n      1778\n      1778\n      1765\n      1369\n      1348\n      1778\n      1778\n      1778\n      1778\n      1778\n      186\n    \n    \n      25\n      25\n      1964\n      1964 Summer\n      Tokyo\n      7702\n      7702\n      7702\n      7702\n      7702\n      7702\n      7659\n      7430\n      7424\n      7702\n      7702\n      7702\n      7702\n      7702\n      1029\n    \n    \n      26\n      26\n      1968\n      1968 Winter\n      Grenoble\n      1891\n      1891\n      1891\n      1891\n      1891\n      1891\n      1872\n      1833\n      1817\n      1891\n      1891\n      1891\n      1891\n      1891\n      199\n    \n    \n      27\n      27\n      1968\n      1968 Summer\n      Mexico City\n      8588\n      8588\n      8588\n      8588\n      8588\n      8588\n      8489\n      8493\n      8493\n      8588\n      8588\n      8588\n      8588\n      8588\n      1057\n    \n    \n      28\n      28\n      1972\n      1972 Summer\n      Munich\n      10304\n      10304\n      10304\n      10304\n      10304\n      10304\n      10211\n      10018\n      9928\n      10304\n      10304\n      10304\n      10304\n      10304\n      1215\n    \n    \n      29\n      29\n      1972\n      1972 Winter\n      Sapporo\n      1655\n      1655\n      1655\n      1655\n      1655\n      1655\n      1652\n      1640\n      1642\n      1655\n      1655\n      1655\n      1655\n      1655\n      199\n    \n    \n      30\n      30\n      1976\n      1976 Winter\n      Innsbruck\n      1861\n      1861\n      1861\n      1861\n      1861\n      1861\n      1850\n      1343\n      1302\n      1861\n      1861\n      1861\n      1861\n      1861\n      211\n    \n    \n      31\n      31\n      1976\n      1976 Summer\n      Montreal\n      8641\n      8641\n      8641\n      8641\n      8641\n      8641\n      8600\n      8283\n      8280\n      8641\n      8641\n      8641\n      8641\n      8641\n      1320\n    \n    \n      32\n      32\n      1980\n      1980 Winter\n      Lake Placid\n      1746\n      1746\n      1746\n      1746\n      1746\n      1746\n      1745\n      1388\n      1374\n      1746\n      1746\n      1746\n      1746\n      1746\n      218\n    \n    \n      33\n      33\n      1980\n      1980 Summer\n      Moskva\n      7191\n      7191\n      7191\n      7191\n      7191\n      7191\n      7005\n      6961\n      6967\n      7191\n      7191\n      7191\n      7191\n      7191\n      1384\n    \n    \n      34\n      34\n      1984\n      1984 Summer\n      Los Angeles\n      9454\n      9454\n      9454\n      9454\n      9454\n      9454\n      9249\n      9032\n      9031\n      9454\n      9454\n      9454\n      9454\n      9454\n      1476\n    \n    \n      35\n      35\n      1984\n      1984 Winter\n      Sarajevo\n      2134\n      2134\n      2134\n      2134\n      2134\n      2134\n      2123\n      1958\n      1954\n      2134\n      2134\n      2134\n      2134\n      2134\n      222\n    \n    \n      36\n      36\n      1988\n      1988 Winter\n      Calgary\n      2639\n      2639\n      2639\n      2639\n      2639\n      2639\n      2635\n      2024\n      2018\n      2639\n      2639\n      2639\n      2639\n      2639\n      263\n    \n    \n      37\n      37\n      1988\n      1988 Summer\n      Seoul\n      12037\n      12037\n      12037\n      12037\n      12037\n      12037\n      11931\n      11719\n      11730\n      12037\n      12037\n      12037\n      12037\n      12037\n      1582\n    \n    \n      38\n      38\n      1992\n      1992 Winter\n      Albertville\n      3436\n      3436\n      3436\n      3436\n      3436\n      3436\n      3435\n      2785\n      2783\n      3436\n      3436\n      3436\n      3436\n      3436\n      318\n    \n    \n      39\n      39\n      1992\n      1992 Summer\n      Barcelona\n      12977\n      12977\n      12977\n      12977\n      12977\n      12977\n      12934\n      10453\n      10473\n      12977\n      12977\n      12977\n      12977\n      12977\n      1712\n    \n    \n      40\n      40\n      1994\n      1994 Winter\n      Lillehammer\n      3160\n      3160\n      3160\n      3160\n      3160\n      3160\n      3158\n      2973\n      2971\n      3160\n      3160\n      3160\n      3160\n      3160\n      331\n    \n    \n      41\n      41\n      1996\n      1996 Summer\n      Atlanta\n      13780\n      13780\n      13780\n      13780\n      13780\n      13780\n      13772\n      11909\n      11959\n      13780\n      13780\n      13780\n      13780\n      13780\n      1842\n    \n    \n      42\n      42\n      1998\n      1998 Winter\n      Nagano\n      3605\n      3605\n      3605\n      3605\n      3605\n      3605\n      3603\n      3521\n      3519\n      3605\n      3605\n      3605\n      3605\n      3605\n      440\n    \n    \n      43\n      43\n      2000\n      2000 Summer\n      Sydney\n      13821\n      13821\n      13821\n      13821\n      13821\n      13821\n      13820\n      13698\n      13695\n      13821\n      13821\n      13821\n      13821\n      13821\n      2004\n    \n    \n      44\n      44\n      2002\n      2002 Winter\n      Salt Lake City\n      4109\n      4109\n      4109\n      4109\n      4109\n      4109\n      4109\n      4080\n      4062\n      4109\n      4109\n      4109\n      4109\n      4109\n      478\n    \n    \n      45\n      45\n      2004\n      2004 Summer\n      Athina\n      13443\n      13443\n      13443\n      13443\n      13443\n      13443\n      13443\n      13407\n      13406\n      13443\n      13443\n      13443\n      13443\n      13443\n      2001\n    \n    \n      46\n      46\n      2006\n      2006 Winter\n      Torino\n      4382\n      4382\n      4382\n      4382\n      4382\n      4382\n      4382\n      4376\n      4366\n      4382\n      4382\n      4382\n      4382\n      4382\n      526\n    \n    \n      47\n      47\n      2008\n      2008 Summer\n      Beijing\n      13602\n      13602\n      13602\n      13602\n      13602\n      13602\n      13600\n      13451\n      13443\n      13602\n      13602\n      13602\n      13602\n      13602\n      2048\n    \n    \n      48\n      48\n      2010\n      2010 Winter\n      Vancouver\n      4402\n      4402\n      4402\n      4402\n      4402\n      4402\n      4402\n      4400\n      4378\n      4402\n      4402\n      4402\n      4402\n      4402\n      520\n    \n    \n      49\n      49\n      2012\n      2012 Summer\n      London\n      12920\n      12920\n      12920\n      12920\n      12920\n      12920\n      12920\n      12752\n      12560\n      12920\n      12920\n      12920\n      12920\n      12920\n      1941\n    \n    \n      50\n      50\n      2014\n      2014 Winter\n      Sochi\n      4891\n      4891\n      4891\n      4891\n      4891\n      4891\n      4891\n      4871\n      4673\n      4891\n      4891\n      4891\n      4891\n      4891\n      597\n    \n    \n      51\n      51\n      2016\n      2016 Summer\n      Rio de Janeiro\n      13688\n      13688\n      13688\n      13688\n      13688\n      13688\n      13688\n      13512\n      13465\n      13688\n      13688\n      13688\n      13688\n      13688\n      2023"
  },
  {
    "objectID": "posts/olympics/2022-07-29-OlympicsSQL_createDFs.html#create-some-smaller-athlete-tables",
    "href": "posts/olympics/2022-07-29-OlympicsSQL_createDFs.html#create-some-smaller-athlete-tables",
    "title": "Olympics data with SQL and pandas- create the tables",
    "section": "Create some smaller athlete tables",
    "text": "Create some smaller athlete tables\nFinally the main athletes df file can be split the up by sex and season, since the id’s for different fields have been added\n\nWe don’t really need sex and Season so we can drop these\nWe can also reorder the index (not the athlete ID)\nIf Medals = NaN this probably means they didn’t get one. So we can replace medals with Gold, Silver and Bronze (one-hot encoding)\nInstead of Games, Year, City lets replace with a uniques INT id. And put that data in another dataframe for cities\n\n\n# split and Reset index, so it increases 1,2,3,4,etc\n\ndf_M_S = df[(df.Sex=='M') & (df.Season=='Summer')]\ndf_M_S.reset_index(inplace = True,drop=True)\n\ndf_F_S = df[(df.Sex=='F') & (df.Season=='Summer')]\ndf_F_S.reset_index(inplace = True,drop=True)\n\ndf_M_W = df[(df.Sex=='M') & (df.Season=='Winter')]\ndf_M_W.reset_index(inplace = True,drop=True)\n\ndf_F_W = df[(df.Sex=='F') & (df.Season=='Winter')]\ndf_F_W.reset_index(inplace = True,drop=True)\n\n\n# Look at the data\n\ndf_M_W.head(10)\n\n\n\n\n\n  \n    \n      \n      event_athlete_ID\n      event_athlete_ID\n      event_id\n      games_id\n      athlete_ID\n      Name\n      Sex\n      Age\n      Height\n      Weight\n      Team\n      NOC\n      Games\n      Year\n      Season\n      City\n      Sport\n      Event\n      Medal\n    \n  \n  \n    \n      0\n      672\n      672\n      13\n      7\n      391\n      Clarence John Abel\n      M\n      23.0\n      185.0\n      102.0\n      United States\n      USA\n      1924 Winter\n      1924\n      Winter\n      Chamonix\n      Ice Hockey\n      Ice Hockey Men's Ice Hockey\n      Silver\n    \n    \n      1\n      1791\n      1791\n      205\n      7\n      992\n      Josef Adolf\n      M\n      25.0\n      NaN\n      NaN\n      Czechoslovakia\n      BOH\n      1924 Winter\n      1924\n      Winter\n      Chamonix\n      Nordic Combined\n      Nordic Combined Men's Individual\n      NaN\n    \n    \n      2\n      1951\n      1951\n      300\n      7\n      1077\n      Xavier Affentranger\n      M\n      26.0\n      NaN\n      NaN\n      Switzerland\n      SUI\n      1924 Winter\n      1924\n      Winter\n      Chamonix\n      Ski Jumping\n      Ski Jumping Men's Normal Hill, Individual\n      NaN\n    \n    \n      3\n      1952\n      1952\n      385\n      7\n      1077\n      Xavier Affentranger\n      M\n      26.0\n      NaN\n      NaN\n      Switzerland\n      SUI\n      1924 Winter\n      1924\n      Winter\n      Chamonix\n      Cross Country Skiing\n      Cross Country Skiing Men's 18 kilometres\n      NaN\n    \n    \n      4\n      1953\n      1953\n      205\n      7\n      1077\n      Xavier Affentranger\n      M\n      26.0\n      NaN\n      NaN\n      Switzerland\n      SUI\n      1924 Winter\n      1924\n      Winter\n      Chamonix\n      Nordic Combined\n      Nordic Combined Men's Individual\n      NaN\n    \n    \n      5\n      2397\n      2397\n      305\n      7\n      1341\n      Johan Petter hln (Andersson-)\n      M\n      44.0\n      NaN\n      NaN\n      Sweden\n      SWE\n      1924 Winter\n      1924\n      Winter\n      Chamonix\n      Curling\n      Curling Men's Curling\n      Silver\n    \n    \n      6\n      4040\n      4040\n      300\n      7\n      2329\n      Louis Albert\n      M\n      25.0\n      NaN\n      NaN\n      France\n      FRA\n      1924 Winter\n      1924\n      Winter\n      Chamonix\n      Ski Jumping\n      Ski Jumping Men's Normal Hill, Individual\n      NaN\n    \n    \n      7\n      4249\n      4249\n      472\n      7\n      2431\n      Henri Eugne Aldebert\n      M\n      43.0\n      NaN\n      NaN\n      France-1\n      FRA\n      1924 Winter\n      1924\n      Winter\n      Chamonix\n      Bobsleigh\n      Bobsleigh Men's Four/Five\n      NaN\n    \n    \n      8\n      5103\n      5103\n      13\n      7\n      2902\n      Karl Ruben Allinger\n      M\n      32.0\n      NaN\n      NaN\n      Sweden\n      SWE\n      1924 Winter\n      1924\n      Winter\n      Chamonix\n      Ice Hockey\n      Ice Hockey Men's Ice Hockey\n      NaN\n    \n    \n      9\n      5174\n      5174\n      7\n      7\n      2939\n      Ernst Alm\n      M\n      23.0\n      174.0\n      NaN\n      Sweden\n      SWE\n      1924 Winter\n      1924\n      Winter\n      Chamonix\n      Cross Country Skiing\n      Cross Country Skiing Men's 50 kilometres\n      NaN\n    \n  \n\n\n\n\n\n\n\n\ndf_M_S = df_M_S.drop(columns=['Name','Sex','Season','City','Games','Sport','Event'])\ndf_F_S = df_F_S.drop(columns=['Name','Sex','Season','City','Games','Sport','Event'])\ndf_M_W = df_M_W.drop(columns=['Name','Sex','Season','City','Games','Sport','Event'])\ndf_F_W = df_F_W.drop(columns=['Name','Sex','Season','City','Games','Sport','Event'])\ndf_F_W.head(5)\n\n\n\n\n\n  \n    \n      \n      event_athlete_ID\n      event_athlete_ID\n      event_id\n      games_id\n      athlete_ID\n      Age\n      Height\n      Weight\n      Team\n      NOC\n      Year\n      Medal\n    \n  \n  \n    \n      0\n      30620\n      30620\n      268\n      7\n      15776\n      22.0\n      165.0\n      NaN\n      France\n      FRA\n      1924\n      NaN\n    \n    \n      1\n      30621\n      30621\n      242\n      7\n      15776\n      22.0\n      165.0\n      NaN\n      France-1\n      FRA\n      1924\n      Bronze\n    \n    \n      2\n      63714\n      63714\n      242\n      7\n      32641\n      25.0\n      NaN\n      NaN\n      Austria\n      AUT\n      1924\n      Gold\n    \n    \n      3\n      94058\n      94058\n      268\n      7\n      47618\n      11.0\n      155.0\n      45.0\n      Norway\n      NOR\n      1924\n      NaN\n    \n    \n      4\n      94533\n      94533\n      242\n      7\n      47845\n      NaN\n      NaN\n      NaN\n      Belgium\n      BEL\n      1924\n      NaN\n    \n  \n\n\n\n\n\nSeperate out medals\n\ndf_F_W = pd.get_dummies(df_F_W,columns=['Medal'])\ndf_M_W = pd.get_dummies(df_M_W,columns=['Medal'])\ndf_F_S = pd.get_dummies(df_F_S,columns=['Medal'])\ndf_M_S = pd.get_dummies(df_M_S,columns=['Medal'])\ndf_M_S.head()\n\n\n\n\n\n  \n    \n      \n      event_athlete_ID\n      event_athlete_ID\n      event_id\n      games_id\n      athlete_ID\n      Age\n      Height\n      Weight\n      Team\n      NOC\n      Year\n      Medal_Bronze\n      Medal_Gold\n      Medal_Silver\n    \n  \n  \n    \n      0\n      3079\n      3079\n      191\n      0\n      1724\n      NaN\n      NaN\n      NaN\n      Greece\n      CRT\n      1896\n      0\n      0\n      0\n    \n    \n      1\n      3080\n      3080\n      92\n      0\n      1724\n      NaN\n      NaN\n      NaN\n      Greece-3\n      CRT\n      1896\n      0\n      0\n      0\n    \n    \n      2\n      3081\n      3081\n      191\n      0\n      1725\n      NaN\n      NaN\n      NaN\n      Greece\n      CRT\n      1896\n      0\n      0\n      0\n    \n    \n      3\n      3082\n      3082\n      92\n      0\n      1725\n      NaN\n      NaN\n      NaN\n      Greece-3\n      CRT\n      1896\n      0\n      0\n      0\n    \n    \n      4\n      7348\n      7348\n      100\n      0\n      4113\n      NaN\n      NaN\n      NaN\n      Greece\n      CRT\n      1896\n      0\n      0\n      0"
  },
  {
    "objectID": "posts/olympics/2022-07-29-OlympicsSQL_createDFs.html#save-it",
    "href": "posts/olympics/2022-07-29-OlympicsSQL_createDFs.html#save-it",
    "title": "Olympics data with SQL and pandas- create the tables",
    "section": "Save it",
    "text": "Save it\n\ndf_F_S.to_csv('athlete_F_S')\ndf_F_W.to_csv('athlete_F_W')\ndf_M_S.to_csv('athlete_M_S')\ndf_M_W.to_csv('athlete_M_W')\n\ndf_all_athletes.to_csv('all_athletes')\ndf_country.to_csv('country')\ndf_event.to_csv('event')\ndf_games.to_csv('games')\ndf_population.to_csv('population')"
  },
  {
    "objectID": "posts/olympics/2022-07-29-OlympicsSQL_GDPpopulation.html#introduction",
    "href": "posts/olympics/2022-07-29-OlympicsSQL_GDPpopulation.html#introduction",
    "title": "Olympics data with SQL and pandas- GDP and population",
    "section": "Introduction",
    "text": "Introduction\nDue to the global importance of the Olympics, in 2020 there was a broadcast audience of more than 3 billion, I was interested to explore whether countries with the most medals will reflect global politics. And to see if the countries with most influence get more medals.\nThere are two relatively easy to obtain metrics that can be used to define the importance of a nation internationally, GDP and population.\nGross domestic product (GDP) is a monetary measure of the market value of all the final goods and services produced in a specific time period by countries. GDP is often used as a metric for international comparisons as well as a broad measure of economic progress. It is often considered to be the “world’s most powerful statistical indicator of national development and progress”. GDP Wikipedia.\nThe population of a country is an important parameter in assessing the global importance of a nation. Furthermore, the more people in a country the greater the pool of potential athletes.\nIn the final part of this page I will briefly look at how the Cold War was reflected in the Olympics data."
  },
  {
    "objectID": "posts/olympics/2022-07-29-OlympicsSQL_GDPpopulation.html#pre-analysis",
    "href": "posts/olympics/2022-07-29-OlympicsSQL_GDPpopulation.html#pre-analysis",
    "title": "Olympics data with SQL and pandas- GDP and population",
    "section": "Pre-analysis",
    "text": "Pre-analysis\n\nGet medals won by country and year\nWhat we want to end up with is a new table with- - Nation - Number of medals - Event or year of the event - Then the details of nations we’re studying: - GDP - Population - Continent\nTo get a unique medal count for this, we can do a count by grouping on - Nation and Year But also on: - Medal type (Gold, Silver, Bronze) - Event id As we only know if a medal is unique for an athlete if the same medal type does not exist for another athlete in the same event at the same games (year). This is to avoid duplication for team sports. If it is a mixed event then we would need to do the grouping on data of male and female athletes.\nFrom inside out\n\nUse UNION to join the males and female summer athletes\n\nGet NOC, medal types, year, event_id\n\nUse GROUP BY to get unique medals and COUNT\nUse GROUP BY to get medals for countries in each event/year, SUM so we are adding up events\n\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pandasql import sqldf\nimport copy\nimport numpy as np\nimport scipy.stats\n\n\ndf_F_S =pd.read_csv('data/athlete_F_S')\n# df_F_W=pd.read_csv('data/athlete_F_W')\ndf_M_S=pd.read_csv('data/athlete_M_S')\n# df_M_W=pd.read_csv('data/athlete_M_W')\n\ndf_all_athletes= pd.read_csv('data/all_athletes')\ndf_country= pd.read_csv('data/country')\ndf_event= pd.read_csv('data/event')\n# df_games= pd.read_csv('data/games')\n# df_population= pd.read_csv('data/population')\n\ndf_country = df_country.groupby('NOC').max()\ndf_country.head(10)\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      Nation\n      Continent\n      Population\n      GDP\n    \n    \n      NOC\n      \n      \n      \n      \n      \n    \n  \n  \n    \n      AFG\n      0\n      Afghanistan\n      Asia\n      32890171\n      19807\n    \n    \n      ALB\n      1\n      Albania\n      Europe\n      2829741\n      14800\n    \n    \n      ALG\n      2\n      Algeria\n      Africa\n      45400000\n      145164\n    \n    \n      AND\n      3\n      Andorra\n      Europe\n      79535\n      3155\n    \n    \n      ANG\n      4\n      Angola\n      Africa\n      33086278\n      62307\n    \n    \n      ANT\n      5\n      Antigua\n      Americas\n      99337\n      1415\n    \n    \n      ANZ\n      7\n      Australia\n      Oceania\n      25921518\n      1330901\n    \n    \n      ARG\n      8\n      Argentina\n      Americas\n      47327407\n      383067\n    \n    \n      ARM\n      9\n      Armenia\n      Asia\n      2963900\n      12645\n    \n    \n      AUT\n      10\n      Austria\n      Europe\n      9027999\n      428965\n    \n  \n\n\n\n\n\ndf_by_year=\\\n     sqldf('SELECT                                                 \\\n              NOC,                                                 \\\n              Year,                                                \\\n              SUM(counta)              AS num_medals               \\\n                FROM(SELECT                                        \\\n                    NOC,                                           \\\n                    Year,                                          \\\n                    event_id,                                      \\\n                    Medal_Bronze, Medal_Gold, Medal_Silver,        \\\n                    COUNT(*) as counta                             \\\n                        FROM(SELECT                                \\\n                            NOC,                                   \\\n                            Year,                                  \\\n                            event_id,                              \\\n                            Medal_Bronze, Medal_Gold, Medal_Silver \\\n                        FROM                                       \\\n                            df_F_S                                 \\\n                        WHERE Medal_Bronze=1 OR Medal_Gold=1 OR Medal_Silver \\\n                        UNION                                      \\\n                        SELECT                                     \\\n                            NOC,                                   \\\n                            Year,                                  \\\n                            event_id,                              \\\n                            Medal_Bronze, Medal_Gold, Medal_Silver \\\n                        FROM                                       \\\n                            df_M_S                                 \\\n                        WHERE Medal_Bronze=1 OR Medal_Gold=1 OR Medal_Silver) as MF                  \\\n                    GROUP BY                                       \\\n                        NOC, Year, event_id, Medal_Bronze, Medal_Gold, Medal_Silver\\\n                    ORDER BY                                       \\\n                        NOC) as inner                              \\\n            GROUP BY                                               \\\n                NOC, Year;',locals()) \n\ndf_by_year[df_by_year.Year==2016].sort_values('num_medals',ascending=False).head(10)\n\n\n\n\n\n  \n    \n      \n      NOC\n      Year\n      num_medals\n    \n  \n  \n    \n      1246\n      USA\n      2016\n      121\n    \n    \n      246\n      CHN\n      2016\n      70\n    \n    \n      527\n      GBR\n      2016\n      67\n    \n    \n      415\n      EUN\n      2016\n      56\n    \n    \n      471\n      FRA\n      2016\n      42\n    \n    \n      497\n      FRG\n      2016\n      42\n    \n    \n      706\n      JPN\n      2016\n      41\n    \n    \n      38\n      ANZ\n      2016\n      29\n    \n    \n      669\n      ITA\n      2016\n      28\n    \n    \n      230\n      CAN\n      2016\n      22\n    \n  \n\n\n\n\n\n\nJoin with the country table\nNow just join to the country table\n\ndf_=\\\n     sqldf('SELECT                              \\\n              NOC,                              \\\n              Nation,                           \\\n              Continent,                        \\\n              SUM(num_medals)  AS number_of_medals,                  \\\n              Population,                       \\\n              GDP                               \\\n            FROM(                               \\\n                SELECT *                        \\\n                FROM                            \\\n                    df_country AS c             \\\n                INNER JOIN                      \\\n                    df_by_year AS d             \\\n                ON                              \\\n                    c.NOC=d.NOC                 \\\n                WHERE                           \\\n                    d.Year>=2008)               \\\n            GROUP BY                            \\\n                NOC,Continent                   \\\n            ORDER BY                            \\\n                num_medals desc;',locals())\n               \ndf_                    \n\n\n\n\n\n  \n    \n      \n      NOC\n      Nation\n      Continent\n      number_of_medals\n      Population\n      GDP\n    \n  \n  \n    \n      0\n      USA\n      USA\n      Americas\n      334\n      332906919\n      20936600\n    \n    \n      1\n      CHN\n      China\n      Asia\n      259\n      1412600000\n      14722731\n    \n    \n      2\n      EUN\n      Russia\n      Europe\n      210\n      147190000\n      1483498\n    \n    \n      3\n      GBR\n      UK\n      Europe\n      180\n      67081234\n      2707744\n    \n    \n      4\n      ANZ\n      Australia\n      Oceania\n      110\n      25921518\n      1330901\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      99\n      TUN\n      Tunisia\n      Africa\n      7\n      11746695\n      39236\n    \n    \n      100\n      UAE\n      United Arab Emirates\n      Asia\n      1\n      9282410\n      421142\n    \n    \n      101\n      UGA\n      Uganda\n      Africa\n      1\n      42885900\n      37372\n    \n    \n      102\n      VEN\n      Venezuela\n      Americas\n      5\n      28705000\n      482359\n    \n    \n      103\n      VIE\n      Vietnam\n      Asia\n      3\n      98505400\n      271158\n    \n  \n\n104 rows × 6 columns"
  },
  {
    "objectID": "posts/olympics/2022-07-29-OlympicsSQL_GDPpopulation.html#correlation-in-population-and-gdp",
    "href": "posts/olympics/2022-07-29-OlympicsSQL_GDPpopulation.html#correlation-in-population-and-gdp",
    "title": "Olympics data with SQL and pandas- GDP and population",
    "section": "Correlation in Population and GDP",
    "text": "Correlation in Population and GDP\nFirst create a function to display correlations: - produce scatter plots - find which countries are poor correlation - find some correlation stats\n\ndef modname(string):\n    string=''.join([string[0].upper(),string[1:].lower()])\n    string=string.replace('_',' ')\n    return string\ndef doFigAddOns(nomX,nomY,fontsize):\n        plt.ylabel(modname(nomY), fontsize=fontsize)\n        plt.xlabel(nomX, fontsize=fontsize)\n        plt.grid(True)\n        \ndef scatter_combo(df_temp,xval,yval):\n    \n    df_temp=df_temp.set_index('Nation')\n    fontsize=14\n    \n    df_temp=copy.copy(df_temp)\n    # remove 0 values\n    df_temp=df_temp[df_temp.loc[:,xval]!=0]\n    \n    # drop NaNs \n    df_temp=df_temp.dropna()\n    \n    # values in millions\n    df_temp.loc[:,xval]=df_temp.loc[:,xval]/1e6\n    \n    # the names of the columns\n    nomX=xval#(df_temp.columns[xval])\n    nomY=yval#modname(df_temp.columns[yval])\n    \n    # define the x and y values\n    X=df_temp.loc[:,xval]\n    Y=df_temp.loc[:,yval]\n    \n    # do some plots\n    fig,ax=plt.subplots(figsize=(7,15))\n    plt.rcParams['font.size'] = '13'\n    \n    # Fig A\n    ax1=plt.subplot(3,1,1)\n    ax1.plot(X ,Y,'+b',markeredgewidth=2,ms=10)\n    m,b=np.polyfit(X,Y,1)\n    Ypred=X*m+b\n    plt.plot(X,Ypred, '--k');\n    doFigAddOns(nomX,nomY,fontsize)\n    \n    \n    \n    # Fig B\n    ax3=plt.subplot(3,1,2)\n    ax3.plot(X ,Y,'+b',markeredgewidth=2,ms=10)\n    X=X.sort_values()\n    Ypred=X*m+b\n    plt.plot(X,Ypred, '--k');\n    plt.yscale('log')\n    plt.xscale('log')\n    doFigAddOns(nomX,nomY,fontsize) \n\n    # Fig C\n    diffa = df_temp.loc[:,yval]-Ypred\n    ax2=plt.subplot(3,1,3)\n    ax2.plot(X,diffa, 'xk',markeredgewidth=2,ms=10);\n    plt.xscale('log')\n    doFigAddOns(nomX,'Difference between \\nlinear fit',fontsize)\n\n    plt.subplots_adjust(hspace=0.35)\n    \n    # because sorted x, need to get original x back\n    X=df_temp.loc[:,xval]\n\n    # Get some correlation values, and what are worst countries\n    corr=scipy.stats.pearsonr(X, Y)\n    print('Correlation of  = {:.2f} ({:.0e}) \\n \\\n           And m= {:.3f} b={:.1f}          '.format(corr[0],corr[1],m,b))\n    \n    country_hi_corr=(diffa).sort_values(ascending=False).index[0:10]\n    print('Countries worst correlation where overachive',diffa[country_hi_corr])\n    \n    country_low_corr=(diffa).sort_values().index[0:10]\n    print('Countries worst correlation where unachieve',diffa[country_low_corr])\n\n    d={'Underachieve':country_low_corr,'Overachieve':country_hi_corr}\n    return pd.DataFrame(data=d)\n\ndef getCorr(df_temp, xval,yval):\n    \n    df_temp=copy.copy(df_temp)\n    # remove 0 values\n    df_temp=df_temp[df_temp.loc[:,xval]!=0]\n    \n    # drop NaNs \n    df_temp=df_temp.dropna()\n    \n    # define the x and y values\n    X=df_temp.loc[:,xval]\n    Y=df_temp.loc[:,yval]\n    \n    # Get some correlation values, and what are worst countries\n    corr=scipy.stats.pearsonr(X, Y)\n    print('Pearson Correlation of  = {:.2f} ({:.0e})'.format(corr[0],corr[1]))\n    # Get some correlation values, and what are worst countries\n    corr=scipy.stats.spearmanr(X, Y)\n    print('Spearman Correlation of  = {:.2f} ({:.0e})'.format(corr[0],corr[1]))\n\n\npopCorr=scatter_combo(df_,'GDP','number_of_medals')\npopCorr\n\nCorrelation of  = 0.84 (4e-29) \n            And m= 17.195 b=14.2          \nCountries worst correlation where overachive Nation\nRussia         170.270535\nUK             119.219670\nAustralia       72.894435\nFrance          59.020671\nGermany         47.334168\nUkraine         41.104002\nSouth Korea     37.742411\nItaly           36.341876\nCuba            33.005895\nKazakhstan      26.858922\ndtype: float64\nCountries worst correlation where unachieve Nation\nIndia                  -48.322884\nUSA                    -40.224834\nSaudi Arabia           -25.259275\nIndonesia              -22.420333\nUnited Arab Emirates   -20.462293\nPhilippines            -19.436562\nIsrael                 -18.132356\nMexico                 -17.725355\nAustria                -17.596809\nChile                  -17.570066\ndtype: float64\n\n\n\n\n\n\n  \n    \n      \n      Underachieve\n      Overachieve\n    \n  \n  \n    \n      0\n      India\n      Russia\n    \n    \n      1\n      USA\n      UK\n    \n    \n      2\n      Saudi Arabia\n      Australia\n    \n    \n      3\n      Indonesia\n      France\n    \n    \n      4\n      United Arab Emirates\n      Germany\n    \n    \n      5\n      Philippines\n      Ukraine\n    \n    \n      6\n      Israel\n      South Korea\n    \n    \n      7\n      Mexico\n      Italy\n    \n    \n      8\n      Austria\n      Cuba\n    \n    \n      9\n      Chile\n      Kazakhstan\n    \n  \n\n\n\n\n\n\n\n\nGDPCorr=scatter_combo(df_,'Population','number_of_medals')\nGDPCorr\n\nCorrelation of  = 0.42 (7e-06) \n            And m= 0.113 b=20.6          \nCountries worst correlation where overachive Nation\nUSA            275.692534\nRussia         172.705692\nUK             151.769692\nGermany         96.889859\nFrance          89.679993\nAustralia       86.426756\nChina           78.529401\nJapan           69.159606\nItaly           55.694604\nSouth Korea     53.504928\ndtype: float64\nCountries worst correlation where unachieve Nation\nIndia          -165.076115\nIndonesia       -41.444198\nNigeria         -40.164409\nPhilippines     -32.332446\nVietnam         -28.785832\nEgypt           -26.362309\nSudan           -24.687085\nUganda          -24.492699\nSaudi Arabia    -23.601957\nCameroon        -22.395235\ndtype: float64\n\n\n\n\n\n\n  \n    \n      \n      Underachieve\n      Overachieve\n    \n  \n  \n    \n      0\n      India\n      USA\n    \n    \n      1\n      Indonesia\n      Russia\n    \n    \n      2\n      Nigeria\n      UK\n    \n    \n      3\n      Philippines\n      Germany\n    \n    \n      4\n      Vietnam\n      France\n    \n    \n      5\n      Egypt\n      Australia\n    \n    \n      6\n      Sudan\n      China\n    \n    \n      7\n      Uganda\n      Japan\n    \n    \n      8\n      Saudi Arabia\n      Italy\n    \n    \n      9\n      Cameroon\n      South Korea\n    \n  \n\n\n\n\n\n\n\n\nscatter_combo(df_[df_['Continent']=='Europe'],'Population','number_of_medals')\n\nCorrelation of  = 0.93 (2e-16) \n            And m= 1.498 b=5.2          \nCountries worst correlation where overachive Nation\nUK                74.302826\nHungary           23.300286\nNetherlands       23.269039\nBelarus           19.808781\nDenmark           17.002417\nFrance            11.114933\nCroatia            9.991807\nCzech Republic     5.060035\nSlovenia           4.658317\nLithuania          4.630427\ndtype: float64\nCountries worst correlation where unachieve Nation\nPoland     -31.142356\nSpain      -24.255647\nPortugal   -16.682378\nRussia     -15.733456\nAustria    -14.709259\nRomania    -12.930475\nBelgium    -11.649525\nItaly      -10.448382\nGreece      -8.813758\nUkraine     -8.812069\ndtype: float64\n\n\n\n\n\n\n  \n    \n      \n      Underachieve\n      Overachieve\n    \n  \n  \n    \n      0\n      Poland\n      UK\n    \n    \n      1\n      Spain\n      Hungary\n    \n    \n      2\n      Portugal\n      Netherlands\n    \n    \n      3\n      Russia\n      Belarus\n    \n    \n      4\n      Austria\n      Denmark\n    \n    \n      5\n      Romania\n      France\n    \n    \n      6\n      Belgium\n      Croatia\n    \n    \n      7\n      Italy\n      Czech Republic\n    \n    \n      8\n      Greece\n      Slovenia\n    \n    \n      9\n      Ukraine\n      Lithuania\n    \n  \n\n\n\n\n\n\n\n\nscatter_combo(df_[df_['Continent']!='Europe'],'Population','number_of_medals')\n\nCorrelation of  = 0.49 (2e-05) \n            And m= 0.109 b=14.0          \nCountries worst correlation where overachive Nation\nUSA            283.666410\nAustralia       93.134297\nChina           90.957076\nJapan           76.277923\nSouth Korea     60.318993\nCanada          40.727568\nCuba            33.748715\nKazakhstan      27.859642\nNew Zealand     25.400672\nAzerbaijan      19.852149\ndtype: float64\nCountries worst correlation where unachieve Nation\nIndia          -152.808654\nIndonesia       -33.720543\nNigeria         -32.669700\nPhilippines     -25.269105\nVietnam         -21.778878\nEgypt           -19.334337\nSudan           -17.902477\nUganda          -17.715178\nSaudi Arabia    -16.856911\nCameroon        -15.694183\ndtype: float64\n\n\n\n\n\n\n  \n    \n      \n      Underachieve\n      Overachieve\n    \n  \n  \n    \n      0\n      India\n      USA\n    \n    \n      1\n      Indonesia\n      Australia\n    \n    \n      2\n      Nigeria\n      China\n    \n    \n      3\n      Philippines\n      Japan\n    \n    \n      4\n      Vietnam\n      South Korea\n    \n    \n      5\n      Egypt\n      Canada\n    \n    \n      6\n      Sudan\n      Cuba\n    \n    \n      7\n      Uganda\n      Kazakhstan\n    \n    \n      8\n      Saudi Arabia\n      New Zealand\n    \n    \n      9\n      Cameroon\n      Azerbaijan\n    \n  \n\n\n\n\n\n\n\n\nscatter_combo(df_[df_['Continent']=='Europe'],'GDP','number_of_medals')\n\nCorrelation of  = 0.80 (6e-09) \n            And m= 45.027 b=10.1          \nCountries worst correlation where overachive Nation\nRussia            133.114656\nUK                 47.990590\nUkraine            40.906665\nBelarus            26.198814\nHungary            25.932286\nCroatia             8.392025\nCzech Republic      4.946635\nDenmark             4.919196\nNetherlands         3.836572\nSerbia              2.527421\ndtype: float64\nCountries worst correlation where unachieve Nation\nGermany       -54.463232\nSwitzerland   -25.766716\nAustria       -25.402938\nBelgium       -22.291826\nIreland       -18.937224\nPortugal      -16.500704\nSpain         -15.776437\nFinland       -14.300792\nItaly         -12.028820\nSweden        -10.294891\ndtype: float64\n\n\n\n\n\n\n  \n    \n      \n      Underachieve\n      Overachieve\n    \n  \n  \n    \n      0\n      Germany\n      Russia\n    \n    \n      1\n      Switzerland\n      UK\n    \n    \n      2\n      Austria\n      Ukraine\n    \n    \n      3\n      Belgium\n      Belarus\n    \n    \n      4\n      Ireland\n      Hungary\n    \n    \n      5\n      Portugal\n      Croatia\n    \n    \n      6\n      Spain\n      Czech Republic\n    \n    \n      7\n      Finland\n      Denmark\n    \n    \n      8\n      Italy\n      Netherlands\n    \n    \n      9\n      Sweden\n      Serbia\n    \n  \n\n\n\n\n\n\n\n\nscatter_combo(df_[df_['Continent']!='Europe'],'GDP','number_of_medals')\n\nCorrelation of  = 0.94 (2e-33) \n            And m= 16.234 b=8.7          \nCountries worst correlation where overachive Nation\nAustralia      79.660038\nSouth Korea    44.796081\nCuba           38.591086\nKazakhstan     32.508245\nKenya          28.660696\nNew Zealand    27.815933\nAzerbaijan     25.573605\nCanada         23.586944\nJamaica        23.041050\nBrazil         19.812142\ndtype: float64\nCountries worst correlation where unachieve Nation\nIndia                  -40.315038\nSaudi Arabia           -19.100124\nIndonesia              -15.916697\nUSA                    -14.609781\nUnited Arab Emirates   -14.571357\nPhilippines            -13.602977\nIsrael                 -12.259868\nChile                  -11.840843\nVenezuela              -11.565125\nMexico                 -11.204664\ndtype: float64\n\n\n\n\n\n\n  \n    \n      \n      Underachieve\n      Overachieve\n    \n  \n  \n    \n      0\n      India\n      Australia\n    \n    \n      1\n      Saudi Arabia\n      South Korea\n    \n    \n      2\n      Indonesia\n      Cuba\n    \n    \n      3\n      USA\n      Kazakhstan\n    \n    \n      4\n      United Arab Emirates\n      Kenya\n    \n    \n      5\n      Philippines\n      New Zealand\n    \n    \n      6\n      Israel\n      Azerbaijan\n    \n    \n      7\n      Chile\n      Canada\n    \n    \n      8\n      Venezuela\n      Jamaica\n    \n    \n      9\n      Mexico\n      Brazil\n    \n  \n\n\n\n\n\n\n\n\ndf_GDP=copy.copy(df_[['Nation','Continent','GDP']])\n\ndf_GDP = df_GDP.sort_values('GDP',ascending=False)\ndf_GDP\nrich_list = df_GDP.head(52).index\n# # index_not_rich=[i for i,country in enumerate(df_.index) if (country not in rich_list) and (df_.iloc[i,2]!='Europe') and (df_.index[i]!='India')]\n# # index_rich=[i for i,country in enumerate(df_.index) if ( (df_.iloc[i,2]!='Europe') )]\nindex_rich=[i for i,country in enumerate(df_.index) if (country in rich_list) ]\nindex_not_rich=[i for i,country in enumerate(df_.index) if (country not in rich_list) ]\n\nprint('         Index Rich- Population')\ngetCorr(df_.iloc[index_rich,:],'Population','number_of_medals')\nprint('         Index Not Rich- Population')\ngetCorr(df_.iloc[index_not_rich,:],'Population','number_of_medals')\nprint('         Index Rich- GDP')\ngetCorr(df_.iloc[index_rich,:],'GDP','number_of_medals')\nprint('         Index Not Rich- GDP')\ngetCorr(df_.iloc[index_not_rich,:],'GDP','number_of_medals')\n\n         Index Rich- Population\nPearson Correlation of  = 0.37 (6e-03)\nSpearman Correlation of  = 0.30 (3e-02)\n         Index Not Rich- Population\nPearson Correlation of  = 0.17 (2e-01)\nSpearman Correlation of  = 0.15 (3e-01)\n         Index Rich- GDP\nPearson Correlation of  = 0.84 (7e-15)\nSpearman Correlation of  = 0.50 (2e-04)\n         Index Not Rich- GDP\nPearson Correlation of  = 0.32 (2e-02)\nSpearman Correlation of  = 0.38 (6e-03)\n\n\n\nprint('         Index Europe- GDP')\ngetCorr(df_[df_['Continent']=='Europe'],'GDP','number_of_medals')\nprint('         Index Not Europe- GDP')\ngetCorr(df_[df_['Continent']!='Europe'],'GDP','number_of_medals')\n\nprint('         Index Europe- Population')\ngetCorr(df_[df_['Continent']=='Europe'],'Population','number_of_medals')\nprint('         Index Not Europe- Population')\ngetCorr(df_[df_['Continent']!='Europe'],'Population','number_of_medals')\n\n         Index Europe- GDP\nPearson Correlation of  = 0.80 (6e-09)\nSpearman Correlation of  = 0.71 (1e-06)\n         Index Not Europe- GDP\nPearson Correlation of  = 0.94 (2e-33)\nSpearman Correlation of  = 0.51 (1e-05)\n         Index Europe- Population\nPearson Correlation of  = 0.93 (2e-16)\nSpearman Correlation of  = 0.81 (3e-09)\n         Index Not Europe- Population\nPearson Correlation of  = 0.49 (2e-05)\nSpearman Correlation of  = 0.42 (3e-04)\n\n\n\nprint('GDP'),getCorr(df_,'GDP','number_of_medals')\nprint('Population'),getCorr(df_,'Population','number_of_medals')\n\nGDP\nPearson Correlation of  = 0.84 (4e-29)\nSpearman Correlation of  = 0.60 (2e-11)\nPopulation\nPearson Correlation of  = 0.42 (7e-06)\nSpearman Correlation of  = 0.41 (1e-05)\n\n\n(None, None)"
  },
  {
    "objectID": "posts/olympics/2022-07-29-OlympicsSQL_GDPpopulation.html#overview-of-gdp-and-population",
    "href": "posts/olympics/2022-07-29-OlympicsSQL_GDPpopulation.html#overview-of-gdp-and-population",
    "title": "Olympics data with SQL and pandas- GDP and population",
    "section": "Overview of GDP and population",
    "text": "Overview of GDP and population\n\nHypothesis: GDP is correlated with Medals, and Population is too but less so\n\n\nMethods Used\nLooking at number of medals against GDP and population, for a period from 2008-2016 inclusive. Using population and GDP data from ~2020. - scatter plots - to visualize the relationships between medals and GDP, and medals and population. Both show a correlation - Pearson & Spearman correlation - To quatify the correlation seen. Pearson looks for a linear relationship whereas Spearman considers the ordering of the variables. The Pearson results showed a moderate to strong relationship for GDP and a weak to moderate relationship for population. The Spearman results were similar (more correlation for GDP) but the correlation values were lower. - I considered looking at the changes with time, but there were a lot of factors affecting country participation and the data was harder to obtain easily\n\n\nOverview\nResults were mainly as expected and hence PROVED.\n\nGDP\n\n\nALL = strong AND significant\nEurope = strong AND significant\nNOT Europe = very strong AND significant\n\n\nPopulation\n\n\nALL = moderate AND significant\nEurope = very strong AND significant\nNOT Europe = moderate AND significant\n\nUsing a line fit of the data (GDP or Population to Medals) gives an indication of which countries are reducing the correlation. If the country has more medals than the fit then it is overachieve and if it has less it is underachieveing. The table below shows which countries are under and overachieveing for GDP and population.\n\n\nGDPCorr.rename(columns={'Overachieve':'GDP - Overachieve','Underachieve':'GDP - Underachieve'},inplace=True)\npopCorr.rename(columns={'Overachieve':'Population - Overachieve','Underachieve':'Population - Underachieve'},inplace=True)\n\ncorrComb=pd.concat([GDPCorr,popCorr],axis=1)\ncorrComb\n\n\n\n\n\n  \n    \n      \n      GDP - Underachieve\n      GDP - Overachieve\n      Population - Underachieve\n      Population - Overachieve\n    \n  \n  \n    \n      0\n      India\n      USA\n      India\n      Russia\n    \n    \n      1\n      Indonesia\n      Russia\n      USA\n      UK\n    \n    \n      2\n      Nigeria\n      UK\n      Saudi Arabia\n      Australia\n    \n    \n      3\n      Philippines\n      Germany\n      Indonesia\n      France\n    \n    \n      4\n      Vietnam\n      France\n      United Arab Emirates\n      Germany\n    \n    \n      5\n      Egypt\n      Australia\n      Philippines\n      Ukraine\n    \n    \n      6\n      Sudan\n      China\n      Israel\n      South Korea\n    \n    \n      7\n      Uganda\n      Japan\n      Mexico\n      Italy\n    \n    \n      8\n      Saudi Arabia\n      Italy\n      Austria\n      Cuba\n    \n    \n      9\n      Cameroon\n      South Korea\n      Chile\n      Kazakhstan\n    \n  \n\n\n\n\nSo based on the above it may make sense to group countries based on their GDP, into a rich and poor list. But as shown above this doesn’t increase the correlation for either group.\nThe only useful metric to increase the correlation was to group the countries into those from Europe. This is probably due to the similarity of countries within Europe (and outside): size, GDP but also culturally; and it may be because the European countries have much greater participation at the Olympics (both over currently and historically).\n\n\nGuide on Correlation\n\npd.DataFrame({'Correlation':['1','0.8 - 1.0','0.6 - 0.8','0.4 - 0.6','0.2 - 0.4','0 - 0.2'],\\\n   'Interpretation':['Pefect','Strong to Perfect','Moderate to Very Strong','Moderate to strong','Weak to moderate','Zero to weak']})\n\n\n\n\n\n\n  \n    \n      \n      Correlation\n      Interpretation\n    \n  \n  \n    \n      0\n      1\n      Pefect\n    \n    \n      1\n      0.8 - 1.0\n      Strong to Perfect\n    \n    \n      2\n      0.6 - 0.8\n      Moderate to Very Strong\n    \n    \n      3\n      0.4 - 0.6\n      Moderate to strong\n    \n    \n      4\n      0.2 - 0.4\n      Weak to moderate\n    \n    \n      5\n      0 - 0.2\n      Zero to weak\n    \n  \n\n\n\n\nTo determine whether the correlation between variables is significant, compare the p-value to your significance level. Usually, a significance level (denoted as α or alpha) of 0.05 works well. An α of 0.05 (5e-2) indicates that the risk of concluding that a correlation exists—when, actually, no correlation exists—is 5%. The p-value tells you whether the correlation coefficient is significantly different from 0. (A coefficient of 0 indicates that there is no linear relationship.)\n\n- P-value ≤ α (5e-2): The correlation is statistically significant\n    If the p-value is less than or equal to the significance level, then you can conclude that the correlation is different from 0. \n- P-value > α (5e-2): The correlation is not statistically significant\n    If the p-value is greater than the significance level, then you cannot conclude that the correlation is different from 0. \n    \nhttps://www.ncbi.nlm.nih.gov/pmc/articles/PMC6107969/\nhttps://support.minitab.com/en-us/minitab-express/1/help-and-how-to/modeling-statistics/regression/how-to/correlation/interpret-the-results/"
  },
  {
    "objectID": "posts/olympics/2022-07-29-OlympicsSQL_GDPpopulation.html#number-of-athletes",
    "href": "posts/olympics/2022-07-29-OlympicsSQL_GDPpopulation.html#number-of-athletes",
    "title": "Olympics data with SQL and pandas- GDP and population",
    "section": "Number of athletes",
    "text": "Number of athletes\nA metric that would make sense to correlate with medals would be the number of athletes each nation sends to an Olympics. This is because, for the most part, participation is done on merit. That is athletes have to qualify against athletes from other nations.\nSo the metrics we want to look at are: - the number of athletes per nation attending a particular games - the number of medals per nation per games\nNB I will just use male athletes for simplicity\n\nChange in average athletes per continent\nFirst, let us look if the greater number of athletes from Europe is also reflected in the average number of athletes each nation sends within each continent.\nAs shown below European countries send around twice as many athletes per nation as other continents.\n\ntempa= sqldf('SELECT                                     \\\n            Year,                                        \\\n             Continent,                                  \\\n             AVG(numbers)  as ath_per_nation             \\\n        FROM(SELECT                                      \\\n            c.Continent,                                 \\\n            a.Year,\\\n            COUNT(*)           AS numbers                \\\n            FROM                                         \\\n                df_country as c                          \\\n            LEFT JOIN                                    \\\n                df_M_S as a                              \\\n            ON                                           \\\n                a.NOC = c.NOC                            \\\n            GROUP BY                                     \\\n                a.Year,c.Continent,c.Nation              \\\n            ORDER BY                                     \\\n                Year asc) A                              \\\n            GROUP BY                                     \\\n                 Year,Continent;',locals())\n\nplt.subplots(figsize=(8,5))\ncola=['g>--','<r-.','bo-','mv-','kp:']\nfor i,continent in enumerate(tempa.Continent.unique()):\n    x=tempa[tempa.Continent==continent]['Year']\n    y=tempa[tempa.Continent==continent]['ath_per_nation']\n    plt.plot(x,y,cola[i]);\nplt.legend(tempa.Continent.unique());\nplt.ylim([-5,200])\nplt.grid(True)\nplt.xlabel('Year')\nplt.ylabel('Number of male athletes\\n per nation');\n\n\n\n\n\n\nAthletes per nation VS medals per nation\nTo look at this correlation we need to do the following steps: - Join athletes with country tables - Group by Nation and whether they have a medal - Count this, this will give number of events with a medal - Then need to remove nations that didn’t get any medals - Get scatter and correlation of these - Group by continent and do the same\n\ntempa= sqldf('SELECT                                     \\\n             Continent,                                  \\\n             Nation,                                     \\\n             Medal,                                      \\\n             SUM(numbers)   AS numbers                   \\\n        FROM(SELECT                                      \\\n            c.Continent,                                 \\\n            c.Nation,                                    \\\n            a.Medal,                                     \\\n            COUNT(*)           AS numbers                \\\n            FROM                                         \\\n                df_country as c                          \\\n            LEFT JOIN                                    \\\n                df_M_S as a                              \\\n            ON                                           \\\n                a.NOC = c.NOC                            \\\n            WHERE Year>2003                              \\\n            GROUP BY                                     \\\n                c.Continent,c.Nation,a.Medal_Gold,a.Medal_Silver,a.Medal_Bronze) A          \\\n            GROUP BY                                     \\\n                Continent, Nation,Medal;',locals())\n\ntempa2= sqldf('SELECT                                     \\\n             Nation,                                     \\\n             COUNT(numbers)   AS numbers                 \\\n        FROM(SELECT                                      \\\n            a.athlete_ID,                                \\\n            c.Nation,                                    \\\n            COUNT(*)           AS numbers                \\\n            FROM                                         \\\n                df_country as c                          \\\n            LEFT JOIN                                    \\\n                df_F_S as a                              \\\n            ON                                           \\\n                a.NOC = c.NOC                            \\\n            WHERE Year=2016                              \\\n            GROUP BY                                     \\\n                c.NOC,a.athlete_ID,a.Year) A             \\\n            GROUP BY                                     \\\n                Nation;',locals())\n\ntempa3= tempa[tempa.Medal==1]\ntempa4=sqldf('\\\n    SELECT A.Nation,A.numbers as num_medals,B.numbers as num_ath                 \\\n    FROM tempa3 as A                              \\\n    LEFT JOIN tempa2 as B                         \\\n    ON A.Nation=B.Nation                          \\\n;',locals())\ntempa4\n\n\n\n\n\n  \n    \n      \n      Nation\n      num_medals\n      num_ath\n    \n  \n  \n    \n      0\n      Algeria\n      4\n      10.0\n    \n    \n      1\n      Botswana\n      1\n      3.0\n    \n    \n      2\n      Egypt\n      9\n      37.0\n    \n    \n      3\n      Eritrea\n      1\n      1.0\n    \n    \n      4\n      Ethiopia\n      12\n      20.0\n    \n    \n      ...\n      ...\n      ...\n      ...\n    \n    \n      95\n      UK\n      245\n      159.0\n    \n    \n      96\n      Ukraine\n      51\n      117.0\n    \n    \n      97\n      Australia\n      251\n      212.0\n    \n    \n      98\n      Fiji\n      13\n      17.0\n    \n    \n      99\n      New Zealand\n      44\n      97.0\n    \n  \n\n100 rows × 3 columns\n\n\n\n\ntempa4=tempa4.dropna()\nx=tempa4.num_ath\ny=tempa4.num_medals\n\nplt.subplots(figsize=(8,5))\nplt.scatter(x,y)\na=scipy.stats.spearmanr(x,y)\nb=scipy.stats.pearsonr(x,y)\nprint('Spearman correlation = {:.2f} ({:.0e}) \\nand Pearson correlation= {:.2f} ({:.0e})'\\\n      .format(a[0],a[1],b[0],b[1]))\n\nplt.grid(True)\nplt.xlabel('Athletes per nation')\nplt.ylabel('Medals per nation');\n\np=np.poly1d( np.polyfit(x,y,2) )\nxx=np.arange(0,300,2)\nyy=p(np.arange(0,300,2))\n\nplt.plot(xx,yy);\n\nSpearman correlation = 0.83 (3e-26) \nand Pearson correlation= 0.87 (5e-32)\n\n\n\n\n\n\n\nOverview Medals VS Athletes per nation\nAs was expected there is a good correlation between the number of athletes a nation sends and the number of medals they get\nWhat may also have been expected and shown in the data, is that the relationship is not linear. Instead the more athletes a nation sends the greater the medals/athlete ratio.\ni.e. If a nation sends more athletes it is more likely that a higher proportion of them will win medals"
  },
  {
    "objectID": "posts/olympics/2022-07-29-OlympicsSQL_GDPpopulation.html#cold-war",
    "href": "posts/olympics/2022-07-29-OlympicsSQL_GDPpopulation.html#cold-war",
    "title": "Olympics data with SQL and pandas- GDP and population",
    "section": "Cold War",
    "text": "Cold War\nThe Cold War was a period of geopolitical tension between the United States and the Soviet Union and their respective allies, the Western Bloc and the Eastern Bloc, which began following World War II and ended in the early 1990s.\nThe conflict was based around the ideological and geopolitical struggle for global influence by these two superpowers.\nThe Soviet Union competed at the Olympics from 1952-1988. The Russian Empire had previously competed at the 1900, 1908 and 1912 Olympics games. In these games the best they ranked was 12th. In contrast the USA competed from the start of the Olympics and all subsequent games. In 1952 they were the most succesful nation, coming 1st in the medals table in 8 out of the previous 11 games (and second in the other 3).\nThe figures below show how during the Cold War period, The Soviet Union was able to compete with USA and in some cases beat them in the medals table. After this Cold War period the medals obtained by both the USA and the Soviet Union fell with respect to the totals from the European nations. Furthermore, USA reasserted it’s dominance after the Cold War period.\nWikipedia Cold War\n\ndef number_of_athletes_USA_USSR(df_F_S,df_M_S):\n    testa2=sqldf('\\\n        SELECT                                 \\\n        Year,                              \\\n        NOCSMALL,                          \\\n        count(*) AS number_of_athletes,     \\\n        \"F\" AS Sex                          \\\n    FROM                                   \\\n         (SELECT                           \\\n         athlete_ID,                        \\\n         Year,                             \\\n         CASE                                \\\n            WHEN NOC IN (\"FRA\",\"ESP\",\"ITA\",\"POR\",\"GBR\",\"IRL\",\"NED\",\"BEL\",\"DEN\",\"SUI\") THEN \"WES\"\\\n            WHEN NOC IN (\"POL\",\"ROU\",\"UKR\",\"LAT\",\"BUL\",\"HUN\",\"LTU\",\"LAT\",\"BLR\",\"ALB\",\"SVK\",\"AUT\",\"EST\",\"BIH\",\"BOH\") THEN \"EST\"\\\n            WHEN NOC=\"USA\" THEN \"USA\"       \\\n            WHEN NOC=\"EUN\" THEN \"EUN\"       \\\n            ELSE \"ROW\"\\\n            END AS NOCSMALL                \\\n         from df_F_S                       \\\n         group by athlete_ID,Year               \\\n         order by Year asc) A              \\\n     GROUP BY Year, NOCSMALL               \\\n     UNION ALL                                 \\\n     SELECT                                \\\n        Year,                              \\\n        NOCSMALL,                          \\\n        count(*) AS number_of_athletes,     \\\n        \"M\" AS Sex                          \\\n    FROM                                   \\\n         (SELECT                           \\\n         athlete_ID,                        \\\n         Year,                             \\\n         CASE                                \\\n            WHEN NOC IN (\"FRA\",\"ESP\",\"ITA\",\"POR\",\"GBR\",\"IRL\",\"NED\",\"BEL\",\"DEN\",\"SUI\") THEN \"WES\"\\\n            WHEN NOC IN (\"POL\",\"ROU\",\"UKR\",\"LAT\",\"BUL\",\"HUN\",\"LTU\",\"LAT\",\"BLR\",\"ALB\",\"SVK\",\"AUT\",\"EST\",\"BIH\",\"BOH\") THEN \"EST\"\\\n            WHEN NOC=\"USA\" THEN \"USA\"       \\\n            WHEN NOC=\"EUN\" THEN \"EUN\"       \\\n            ELSE \"ROW\"\\\n            END AS NOCSMALL                \\\n         from df_M_S                       \\\n         group by athlete_ID,Year          \\\n         order by Year asc) A              \\\n     GROUP BY Year, NOCSMALL;',locals()  )\n    return testa2\n\ndef number_of_medals_USA_USSR(df_F_S,df_M_S):\n    testa2=sqldf('\\\n        SELECT                                 \\\n            COUNT(*) AS number_of_medals,\\\n            Year, Sex, NOCSMALL\\\n        FROM \\\n        (SELECT NOCSMALL,Year,Sex,COUNT(*) AS counta\\\n        FROM                                   \\\n         (SELECT                           \\\n         athlete_ID,                        \\\n         event_ID,                          \\\n         \"F\"   AS Sex,                      \\\n         Medal_Gold,Medal_Silver,Medal_Bronze,\\\n         Year,                             \\\n         CASE                                \\\n            WHEN NOC IN (\"FRA\",\"ESP\",\"ITA\",\"POR\",\"GBR\",\"IRL\",\"NED\",\"BEL\",\"DEN\",\"SUI\") THEN \"WES\"\\\n            WHEN NOC IN (\"POL\",\"ROU\",\"UKR\",\"LAT\",\"BUL\",\"HUN\",\"LTU\",\"LAT\",\"BLR\",\"ALB\",\"SVK\",\"AUT\",\"EST\",\"BIH\",\"BOH\") THEN \"EST\"\\\n            WHEN NOC=\"USA\" THEN \"USA\"       \\\n            WHEN NOC=\"EUN\" THEN \"EUN\"       \\\n            ELSE \"ROW\"\\\n            END AS NOCSMALL                \\\n         from df_F_S                       \\\n         WHERE Medal_Gold=1 OR Medal_Silver=1 OR Medal_Bronze=1\\\n         UNION ALL                                 \\\n         SELECT                           \\\n         athlete_ID,                        \\\n         event_ID,                          \\\n         \"M\" AS Sex,                       \\\n         Medal_Gold,Medal_Silver,Medal_Bronze,\\\n         Year,                             \\\n         CASE                                \\\n            WHEN NOC IN (\"FRA\",\"ESP\",\"ITA\",\"POR\",\"GBR\",\"IRL\",\"NED\",\"BEL\",\"DEN\",\"SUI\") THEN \"WES\"\\\n            WHEN NOC IN (\"POL\",\"ROU\",\"UKR\",\"LAT\",\"BUL\",\"HUN\",\"LTU\",\"LAT\",\"BLR\",\"ALB\",\"SVK\",\"AUT\",\"EST\",\"BIH\",\"BOH\") THEN \"EST\"\\\n            WHEN NOC=\"USA\" THEN \"USA\"       \\\n            WHEN NOC=\"EUN\" THEN \"EUN\"       \\\n            ELSE \"ROW\"\\\n            END AS NOCSMALL                \\\n         from df_M_S                       \\\n         WHERE Medal_Gold=1 OR Medal_Silver=1 OR Medal_Bronze=1\\\n         order by Year asc) A\\\n     GROUP BY \\\n         Year, NOCSMALL,event_id,Medal_Gold,Medal_Silver,Medal_Bronze)  AS B\\\n GROUP BY Year, NOCSMALL, Sex\\\n                 ;',locals()  )                                       \n    return testa2\n\n\nUSA_USSR_medals=number_of_medals_USA_USSR(df_F_S,df_M_S)\nUSA_USSR_athletes=number_of_athletes_USA_USSR(df_F_S,df_M_S)\n\n\ndef yrplot(df__,whatplot= 'avg_weight'): \n    \n    countries=['EST', 'EUN' ,'ROW', 'USA' ,'WES']\n    \n#     df__.NOCSMALL.unique()\n#     countries=np.sort(countries)\n    print(countries)\n    cola=['>','o','+','*','<']\n    colur=[[1,0.6,.6],[1,0,0],[.5,.5,.5],[0,0,1],[.6,.6,1]]\n#     ['EST' 'EUN' 'ROW' 'USA' 'WES']\n#     'EST','USA','WES','ROW','EUN'\n\n    fig,ax1=plt.subplots(figsize=(8,5))\n    \n    for i,country in enumerate(countries):\n        if country!='ROW':\n            ax1.plot(df__[df__.NOCSMALL==country].Year,\\\n                 df__[df__.NOCSMALL==country][whatplot],\\\n                 marker=cola[i],linestyle='None',color=colur[i]\\\n                 ,markersize=10)\n\n\n    def doPlot(df_F,avgNo,whatplot,country,col,lw,ax1):\n        bb = df_F[df__.NOCSMALL==country].Year.rolling(avgNo).mean()\n        cc = df_F[df__.NOCSMALL==country][whatplot]\n        cc = cc.rolling(avgNo).mean()\n        ax1.plot(bb,cc,linewidth=lw,color=col)\n        return ax1\n\n    for i,country in enumerate(countries):\n        if country!='ROW':\n            ax1=doPlot(df__,avgNo=3,whatplot=whatplot,country=country,col=colur[i],lw=3,ax1=ax1)\n    \n    lega = ['East Europe','Russia','USA','West Europe']\n    plt.legend(lega)\n    plt.grid(True)\n    plt.ylabel(modname(whatplot))\n    \n    return ax1\n\n\nyrplot(USA_USSR_medals[USA_USSR_medals.Sex=='F'],whatplot= 'number_of_medals')\n\n\n\nyrplot(USA_USSR_medals[USA_USSR_medals.Sex=='M'],whatplot= 'number_of_medals')"
  },
  {
    "objectID": "posts/olympics/2022-07-29-Olympics_HomeEvent.html#overview",
    "href": "posts/olympics/2022-07-29-Olympics_HomeEvent.html#overview",
    "title": "Olympics data with SQL and pandas- home games",
    "section": "Overview",
    "text": "Overview\nThis is part of a project that looks at an Olympics dataset.\nIn this part the hypothesis considered is:\nAt a home Olympic games a nation will on average obtain more medals than at other games\n\nBut can we quantify this effect?\nAre there any residual effects before and after the games?\nWhat about a home continent games?"
  },
  {
    "objectID": "posts/olympics/2022-07-29-Olympics_HomeEvent.html#methodology",
    "href": "posts/olympics/2022-07-29-Olympics_HomeEvent.html#methodology",
    "title": "Olympics data with SQL and pandas- home games",
    "section": "Methodology",
    "text": "Methodology\nTo be able to answer the question, the steps taken are as follows. Note the data and tables created in a previous page.\n\nCreate table of medal athletes as athlete\n\nMedals\nYear\nNOC\nOthers? Sex, number_of_athletes\n\nJoin athlete table to country table to get the country added to athlete\n\nNow the next step is trickier. There are several ways we could look at the effect of a home game and the nearness to it. The way I like to do this is to visualise the plot(s) I would like and work back from that.\nThe plot I’d like is: - x-axis = year from games (0 = at the games, +ve after and -ve before) - y-axis = number of medals\nIf we normalise the y-axis we can put all the events together to get an effect of games, before and after. We can also pick out individual games or do the averageing on different time-periods.\nSo to achieve this we need a vector for each games of: - Year from games - Number of medals\nThere are a number of potential problems with this: 1. If a nation holds two games close to each other 1. Normally games are every 4 years but how do we deal with exceptions to this? 1. What do we do when there is a lack of data before or after the games? e.g. games at the start of Olympics or current games, or if a nation stops partipation 1. When looking at the effect of continent, the approach would need to be adjusted based on there being fewer continents and most games being in Europe\n\ncan be solved by reducing the times to +- 20 years, and because we are averageing any exceptions should be covered\na way to solve this is to fit the data across the years/medals data we have with a function then use the function to give us values on a set scale (e.g. -20 to 20 in steps of 4 yrs)\na bit trickier, may we fit values that have positive and negative values up to 20 yrs first. Then if an event has a gap in years fill with the average values below and above 0 yrs\nfor this maybe just look at a partcular game based on recent games that are not in the same continent"
  },
  {
    "objectID": "posts/olympics/2022-07-29-Olympics_HomeEvent.html#some-starting-code",
    "href": "posts/olympics/2022-07-29-Olympics_HomeEvent.html#some-starting-code",
    "title": "Olympics data with SQL and pandas- home games",
    "section": "Some Starting Code",
    "text": "Some Starting Code\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom pandasql import sqldf\nimport copy\nimport numpy as np\nimport scipy.stats\n\n\ndf_F_S =pd.read_csv('data/athlete_F_S')\ndf_F_W=pd.read_csv('data/athlete_F_W')\ndf_M_S=pd.read_csv('data/athlete_M_S')\ndf_M_W=pd.read_csv('data/athlete_M_W')\n\ndf_all_athletes= pd.read_csv('data/all_athletes')\ndf_country= pd.read_csv('data/country')\n# df_event= pd.read_csv('data/event')\ndf_games= pd.read_csv('data/games')\n# df_population= pd.read_csv('data/population')\n\ndf_country = df_country.groupby('NOC').max()\n# df_country.head(10)\n\n\ndf_games.head()\n\n\n\n\n\n  \n    \n      \n      Unnamed: 0\n      games_id\n      City\n      Country\n      Year\n      Region\n      Summer\n      Winter\n      Latitude\n      Longitude\n    \n  \n  \n    \n      0\n      0\n      0\n      Athens\n      Greece\n      1896\n      Europe\n      1\n      0\n      37.9838\n      23.7275\n    \n    \n      1\n      1\n      1\n      Paris\n      France\n      1900\n      Europe\n      1\n      0\n      48.8566\n      2.3522\n    \n    \n      2\n      2\n      2\n      St. Louis\n      USA\n      1904\n      North America\n      1\n      0\n      38.6270\n      90.1994\n    \n    \n      3\n      3\n      3\n      Athens\n      Greece\n      1906\n      Europe\n      1\n      0\n      37.9838\n      23.7275\n    \n    \n      4\n      4\n      4\n      London\n      UK\n      1908\n      Europe\n      1\n      0\n      51.5072\n      0.1276"
  },
  {
    "objectID": "posts/olympics/2022-07-29-Olympics_HomeEvent.html#create-a-table-of-country-medals",
    "href": "posts/olympics/2022-07-29-Olympics_HomeEvent.html#create-a-table-of-country-medals",
    "title": "Olympics data with SQL and pandas- home games",
    "section": "Create a table of Country & Medals",
    "text": "Create a table of Country & Medals\nThis first step is to create a table of athletes data that includes both women and men athletes from the summer games, we want the following columns: - The Olympic Year - The Nation - The Continent of the Nation - The number of medals the nation got that year\nBasically just UNION to join the male and female athletes- with some additions so we get the total number of medals per country\nFollowed by a JOIN on the country table to get more information about the countries.\n\ndef number_of_medals(df_F_S,df_M_S):\n    testa2=sqldf('\\\n        SELECT                                                         \\\n            COUNT(*)              AS number_of_medals,                 \\\n            Year, NOC                                                  \\\n        FROM                                                           \\\n        (SELECT NOC,Year,COUNT(*) AS counta                            \\\n           FROM                                                        \\\n             (SELECT                                                   \\\n             athlete_ID,                                               \\\n             event_ID,                                                 \\\n             Medal_Gold,Medal_Silver,Medal_Bronze,                     \\\n             Year,                                                     \\\n             NOC                                                       \\\n             from df_F_S                                               \\\n             WHERE Medal_Gold=1 OR Medal_Silver=1 OR Medal_Bronze=1    \\\n             UNION ALL                                                 \\\n             SELECT                                                    \\\n             athlete_ID,                                               \\\n             event_ID,                                                 \\\n             Medal_Gold,Medal_Silver,Medal_Bronze,                     \\\n             Year,                                                     \\\n             NOC                                                       \\\n             from df_M_S                                               \\\n             WHERE Medal_Gold=1 OR Medal_Silver=1 OR Medal_Bronze=1    \\\n             order by Year asc) A                                      \\\n           GROUP BY                                                    \\\n             Year, NOC,event_id,Medal_Gold,Medal_Silver,Medal_Bronze)  AS B\\\n         GROUP BY Year, NOC                                            \\\n                 ;',locals()  )                                       \n    return testa2\n\ndef join_country(df_,df_country):\n    testa2=sqldf('\\\n        SELECT                             \\\n            c.NOC,c.Nation,c.Continent,    \\\n            a.number_of_medals,            \\\n            a.Year                         \\\n        FROM                               \\\n            df_ AS a                       \\\n        INNER JOIN                         \\\n            df_country AS c                \\\n        ON                                 \\\n            c.NOC=a.NOC                    \\\n        GROUP BY                           \\\n            c.NOC,a.Year                   \\\n     ;',locals()  ) \n    return testa2\n\ndf_medals=number_of_medals(df_F_S,df_M_S)\ndf_medals2=join_country(df_medals,df_country)\ndf_medals2.head()\n\n\n\n\n\n  \n    \n      \n      NOC\n      Nation\n      Continent\n      number_of_medals\n      Year\n    \n  \n  \n    \n      0\n      AFG\n      Afghanistan\n      Asia\n      1\n      2008\n    \n    \n      1\n      AFG\n      Afghanistan\n      Asia\n      1\n      2012\n    \n    \n      2\n      ALG\n      Algeria\n      Africa\n      2\n      1984\n    \n    \n      3\n      ALG\n      Algeria\n      Africa\n      2\n      1992\n    \n    \n      4\n      ALG\n      Algeria\n      Africa\n      3\n      1996"
  },
  {
    "objectID": "posts/olympics/2022-07-29-Olympics_HomeEvent.html#find-the-change-in-medals-around-a-home-games",
    "href": "posts/olympics/2022-07-29-Olympics_HomeEvent.html#find-the-change-in-medals-around-a-home-games",
    "title": "Olympics data with SQL and pandas- home games",
    "section": "Find the change in medals around a home games",
    "text": "Find the change in medals around a home games\nThe easy solution here would be to take an average of the medals a country obtains obtains at a home games versus non-home games. However, this wouldn’t work mainly due to the changes in medals with time (they increase).\nWe could correct for overall changes in the number of medals by normalising based on the total number of medals in a games. This would be better, but we may need an extra step to account for the lack of diversity of nations in earlier years too.\nBut also we wouldn’t be able to see what effect a home games had before and after the games were held.\nTo observe how medal count is effected before and after a games, and to somewhat reduce the increase in medals with time, the following steps are taken.\n\nScoll through the games table for each summer games\nFor each game add data to the following variables (of length of the number of games) for each particular games\n\nnation the nation hosting the particular games\nmedals the number of medals for each games for the nation hosting the particular games, in all the other games\nyears the difference in years between the particular games and all other games\n\n\nFor example:\nUSA 1984, Los Angeles\n\nnation[20] is ‘USA’\nyears[20] is [-88, -84, -80, -78, -76, -72, -64, -60, -56, -52, -48, -36, -32,-28, -24, -20, -16, -12,  -8,   0,   4,   8,  12,  16,  20,  24, 28,  32]\n\nthe 0 being the 1984 games, and the 32 being the 2016 games\n\nmedals[20] is [ 19,  54, 230,  23,  46,  63,  95,  99,  56, 110,  57,  84,  76, 74,  71,  90, 107,   94,  94, 173,  94, 108, 101,  91, 101, 110, 103, 121]\n\nthe 173 being the number of medals obtained in 1984 and 121 in 2016\n\nCreate a vector for each games of medals against years around the home game.\n\n\nSince not all games are seperated by 4 years and some games can be missed the data needs to be interpolated onto a range in steps of 4 years. Outer bounds of +-28 years ae chosen\nWhen data is missing because the full range of data can’t be obtained (i.e. for recent games or ones near the start of Olympics) replace any missing values with 0\n\n\nAdd up all the vectors for different games\n\n\nignore games where the range isn’t full (-32->32)\nthis gives the average effect of a home games\n\n\n# Step 1 and 2\n\n# Since we just want the summer games\ndf_games=df_games[df_games.Summer==1].reset_index()\n\nmedals=[]\nyears=[]\nnation=[]\nfor i in range(len(df_games)):\n    country=df_games.loc[i,'Country']\n    year=df_games.loc[i,'Year']\n    \n    nation.append(country)\n    medals.append(df_medals2[df_medals2.Nation==country].number_of_medals.values)\n    years.append(df_medals2[df_medals2.Nation==country].Year.values - year)\n    \n\n\n\"\"\"\nStep 3\nFunction to interpolate number of medals in years around a games\nTakes as input x=years and y=number of medals and outputs a new x and y values \nthat have been interpolated between -28 to +28 years in steps of 4 years\nFor games where the full range can't be obtained (e.g. an event close to 2016 \nwill have missing data for years after it) the data is interpolated to the nearest \n4 years and missing data replaced with zeroes.\n\"\"\"\n\ndef do_interp(x,y):\n    from scipy.interpolate import interp1d\n    \n    xx,yy=x,y\n    xhi,xlo=28,-28\n    \n    xnew=np.arange(xlo,xhi+4,4)\n    ynew=np.zeros(np.shape(xnew))\n    # here make adjustments if the whole range doesn't exist to go to nearest 4 yrs\n    # or if inbetween go outside to next one i.e. 11 years->12 years, 8->8,-5->-8\n    # lower years are dealt with separately to later years then combined\n    # and normalised\n    maxx,minx=np.max(x),np.min(x)\n    if maxx<xhi:\n        xhi= 4*(np.ceil(maxx/4))\n        xx=xx[1:]\n        yy=yy[1:]\n    if minx>xlo:\n        xlo= -4*(np.ceil(abs(minx)/4))\n        xx=xx[:-1]\n        yy=yy[:-1]\n        \n    cond =((xnew<=xhi) & (xnew>=xlo))\n    try:\n        f2 = interp1d(x, y, kind='cubic')\n        ynew[cond]=f2(xnew[cond])\n    except:\n        f2 = interp1d(xx, yy, kind='cubic')\n        cond =((xnew<=xhi-4) & (xnew>=xlo+4))\n        ynew[cond]=f2(xnew[cond])\n    \n    ynew=ynew/max(ynew)\n    return xnew,ynew\n\n\n# Step 3\n\nXY=[]\nfor i,year_range in enumerate(years):\n    x=year_range\n    y=medals[i]\n    xnew,ynew=do_interp(x,y)\n    XY.append(np.array(ynew))\n    \n\n\nnp.shape(XY)\n\n(29, 15)\n\n\n\n# If values are less than 0 put as just above zero- can occur when \n\n# make XY a numpy array\nXY = np.array(XY)\n\n# if interpolation is not good\nXY[XY<0]=0\n\n\n\n\n# create vectors for y-data either side of home-event\nyallL = np.zeros((8))\nyallR = np.zeros((8))\n\n# scroll through each games\n# find if the low years or high years have zeros in them\n# if they don't include them in the sum \nfor i,ygames in enumerate(XY):\n    \n    if i>11:\n        if 0 not in ygames[-8:]:\n            yallL[-8:]=yallL[-8:]+ygames[-8:]\n        \n        if 0 not in ygames[0:8]:\n            yallR[0:8]=yallR[0:8]+ygames[0:8]\n            \n\n# normalise\nyallL=yallL/max(yallL)\nyallR=yallR/max(yallR)\n\n# combine low and high years and only use home game year once\nyall=np.concatenate([yallR[0:-1],yallL])\n\n# plot the results\nplt.subplots(figsize=(6,4))\nplt.plot(xnew,yall/max(yall),'ok-')\n\nplt.ylim([0 ,1.05])\nplt.grid(True)\nplt.ylabel('Normalise medals won',fontsize=14)\nplt.xlabel('Years either side of a home games',fontsize=14)\n# plt.plot(xnew,yall2/max(yall2),'m+--')\n\nText(0.5, 0, 'Years either side of a home games')"
  },
  {
    "objectID": "posts/olympics/2022-07-29-Olympics_HomeEvent.html#plot-the-effect-of-home-games-for-different-games",
    "href": "posts/olympics/2022-07-29-Olympics_HomeEvent.html#plot-the-effect-of-home-games-for-different-games",
    "title": "Olympics data with SQL and pandas- home games",
    "section": "Plot the effect of home games for different games",
    "text": "Plot the effect of home games for different games\nLook at the individual games plots\n\n\nstartnum=11\n\ncola=['mo--','gs:','bv-.']\nfor i2 in range(6):\n    fig,ax=plt.subplots()\n    vals = np.arange(startnum,startnum+3,1,dtype=int)\n    i1=0\n    for i in vals: \n        plt.plot(xnew,XY[i,:],cola[i1])\n        plt.ylim([-.2 ,1])\n        plt.grid(True)\n        plt.legend([ nation[ii] + ' ' + str(df_games.loc[ii,'Year']) for jj,ii in enumerate(vals)]);\n        plt.ylabel('Normalise medals won')\n        plt.xlabel('Years either side of a home games')\n        i1=i1+1\n    startnum=startnum+3"
  },
  {
    "objectID": "posts/olympics/2022-07-29-Olympics_HomeEvent.html#continent-home-games",
    "href": "posts/olympics/2022-07-29-Olympics_HomeEvent.html#continent-home-games",
    "title": "Olympics data with SQL and pandas- home games",
    "section": "Continent home games",
    "text": "Continent home games\nThe Olympics have mainly been held in Europe and North America. Because of this it is not possible to do the same analysis as for home country games.\nInstead we can look at the difference if a games is a home continent or not.\n\ncontinentYorN=sqldf('\\\n    SELECT                                     \\\n    home_continent,                            \\\n    Year,                                      \\\n    COUNT(*)         AS num_countries,         \\\n    AVG(avg_medals)  AS avg_medals             \\\n    FROM                                       \\\n      (SELECT                                  \\\n            Nation,home_continent, Year,       \\\n            AVG(number_of_medals)  AS avg_medals\\\n        FROM                                   \\\n            (SELECT                            \\\n               a.*,                            \\\n               CASE WHEN                       \\\n                  c.Region =a.Continent THEN 1 \\\n               ELSE 0 END AS home_continent    \\\n            FROM                               \\\n                df_medals2 AS a                \\\n            INNER JOIN                         \\\n                df_games AS c                  \\\n            ON                                 \\\n                c.Year=a.Year                  \\\n            WHERE c.Year> 1951) as inner       \\\n        GROUP BY                               \\\n            Nation, Year ) as midder           \\\n    GROUP BY home_continent,Year               \\\n     ;',locals()  ) \n\n\ncontinentYorN.head()\n\n\n\n\n\n  \n    \n      \n      home_continent\n      Year\n      num_countries\n      avg_medals\n    \n  \n  \n    \n      0\n      0\n      1952\n      20\n      7.450000\n    \n    \n      1\n      0\n      1956\n      36\n      11.944444\n    \n    \n      2\n      0\n      1960\n      21\n      7.095238\n    \n    \n      3\n      0\n      1964\n      34\n      13.470588\n    \n    \n      4\n      0\n      1968\n      35\n      11.285714\n    \n  \n\n\n\n\n\nfig,ax=plt.subplots(figsize=(8,5))\n\n\n\nplt.plot(continentYorN[continentYorN.home_continent==1].Year,continentYorN[continentYorN.home_continent==1].avg_medals,'rs--')\nplt.plot(continentYorN[continentYorN.home_continent==0].Year,continentYorN[continentYorN.home_continent==0].avg_medals,'ob--')\n\nplt.grid(True)\nplt.ylabel('Average medals')\nplt.xlabel('Games year')\n\n# plt.plot([df_games[df_games.Region=='Asia'].Year.values,df_games[df_games.Region=='Asia'].Year.values],[0,35],color=[1,.8,.8]);\n\nplt.plot([0,0],'m--');\nplt.plot([0,0],'g-.');\n\nplt.plot([df_games[df_games.Region=='Americas'].Year.values,df_games[df_games.Region=='Americas'].Year.values],[0,35],'m--');\n\n\nplt.plot([df_games[df_games.Region=='Europe'].Year.values,df_games[df_games.Region=='Europe'].Year.values],[0,35],'g-.');\n\n\n# plt.plot([df_games[df_games.Region=='Oceania'].Year.values,df_games[df_games.Region=='Oceania'].Year.values],[0,35],color=[0.7,1,.7]);\n\nplt.xlim([1950,2020])\n\nplt.legend(['Home continent','Another continent','Americas','Europe'])\n\n<matplotlib.legend.Legend at 0x1f4339ed580>\n\n\n\n\n\n\nNot a good measure\nInstead need following columns\n\ntotal or average medals per continent\nHome Continent\nYear\n\n\n\n\"\"\"\nA check on the inner part of the SQL  statement\n\n\"\"\"\n\nsqldf('\\\n    SELECT *   FROM                            \\\n        (SELECT                                \\\n               a.*,                            \\\n               CASE WHEN                       \\\n                  c.Region =a.Continent THEN 1 \\\n               ELSE 0 END AS home_continent    \\\n            FROM                               \\\n                df_medals2 AS a                \\\n            INNER JOIN                         \\\n                df_games AS c                  \\\n            ON                                 \\\n                c.Year=a.Year                  \\\n            WHERE c.Year> 1951) as inner       \\\n     ;',locals()  ) \n\n\n\n\n\n\n  \n    \n      \n      NOC\n      Nation\n      Continent\n      number_of_medals\n      Year\n      home_continent\n    \n  \n  \n    \n      0\n      ANZ\n      Australia\n      Oceania\n      11\n      1952\n      0\n    \n    \n      1\n      ARG\n      Argentina\n      Americas\n      5\n      1952\n      0\n    \n    \n      2\n      AUT\n      Austria\n      Europe\n      2\n      1952\n      1\n    \n    \n      3\n      BEL\n      Belgium\n      Europe\n      4\n      1952\n      1\n    \n    \n      4\n      BOH\n      Czech Republic\n      Europe\n      13\n      1952\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      960\n      UKR\n      Ukraine\n      Europe\n      11\n      2016\n      0\n    \n    \n      961\n      USA\n      USA\n      Americas\n      121\n      2016\n      0\n    \n    \n      962\n      UZB\n      Uzbekistan\n      Asia\n      13\n      2016\n      0\n    \n    \n      963\n      VEN\n      Venezuela\n      Americas\n      3\n      2016\n      0\n    \n    \n      964\n      VIE\n      Vietnam\n      Asia\n      2\n      2016\n      0\n    \n  \n\n965 rows × 6 columns\n\n\n\n\n\"\"\"\nThe complete SQL statement\n\n\"\"\"\n\ncontinentYorN2=sqldf('\\\n    SELECT \\\n      Year, sum(number_of_medals) AS num_medals, Continent,home_continent\\\n          FROM                                 \\\n        (SELECT                                \\\n               a.*,                            \\\n               CASE WHEN                       \\\n                  c.Region =a.Continent THEN 1 \\\n               ELSE 0 END AS home_continent    \\\n            FROM                               \\\n                df_medals2 AS a                \\\n            INNER JOIN                         \\\n                df_games AS c                  \\\n            ON                                 \\\n                c.Year=a.Year                  \\\n            WHERE c.Year> 1951) as inner       \\\n        GROUP BY Continent,Year, home_continent\\\n        ORDER BY Year\\\n     ;',locals()  ) \n\ncontinentYorN2.head()\n\n\n\n\n\n  \n    \n      \n      Year\n      num_medals\n      Continent\n      home_continent\n    \n  \n  \n    \n      0\n      1952\n      11\n      Africa\n      0\n    \n    \n      1\n      1952\n      100\n      Americas\n      0\n    \n    \n      2\n      1952\n      24\n      Asia\n      0\n    \n    \n      3\n      1952\n      308\n      Europe\n      1\n    \n    \n      4\n      1952\n      14\n      Oceania\n      0\n    \n  \n\n\n\n\n\nplt.subplots(figsize=(8,5))\nregion='Asia'\ncond=((continentYorN2.Continent==region)&(continentYorN2.home_continent==1))\nplt.plot(continentYorN2[cond].Year,continentYorN2[cond].num_medals,'or',markersize=10)\ncond=((continentYorN2.Continent==region)&(continentYorN2.home_continent==0))\nplt.plot(continentYorN2[cond].Year,continentYorN2[cond].num_medals,':or')\n\nregion='Europe'\ncond=((continentYorN2.Continent==region)&(continentYorN2.home_continent==1))\nplt.plot(continentYorN2[cond].Year,continentYorN2[cond].num_medals,'bo',markersize=10)\ncond=((continentYorN2.Continent==region)&(continentYorN2.home_continent==0))\nplt.plot(continentYorN2[cond].Year,continentYorN2[cond].num_medals,'b--o')\n\nregion='Americas'\ncond=((continentYorN2.Continent==region)&(continentYorN2.home_continent==1))\nplt.plot(continentYorN2[cond].Year,continentYorN2[cond].num_medals,'m<',markersize=10)\ncond=((continentYorN2.Continent==region)&(continentYorN2.home_continent==0))\nplt.plot(continentYorN2[cond].Year,continentYorN2[cond].num_medals,'m-o')\n\nregion='Oceania'\ncond=((continentYorN2.Continent==region)&(continentYorN2.home_continent==1))\nplt.plot(continentYorN2[cond].Year,continentYorN2[cond].num_medals,'cv',markersize=10)\ncond=((continentYorN2.Continent==region)&(continentYorN2.home_continent==0))\nplt.plot(continentYorN2[cond].Year,continentYorN2[cond].num_medals,'c-o')\n\nplt.legend(['Asia','','Europe','','Americas','','Oceania',''])\nplt.grid(True)\n\n\n\n\n\nregion='Europe'\ncond=((continentYorN2.Continent==region)&(continentYorN2.home_continent==1))\nasum=np.average(continentYorN2[cond].num_medals)\ncond=((continentYorN2.Continent==region)&(continentYorN2.home_continent==0))\nbsum=np.average(continentYorN2[cond].num_medals)\n\nprint('For {} the average medals at home continent = {:.0f} and for away continent = {:.0f}\\n \\\n      With a ratio of {:.2f}'.format(region,asum,bsum,asum/bsum))\n\n\nregion='Asia'\ncond=((continentYorN2.Continent==region)&(continentYorN2.home_continent==1))\nasum=np.average(continentYorN2[cond].num_medals)\ncond=((continentYorN2.Continent==region)&(continentYorN2.home_continent==0))\nbsum=np.average(continentYorN2[cond].num_medals)\n\nprint('For {} the average medals at home continent = {:.0f} and for away continent = {:.0f}\\n \\\n      With a ratio of {:.2f}'.format(region,asum,bsum,asum/bsum))\n\nregion='Americas'\ncond=((continentYorN2.Continent==region)&(continentYorN2.home_continent==1))\nasum=np.average(continentYorN2[cond].num_medals)\ncond=((continentYorN2.Continent==region)&(continentYorN2.home_continent==0))\nbsum=np.average(continentYorN2[cond].num_medals)\n\nprint('For {} the average medals at home continent = {:.0f} and for away continent = {:.0f}\\n \\\n      With a ratio of {:.2f}'.format(region,asum,bsum,asum/bsum))\n\nregion='Oceania'\ncond=((continentYorN2.Continent==region)&(continentYorN2.home_continent==1))\nasum=np.average(continentYorN2[cond].num_medals)\ncond=((continentYorN2.Continent==region)&(continentYorN2.home_continent==0))\nbsum=np.average(continentYorN2[cond].num_medals)\n\nprint('For {} the average medals at home continent = {:.0f} and for away continent = {:.0f}\\n \\\n      With a ratio of {:.2f}'.format(region,asum,bsum,asum/bsum))\n\nFor Europe the average medals at home continent = 420 and for away continent = 396\n       With a ratio of 1.06\nFor Asia the average medals at home continent = 121 and for away continent = 103\n       With a ratio of 1.18\nFor Americas the average medals at home continent = 177 and for away continent = 130\n       With a ratio of 1.36\nFor Oceania the average medals at home continent = 50 and for away continent = 32\n       With a ratio of 1.57\n\n\nAgain the above stats are not really great as\n\ndon’t take account of changes in the average number of medals with time for a nation\ngames in Oceania and Americas are often also home games\n\nThe best metric uses Europe and suggests a less than 6% increase for a home continent"
  },
  {
    "objectID": "posts/olympics/2022-07-31-OlympicsSQL.html#introduction",
    "href": "posts/olympics/2022-07-31-OlympicsSQL.html#introduction",
    "title": "Olympics data with SQL and pandas",
    "section": "Introduction",
    "text": "Introduction\nThis project looks to understand the changing nature of the Olympics and how it reflects changes in athletes, sporting activities, and global politics. The Olympic Games are considered the world’s foremost sports competition with more than 200 nations participating [1,2], and in Tokyo in 2020 there was a broadcast audience of more than 3 billion with estimates of 3 out of 4 people following the Olympics [3].\nSince it’s inception over 100 years ago many changes have occured. In global politics countries have split and unified, populations have changed and the power distribution across nations fluctuated. In society there have been changes in the rights and roles of women. Finally, in sport there has been a move from amateur atheletes to professionalism and a change in the popularity of different sports.\nDue to it’s global importance, the question this work looks to answer is if data on the Olympics reflect the changes that have occured in the world.\n1.“Overview of Olympic Games”. Encyclopaedia Britannica. Retrieved 4 June 2008\n2.Olympic Games- Wikipedia\n3.Tokyo 2020 audience & insights report December 2021"
  },
  {
    "objectID": "posts/olympics/2022-07-31-OlympicsSQL.html#the-data",
    "href": "posts/olympics/2022-07-31-OlympicsSQL.html#the-data",
    "title": "Olympics data with SQL and pandas",
    "section": "The Data",
    "text": "The Data\nThe most important part of any analysis is the data. In Olympics data with SQL and pandas- create the tables I present the data to be analysed and do some initial processing.\nThe main thing here is to seperate the data into useable tables for analysis, as summarised in the entity relationship diagram (ERD) below."
  },
  {
    "objectID": "posts/olympics/2022-07-31-OlympicsSQL.html#analysis",
    "href": "posts/olympics/2022-07-31-OlympicsSQL.html#analysis",
    "title": "Olympics data with SQL and pandas",
    "section": "Analysis",
    "text": "Analysis\nBased on a brief analysis of the data three broad questions to be investigated were posed:\n\nWhat are the characteristics of athletes? How does this change with time, and can it be linked with societal or global changes?\nWhat countries do better at the Olympics? Is there a way to quantify this?\nWhat is the influence of a games being a home event?\n\nIn the following parts these are explore in more detail.\n\nAthlete Analysis\nI did some initial plots on the changes in the characteristics of athletes given in the data, height, weight and age, of athletes attending the Olympics by year (see below).\nFrom these plots I was really intrigued as to what may be the cause of these changes.\nMainly what was happening between 1960 and 1980 were there seemed to be changes in each of the parameters?\nMy initial thought was this could be related to some combination of - a switch from amateurs to professionals - the Cold War between USA and USSR - an after effect of WWII\nOlympics data with SQL and pandas- height weight and age\n\n\nNation Analysis\nDue to the global importance of the Olympics, in 2020 there was a broadcast audience of more than 3 billion, I was interested to explore whether countries with the most medals will reflect global politics. And to see if the countries with most influence get more medals.\nOlympics data with SQL and pandas- GDP and population\n\n\nGames Analysis\nIn this part the hypothesis considered is:\nAt a home Olympic games a nation will on average obtain more medals than at other games\n\nBut can we quantify this effect?\nAre there any residual effects before and after the games?\nWhat about a home continent games?\n\nOlympics data with SQL and pandas- home games"
  },
  {
    "objectID": "posts/olympics/2022-07-31-OlympicsSQL.html#presentation",
    "href": "posts/olympics/2022-07-31-OlympicsSQL.html#presentation",
    "title": "Olympics data with SQL and pandas",
    "section": "Presentation",
    "text": "Presentation\nTo present this data in a unified form the following presentation was produced.\nThis is a hypothetical presentation:\n\nWho\nThe audience is a fictional research group at Swansea University (UK) called the Sports History Group.\nThis group is a cross-departmental, working across the History and Sports Science department. The group consists of two lecturers (one in each department), three post doctoral researchers, five PhD students and three Masters students.\n\n\nWhy\nThe work I am presenting has overlap with several of the reserachers/students.\nThe main goal is a scoping exercise with one of the post doctoral researchers and the two lecturers who have identified a grant proposal. The Olympics commitee have put out a grant application. The aim of this is to produce a report on the influence the Olympics has had on Geo-Politics and on Athletes and Sport in general. With guidance on what the Olympics can do in the future to maintain and enhance its globally importance, and how it can positively impact Olympic athletes.\n\n\nWhat / How\nMore details are in the presentation"
  },
  {
    "objectID": "posts/tensorflow/2022-09-28-Tensorflow.html#general",
    "href": "posts/tensorflow/2022-09-28-Tensorflow.html#general",
    "title": "TensorFlow cheat sheet",
    "section": "General",
    "text": "General\nLibrary websites:\nkeras.io\ntensorflow.org"
  },
  {
    "objectID": "posts/tensorflow/2022-09-28-Tensorflow.html#courses",
    "href": "posts/tensorflow/2022-09-28-Tensorflow.html#courses",
    "title": "TensorFlow cheat sheet",
    "section": "Courses",
    "text": "Courses\nI have tried both of the following courses: DeepLearning.AI TensorFlow Developer Professional Certificate and TensorFlow 2 for Deep Learning Specialization.\n\nDeepLearning.AI TensorFlow Developer Professional Certificate\nTensorFlow 2 for Deep Learning Specialization\n\nThe Tensorflow2 course is a bit longer and goes into more depth, although there are additional extended courses for the deeplearning one. The deeplearning one can be done within the current 7 days trail period of coursera. The Tensorflow2 course is tricky to do in this timeframe. This is due to more material, the harder coursework, and waiting for capstone projects to be marked.\nIn the end I only did the first course of Tensorflow2 as I found the tests had material that wasn’t explained within the course and I found the lectures lacking in detail and the instructors became increasingly boring. I gave up after getting to the capstone in course 2 (of 3) when they asked a question about an NLP network that was never explained anywhere. However, the coursework is a good challenge, so it may be worth doing the course for this alone and learning from other sources in addition to this one.\nI prefered the DeepLearning courses as they were more in depth and didn’t assume as much prior knowledge, and the presentation was better. I am currently working through the follow-up course TensorFlow: Advanced Techniques Specialization and given time will do the NLP and MLOps courses.\nSome Others:\nOne by Udacity Intro to TensorFlow for Deep Learning is being offered for free and looks okay too.\nTensorFlow example tutorials written as Jupyter notebooks and run directly in Google Colab—a hosted notebook environment that requires no setup. Click the Run in Google Colab button. Part of the TensorFlow resources."
  },
  {
    "objectID": "posts/tensorflow/2022-09-28-Tensorflow.html#resources",
    "href": "posts/tensorflow/2022-09-28-Tensorflow.html#resources",
    "title": "TensorFlow cheat sheet",
    "section": "Resources",
    "text": "Resources\n\nTensorFlow’s website recommendations\nFrançois Chollet\n\nHis book Deep Learning with Python, Second Edition can be read online.\n\nDeep learning book by Ian Goodfellow and Yoshua Bengio and Aaron Courville\n\navailable free online\n\nProbabilistic Programming & Bayesian Methods for Hackers\n\navailable online"
  },
  {
    "objectID": "posts/tensorflow/2022-09-28-Tensorflow.html#model-creations",
    "href": "posts/tensorflow/2022-09-28-Tensorflow.html#model-creations",
    "title": "TensorFlow cheat sheet",
    "section": "Model creations",
    "text": "Model creations\nThe easiest way to create a model in Keras is through keras.Sequential, which creates a neural network as a stack of layers.\nSo in the examplel below - the 1st layer has units = 4 and input_shape=2 with a relu activation function - the 2nd layer has units = 3 with a relu activation function - the 3rd layer has units = 1\nThe 3rd layer is the output layer. Since there is no activation function this would be a regression problem to predict one value.\n\nFor non sequential models or with multiple inputs/outputs see functional API\n\nTabular Data\nhttps://www.kaggle.com/code/thomassimm/premier-league-predictions-using-tensorflow\n\nmodel = tf.keras.Sequential([\n        tf.keras.layers.Dense(512//8,activation='relu',input_shape=[input_shape]),\n        tf.keras.layers.Dense(512//8,activation='relu'),\n        tf.keras.layers.Dense(1,activation='sigmoid')        \n    ])\n\n\n\nImage Data\n\nmodel = tf.keras.models.Sequential([ \n          tf.keras.layers.Convolution2D( 64,(3,3),activation='relu',input_shape=(28,28,1) ),\n          tf.keras.layers.MaxPool2D(2,2),\n          tf.keras.layers.Flatten(),\n          tf.keras.layers.Dense(256//2,activation='relu'),\n          tf.keras.layers.Dense(1,activation='sigmoid') ])\n\n\n\nLanguage Data\nThe standard language model starts with an embedding layer, this then needs to be flattened to a vector, then we can add a dense layer before an output layer.\nThe Embedding layer creates a vector-space for the text data. So for example, the words beautiful and ugly may be in opposite directions. And words such as cat and kitten may be close together in vector space.\nGlobalAveragePooling1Dcan be replaced by Flatten()\n\nmodel = tf.keras.Sequential([ \n    tf.keras.layers.Embedding(num_words,embedding_dim,input_length=maxlen),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(24,activation='relu'),\n    tf.keras.layers.Dense(5,'softmax')\n])\n\nThe model above does not take account for the order of words,\nIf we want to do this we can insert an additional layer after the embedding layer. For example, by using the LSTM model as below\n\nmodel_lstm= tf.keras.Sequential([\n    tf.keras.layers.Embedding(VOCAB_SIZE,EMBEDDING_DIM,input_length=MAXLEN),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(24,activation='relu'),\n    tf.keras.layers.Dense(NUM_CLASSES,activation='softmax')   \n])\n\nWe can even insert a conolution layer after the embedding instead\ntf.keras.layers.Conv1D(128,5,activation='relu')\nFor two consecutive layers of RNNs use return_sequences=True\n\ntf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm1_dim, return_sequences=True)),\ntf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm2_dim)),"
  },
  {
    "objectID": "posts/tensorflow/2022-09-28-Tensorflow.html#compile-the-model",
    "href": "posts/tensorflow/2022-09-28-Tensorflow.html#compile-the-model",
    "title": "TensorFlow cheat sheet",
    "section": "Compile the model",
    "text": "Compile the model\nTo compile the model, need to define the following:\n\nthe optimizer to use, i.e. how to update the parameters to improve model during fitting\nthe loss, i.e. what defines the goodness of the fit\nany metrics to record\n\n\nopt = keras.optimizers.Adam(learning_rate=0.0001)\nmodel.compile(\n    optimizer=opt,\n    loss='binary_crossentropy',\n    metrics=['binary_accuracy'],\n)"
  },
  {
    "objectID": "posts/tensorflow/2022-09-28-Tensorflow.html#evaluate-predict-the-model",
    "href": "posts/tensorflow/2022-09-28-Tensorflow.html#evaluate-predict-the-model",
    "title": "TensorFlow cheat sheet",
    "section": "Evaluate / Predict the model",
    "text": "Evaluate / Predict the model\ntest_loss, test_accuracy = model.evaluate(scaled_test_images,\ntf.keras.utils.to_categorical(test_labels),\nverbose=0)\nand more or less outputs depending on metrics used\npred = model.predict(X_sample)"
  },
  {
    "objectID": "posts/tensorflow/2022-09-28-Tensorflow.html#saving-and-loading",
    "href": "posts/tensorflow/2022-09-28-Tensorflow.html#saving-and-loading",
    "title": "TensorFlow cheat sheet",
    "section": "Saving and Loading",
    "text": "Saving and Loading\n\nSaving / Loading weights\n\nmodel_weights_file='my_file'\n\nmodel.save_weights(model_weights_file)\n\nmodel.load_weights(model_weights_file)\n\n\n# All the model\n\nmodel.save('saved_model/my_model')\n\nnew_model = tf.keras.models.load_model('saved_model/my_model')"
  },
  {
    "objectID": "posts/tensorflow/2022-09-28-Tensorflow.html#callbacks",
    "href": "posts/tensorflow/2022-09-28-Tensorflow.html#callbacks",
    "title": "TensorFlow cheat sheet",
    "section": "Callbacks",
    "text": "Callbacks\nwithin model.fit(....)\ncallbacks=[callback_function]\nhttps://www.tensorflow.org/api_docs/python/tf/keras/callbacks\n\nEarlyStopping\n\nto stop the model early if some conditions are met\n\nModelCheckpoint\n\nsave the model/model weights (https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint)\nmain data stored similar to ‘.data-00000-of-00001’\nhttps://www.tensorflow.org/tutorials/keras/save_and_load#what_are_these_files\nCan give file names with variables using {}\n\nval_loss\nval_accuracy\nbatch\nepoch\n\n\nearly_stopping = tf.keras.callbacks.EarlyStopping(\n    min_delta=0.0001, # minimium amount of change to count as an improvement\n    patience=20, # how many epochs to wait before stopping\n    restore_best_weights=True,\n    monitor='val_binary_accuracy',\n)\n\nfilepath = os.path.join(cwd,'checkpoints_every_epoch/checkpoint.{epoch}.{batch}')\ncheckpoint = tf.keras.callbacks.ModelCheckpoint(\n        save_weights_only=True,\n        save_best_only=True,\n        filepath=filepath,\n    )"
  },
  {
    "objectID": "posts/tensorflow/2022-09-28-Tensorflow.html#overfitting-strategies",
    "href": "posts/tensorflow/2022-09-28-Tensorflow.html#overfitting-strategies",
    "title": "TensorFlow cheat sheet",
    "section": "Overfitting strategies",
    "text": "Overfitting strategies\n\nDropout\nThe idea behind dropout is to randomly drop out some fraction of a layer’s input units every step of training, making it much harder for the network to learn those spurious patterns in the training data that leads to overfitting.\nInstead, it has to search for broad, general patterns, whose weight patterns tend to be more robust.\nYou could also think about dropout as creating a kind of ensemble of networks like with RandomForests.\nExample useage, apply 30% dropout to the next layer\nlayers.Dropout(rate=0.3),\nlayers.Dense(16)\nExample taken from kaggle https://www.kaggle.com/code/ryanholbrook/dropout-and-batch-normalization\n\n\n\nBatch Normalization\nNormalization is important in neural networks, it can really significantly improve the results of the values of the input and output are between 0 and 1.\nIn contrast, it is not important in other models such as random forests. Hence, the input data is often normalised such as train_datagen = ImageDataGenerator(rescale=1./255.) in image models.\nSo if it is good to normalize the input data it can also be good to normalize the layers of the network. This can be done with a BatchNormalization layer.A batch normalization layer looks at each batch as it comes in, first normalizing the batch with its own mean and standard deviation, and then also putting the data on a new scale with two trainable rescaling parameters. Batchnorm, in effect, performs a kind of coordinated rescaling of its inputs.\nAs stated in https://www.kaggle.com/code/ryanholbrook/dropout-and-batch-normalization batchnorm: - batchnorm is most often added as an aid to the optimization process - but it can sometimes also help prediction performance - Models with batchnorm tend to need fewer epochs to complete training. - And can fix various problems that can cause the training to get “stuck”. - it can be used at almost any point in a network.\nlayers.Dense(16, activation='relu'),\nlayers.BatchNormalization(),\n… or between a layer and its activation function:\nlayers.Dense(16),\nlayers.BatchNormalization(),\nlayers.Activation('relu'),\n… Or if you add it as the first layer of your network it can act as a kind of adaptive preprocessor like Sci-Kit Learn’s StandardScaler."
  },
  {
    "objectID": "posts/tensorflow/2022-09-28-Tensorflow.html#multiple-inputs-and-outputs",
    "href": "posts/tensorflow/2022-09-28-Tensorflow.html#multiple-inputs-and-outputs",
    "title": "TensorFlow cheat sheet",
    "section": "Multiple inputs and outputs",
    "text": "Multiple inputs and outputs\n3 inputs and 2 outputs. Simple model\nInputs: - temp_train - nocc_train - lumbp_train\nOutputs: - out1_train - out2_train\nThings to note: - inputs in the model is a list of the Input() parts - concatentate is used with the input list to provide input to the next layers of the model - outputs in the model is a list of the output layers - When compiling use dictionary to set loss and metrics to each output - or lists ['binary_crossentropy','binary_crossentropy'] - When fitting, for the inputs/outputs either: - provide a list of the inputs [temp_train,nocc_train, lumbp_train] - give as a dict {'layer_name':variable_name}\n\n## Functional: multiple inputs\n# N.B. lowercase 'c' concatenate\nimport tensorflow as tf \nfrom tensorflow.keras.layers import Input, Dense, concatenate\ninput_shape=(1,)\n\n# get individual inputs\ninputs_temp = Input(shape=input_shape,name='temp')\ninputs_nocc = Input(shape=input_shape,name='nocc')\ninputs_lumbp = Input(shape=input_shape,name='lumbp')\n\n# combine them\ninput_list = [inputs_temp,inputs_nocc,inputs_lumbp]\ninput_layer =concatenate(input_list)\n\n# add inputs to the model for two outputs\noutput_pred1  = Dense(2,activation='sigmoid',name='out_1')(input_layer)\noutput_pred2 = Dense(2,activation='sigmoid',name='out_2')(input_layer)\n\n# output layer\noutput_list = [output_pred1, output_pred2]\n\n# create the model object\nmodel = tf.keras.Model(inputs=input_list, outputs=output_list )\n\n# show the model\nmodel.summary()\n\n# Compile\nmodel.compile(\n        optimizer='SGD',\n        loss={'out_1':'binary_crossentropy',\n              'out_2':'binary_crossentropy'},\n        metrics={'out_1':['accuracy'],\n                 'out_2':['accuracy']},\n        loss_weights=[1,0.2]\n        )\n\ntf.keras.utils.plot_model(model)\n\n\n# Define training inputs and outputs\ninputs_train = {'temp': temp_train, 'nocc': nocc_train, 'lumbp': lumbp_train}\noutputs_train = {'out_1': out1_train, 'out_2': out2_train}\n\n# fit the model\nmodel.fit(inputs_train,outputs_train)"
  },
  {
    "objectID": "posts/tensorflow/2022-09-28-Tensorflow.html#inception-images",
    "href": "posts/tensorflow/2022-09-28-Tensorflow.html#inception-images",
    "title": "TensorFlow cheat sheet",
    "section": "Inception (images)",
    "text": "Inception (images)\n\nLoad the model pre-trained weights\nImport the model architecture\nGive the model the input shape for data\nLoad the weights into the model\nFreeze all the layers\nPick out the front part of the model, as the layers to the end are more specialized\nAdd extra layers to the model that can be fitted to\n\n\n# 1- Download the inception v3 weights\n!wget --no-check-certificate \\\n    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n\n# 2- Import the inception model  \nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\n\n# Create an instance of the inception model from the local pre-trained weights\nlocal_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n\n# 3- create the model and load in the weights\npre_trained_model = InceptionV3(input_shape = (150, 150, 3),\n                                  include_top = False, \n                                  weights = None) \n\n# 4- load weights into the model\npre_trained_model.load_weights(local_weights_file)\n\n# 5- Make all the layers in the pre-trained model non-trainable\nfor layers in pre_trained_model.layers:\n    layers.trainable = False\n    \n# 6- Pick out part of the model\nlast_desired_layer = pre_trained_model.get_layer('mixed7')    \nlast_output = last_desired_layer.output\n\n# 7- Add extra layers to the model\n\n# Flatten the output layer to 1 dimension\nx = layers.Flatten()(last_output)  \n\n# Add a fully connected layer with 1024 hidden units and ReLU activation\nx = layers.Dense(1024,activation='relu')(x)\n# Add a dropout rate of 0.2\nx = layers.Dropout(0.2)(x)  \n# Add a final sigmoid layer for classification\nx = layers.Dense(1,activation='sigmoid')(x) \n\n# Create the complete model by using the Model class\nmodel = Model(inputs=pre_trained_model.input, outputs=x)\n\n# Compile the model\nmodel.compile(optimizer = RMSprop(learning_rate=0.0001), \n            loss = 'binary_crossentropy',\n            metrics = ['accuracy'])"
  },
  {
    "objectID": "posts/tensorflow/2022-09-28-Tensorflow.html#summary-info-of-model",
    "href": "posts/tensorflow/2022-09-28-Tensorflow.html#summary-info-of-model",
    "title": "TensorFlow cheat sheet",
    "section": "Summary / info of model",
    "text": "Summary / info of model\nmodel.summary()\nGet summary of the model"
  },
  {
    "objectID": "posts/tensorflow/2022-09-28-Tensorflow.html#class-or-function---metrics-and-losses",
    "href": "posts/tensorflow/2022-09-28-Tensorflow.html#class-or-function---metrics-and-losses",
    "title": "TensorFlow cheat sheet",
    "section": "Class or Function - Metrics and Losses",
    "text": "Class or Function - Metrics and Losses\nIn general, classes use camel formatting CategoricalAccuracy whereas function use underscores and lower case categorical_accuracy and sometimes initials MAE\n\nhttps://www.tensorflow.org/api_docs/python/tf/keras/losses\nhttps://www.tensorflow.org/api_docs/python/tf/keras/metrics"
  },
  {
    "objectID": "posts/tensorflow/2022-09-28-Tensorflow.html#low-level-handling-of-metrics",
    "href": "posts/tensorflow/2022-09-28-Tensorflow.html#low-level-handling-of-metrics",
    "title": "TensorFlow cheat sheet",
    "section": "Low level handling of metrics",
    "text": "Low level handling of metrics\n\nmetric.update_state() to accumulate metric stats after each batch\nmetric.result get current value of metric to display\nmetric.reset_state() to reset metric value typically at the end of epoch"
  },
  {
    "objectID": "posts/tensorflow/2022-09-28-Tensorflow.html#categorical-binary-versus-multiple",
    "href": "posts/tensorflow/2022-09-28-Tensorflow.html#categorical-binary-versus-multiple",
    "title": "TensorFlow cheat sheet",
    "section": "Categorical: Binary versus Multiple",
    "text": "Categorical: Binary versus Multiple\nFor categorical data there is a slight difference between if there are only 2 categories or more.\nGoing from binary to multiple: - We need to change activation in model from sigmoid to softmax in final Dense layer - Change loss function from binary_crossentropy to categorical_crossentropy in compile - Making data one-hot encoded, i.e. columns for each outcome - Or use SparseCategoricalCrossentropy\n\nmodel_binary = tf.keras.Sequential([\n        tf.keras.layers.Dense(512//8,activation='relu',input_shape=[input_shape]),\n        tf.keras.layers.Dense(1,activation='sigmoid')        \n    ])\n\nmodel_multi = tf.keras.Sequential([\n        tf.keras.layers.Dense(512//8,activation='relu',input_shape=[input_shape]),\n        tf.keras.layers.Dense(4,activation='softmax')        \n    ])\n\n\nmodel_binary.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\n\nmodel_multi.compile(loss='categorical_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\n\n\n# One-hot encoding method\n\ny_binary =[1,0,0,0,0,0,1,1,1,1,0,1,0,1]\n\ny_multi=[1,2,4,6,1,3,4,2,4,2,5,2,1,4,2,1]\ny_multi=tf.keras.utils.to_categorical(y_multi)\ny_multi\n\narray([[0., 1., 0., 0., 0., 0., 0.],\n       [0., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0.],\n       [0., 0., 0., 0., 0., 0., 1.],\n       [0., 1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 1., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0.],\n       [0., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0.],\n       [0., 0., 1., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 0., 1., 0.],\n       [0., 0., 1., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0.],\n       [0., 0., 0., 0., 1., 0., 0.],\n       [0., 0., 1., 0., 0., 0., 0.],\n       [0., 1., 0., 0., 0., 0., 0.]], dtype=float32)\n\n\nAlternatively, with output data like [0, 1, 4, 0, 2, 3, 3, 0, …]\nuse: - SparseCategoricalCrossentropy(from_logits=True)\n\nmodel.compile(\n                 loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n                 optimizer='adam',\n                 metrics=['accuracy'])"
  },
  {
    "objectID": "posts/tensorflow/2022-09-28-Tensorflow.html#learning-rate",
    "href": "posts/tensorflow/2022-09-28-Tensorflow.html#learning-rate",
    "title": "TensorFlow cheat sheet",
    "section": "Learning rate",
    "text": "Learning rate\nFind the best learning rate by using callbacks\nThe learning rate to use on the data below would be where the loss is low (y-axis) but not too close to where it increases or is unstable.\nSo for this on the downward part of the curve between 10E-6 and 10E-5\n\n# Set the learning rate scheduler\nlr_schedule = tf.keras.callbacks.LearningRateScheduler(\n    lambda epoch: 1e-8 * 10**(epoch / 20) )\n    \n# Set the training parameters\nmodel_tune.compile(loss=\"mse\", optimizer=optimizer)\n    \n# train the model\nhistory = model_tune.fit(dataset, epochs=100, callbacks=[lr_schedule])\n\n# plot the results\n# Define the learning rate array\nlrs = 1e-8 * (10 ** (np.arange(100) / 20))\n\n# Plot the loss in log scale\nplt.semilogx(lrs, history.history[\"loss\"])"
  },
  {
    "objectID": "posts/tensorflow/2022-09-28-Tensorflow.html#lambda-functions",
    "href": "posts/tensorflow/2022-09-28-Tensorflow.html#lambda-functions",
    "title": "TensorFlow cheat sheet",
    "section": "lambda functions",
    "text": "lambda functions\nhttps://www.tensorflow.org/api_docs/python/tf/keras/layers/Lambda\ntf.keras.layers.Lambda(     function, output_shape=None, mask=None, arguments=None, **kwargs )\nAdd a function that works on the data within the model\n\n# expand the dimensions\ntf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\n                      input_shape=[window_size])\n\n# make the output larger (can be useful if predicting to large values, but previous layer have activation function so values are close to 1)\ntf.keras.layers.Lambda(lambda x: x * 100.0)"
  },
  {
    "objectID": "posts/tensorflow/2022-09-28-Tensorflow.html#force-cpugpu",
    "href": "posts/tensorflow/2022-09-28-Tensorflow.html#force-cpugpu",
    "title": "TensorFlow cheat sheet",
    "section": "Force CPU/GPU",
    "text": "Force CPU/GPU\nhttps://www.tensorflow.org/tutorials/distribute/multi_worker_with_keras\n\n# Check available CPU/GPU devices\n\nprint(tf.config.list_physical_devices('CPU'))\n\nprint(tf.config.list_physical_devices('GPU'))\n\nwith tf.device(\"CPU:0\"):\n    model.fit(....)\n    \nwith tf.device(\"GPU:0\"):\n    model.fit(....)"
  },
  {
    "objectID": "posts/tensorflow/2022-10-07-Tensorflow-2.html#dataset-generators",
    "href": "posts/tensorflow/2022-10-07-Tensorflow-2.html#dataset-generators",
    "title": "TensorFlow cheat sheet 2",
    "section": "Dataset generators",
    "text": "Dataset generators\nHave you ever had to work with a dataset so large that it overwhelmed your machine’s memory? Or maybe you have a complex function that needs to maintain an internal state every time it’s called, but the function is too small to justify creating its own class. In these cases and more, generators and the Python yield statement are here to help\nhttps://realpython.com/introduction-to-python-generators/\nHow yield works:\n\ndef do_yield():\n    for i in range(20):\n        yield i\n        \ngot_yield =  do_yield()\n\nprint(got_yield)\nprint(next(got_yield))\nprint(next(got_yield))\n\nnext(got_yield)\n\nprint(next(got_yield))\ngot_yield\nprint(next(got_yield))\n\nWhen datasets are large and won’t fit into memory, a way to handle this is to use detaset generators. Where data is fed into the model without loading it into memory at once.Each time we iterate the generator, it yields the next value in the series.\nAn example is below. The function takes a path to a file but returns a yield statement, or a data generator, and not a line of the data.\nAs above x,y = next(text_datagen) gets the next line of the text.\nThis can be used when fitting to the model using model.fit_generator(text_datagen)\nSee also load images.\n\ndef get_data(filepath):\n    with open(filepath,'r') as f:\n        for row in f:\n            x=row[0]\n            y=row[1]\n            yield (x,y)\n            \ntext_datagen = get_data('file.txt')\n\nmodel.fit_generator(text_datagen, steps_per_epoch=1000, epochs=5)\n\n\n# or something more practical:\ndef get_generator(features, labels, batch_size=1):\n    for n in range(int(len(features)/batch_size)):\n        x = features[n*batch_size: (n+1)*batch_size]\n        y = labels[n*batch_size: (n+1)*batch_size]\n        yield (x,y)"
  },
  {
    "objectID": "posts/tensorflow/2022-10-07-Tensorflow-2.html#the-dataset-class",
    "href": "posts/tensorflow/2022-10-07-Tensorflow-2.html#the-dataset-class",
    "title": "TensorFlow cheat sheet 2",
    "section": "The dataset Class",
    "text": "The dataset Class\n\nx=np.random.randint(0,255,(100,20,2,2))\ny=np.random.randint(0,4,size=(100,1))\n\ndataset_1 = tf.data.Dataset.from_tensor_slices(x)\n\nprint(\">>\",dataset_1.element_spec)\nprint('>> N.B. first dimension inetrpreted as batch size')\n\ndataset_2 = tf.data.Dataset.from_tensor_slices(y)\nprint(\">>\",dataset_2.element_spec)\n\ndataset_zipped = tf.data.Dataset.zip((dataset_1,dataset_2))\nprint(\">>\",dataset_zipped.element_spec)\n\ndataset_comb = tf.data.Dataset.from_tensor_slices((x,y))\nprint(\">>\",dataset_comb.element_spec)\n\n>> TensorSpec(shape=(20, 2, 2), dtype=tf.int32, name=None)\n>> N.B. first dimension inetrpreted as batch size\n>> TensorSpec(shape=(1,), dtype=tf.int32, name=None)\n>> (TensorSpec(shape=(20, 2, 2), dtype=tf.int32, name=None), TensorSpec(shape=(1,), dtype=tf.int32, name=None))\n>> (TensorSpec(shape=(20, 2, 2), dtype=tf.int32, name=None), TensorSpec(shape=(1,), dtype=tf.int32, name=None))\n\n\nCan access the values by iterating\n\ndef check3s(dataset_comb):\n    dataset_iter = iter(dataset_comb)\n    for i,x in enumerate(dataset_iter):\n        if tf.squeeze(x[1])==3:\n            print('Has 3s')\n            return\n    return 'no 3s'\ncheck3s(dataset_comb)\n\nHas 3s\n\n\n\nFilter\nFilter certain values\n\ndef label_func(image,label):\n    return tf.squeeze(label) != 3\n\ndataset_comb = dataset_comb.filter(label_func)\n\ncheck3s(dataset_comb)\n\n'no 3s'\n\n\n\n\nMap\nModify values. Below creates one-hot encoding\n\ndef map_func(image, x):\n    return (image,tf.one_hot(x,depth=3) )\ndataset_comb_2=dataset_comb.map(map_func)\n\nfor i,x in enumerate(dataset_comb):\n    if i<5:\n        print(i,x[1].numpy())    \n    else:\n        break\n        \nfor i,x in enumerate(dataset_comb_2):\n    if i<5:\n        print(i,x[1].numpy())    \n    else:\n        break\n\n0 [0]\n1 [1]\n2 [2]\n3 [2]\n4 [2]\n0 [[1. 0. 0.]]\n1 [[0. 1. 0.]]\n2 [[0. 0. 1.]]\n3 [[0. 0. 1.]]\n4 [[0. 0. 1.]]\n\n\n\ndataset.batch(20), drop_remainder=True set batch size to 16 and remove any remaining samples if not divisible\ndataset.repeat(10) set the number of epochs. No value inside is indefinitely\ndataset.shuffle(100) shuffle the data, no of sample in the buffer\ndataset.filter(function_name) filter the values use lambda or a function that returns a boolean\ndataset.map(func_name) transform the values\n\ne.g. dataset.map(lambda x:x*2) doubles all values\n\ndataset.take(1) take a value from the dataset"
  },
  {
    "objectID": "posts/tensorflow/2022-10-07-Tensorflow-2.html#tensor-math",
    "href": "posts/tensorflow/2022-10-07-Tensorflow-2.html#tensor-math",
    "title": "TensorFlow cheat sheet 2",
    "section": "Tensor Math",
    "text": "Tensor Math\n\nimport tensorflow.keras.backend as K\n\nx = K.arange(0,10)\ny = K.square(x)\ny_mean = K.mean(y)\n\nprint(f\"x = {x},\\ny = {y},\\ny_mean = {y_mean}\")\n\nx = [0 1 2 3 4 5 6 7 8 9],\ny = [ 0  1  4  9 16 25 36 49 64 81],\ny_mean = 28\n\n\n\n#hide-input\nprint(f\"tf.add([1,2],[3,4]) = {tf.add([1,2],[3,4])}\\n\")\nprint(\"Or with operator overloading\")\nprint(f\"tf.Variable([1,2])+tf.Variable([3,4]) = {tf.Variable([1,2])+tf.Variable([3,4])}\\n\")\nprint(f\"x = tf.Variable([[1,2],[3,4]])\\n\")\nx=tf.Variable([[1,2],[3,4]])\nprint(f\"tf.square(x) = {tf.square(x)}\\n\")\nprint(\"Reduces dimension by adding up components\")\nprint(f\"tf.reduce_sum(x) = {tf.reduce_sum(x)}\\n\")\n      \n\ntf.add([1,2],[3,4]) = [4 6]\n\nOr with operator overloading\ntf.Variable([1,2])+tf.Variable([3,4]) = [4 6]\n\nx = tf.Variable([[1,2],[3,4]])\n\ntf.square(x) = [[ 1  4]\n [ 9 16]]\n\nReduces dimension by adding up components\ntf.reduce_sum(x) = 10"
  },
  {
    "objectID": "posts/tensorflow/2022-10-07-Tensorflow-2.html#tensor-operations",
    "href": "posts/tensorflow/2022-10-07-Tensorflow-2.html#tensor-operations",
    "title": "TensorFlow cheat sheet 2",
    "section": "Tensor Operations",
    "text": "Tensor Operations\n\nEvaluated immediately\nTensorFlow supports two types of code execution, graph-based where all of the data and ops are loaded into a graph before evaluating them within a session, or eager based where all of the code is executed line by line.\nIf eager mode was off, tensor would not be evaluated so\nx_sq = tf.square(2)\nprint(x_sq)\nthe print statement would just show details of the tensor object, such as its name, shape, data type and all that but it would not yet store the number 4 as a value.\nOtherwise values are evaluated immediately in custom eager mode = On\n\n\nBroadcasting\nBroadcasting is where adding or subtracting two tensors of different dimensions is handled in a way where the tensor with fewer dimensions is replicated to match the dimensions of the tensor with more dimensions.\na = tf.constant([[1,2],[3,4]])\n>>tf.add(a,1)\n=tf.Tensor([[2,3],[4,5]])\nOr overloading can utilise Python syntax such as\n>>a ** 2\n=tf.Tensor([[1,4],[9,16]])\nOr using numpy math operations. TensorFlow will convert the tensor objects a and b into ndarrays, and then pass those ndarrays to the np.cos function.\n>>np.cos(a)\n=array([[ 0.54030231, -0.41614684],        [-0.9899925 , -0.65364362]])\nDon’t need to preconvert from the ndarray data type into a tensor data type. TensorFlow handles this automatically.\nndarray = np.ones([3,3])\n=[[1. 1. 1.] [1. 1. 1.] [1. 1. 1.]]\ntf.multiply(ndarray,3)\n=tf.Tensor( [[1. 1. 1.] [1. 1. 1.] [1. 1. 1.]], shape=(3,3), dtype=float64)\nTensors can be easily converted back to numpy arrays using tensor.numpy()"
  },
  {
    "objectID": "posts/tensorflow/2022-10-07-Tensorflow-2.html#simple-example-of-using-gradient-tape-to-calculate-gradient-of-a-function",
    "href": "posts/tensorflow/2022-10-07-Tensorflow-2.html#simple-example-of-using-gradient-tape-to-calculate-gradient-of-a-function",
    "title": "TensorFlow cheat sheet 2",
    "section": "Simple example of using gradient tape to calculate gradient of a function",
    "text": "Simple example of using gradient tape to calculate gradient of a function\n\ndef myfunc(x):\n    return tf.math.sin(x) + tf.math.exp(x/3)\n\nw = tf.Variable(np.arange(0, np.pi*2,.2))\nwith tf.GradientTape() as tape:\n    loss = myfunc(w)\ngradient = tape.gradient(loss, w).numpy()\n\nplt.plot(w.numpy(), loss,'.-')\nplt.grid(True)\nplt.plot(w.numpy(), gradient,'x--');\nplt.legend(['Loss','Gradient of Loss']);"
  },
  {
    "objectID": "posts/tensorflow/2022-10-07-Tensorflow-2.html#using-watch",
    "href": "posts/tensorflow/2022-10-07-Tensorflow-2.html#using-watch",
    "title": "TensorFlow cheat sheet 2",
    "section": "Using watch",
    "text": "Using watch\nIf use watch on a variable the following variables referencing that variable are also watched\nBut the calls to new functions need to be within the with statement, but the gradient getting doesn’t\n\nx = tf.Variable(np.arange(0, np.pi*2,.2))\n\nwith tf.GradientTape() as t:\n    t.watch(x)\n    \n    y = tf.sin(x)\n    z = tf.exp(y)\ndz_dx = t.gradient(z,x)\n\nplt.plot(x.numpy(),z.numpy())\nplt.plot(x.numpy(),dz_dx.numpy(),'--')\nplt.legend(['z','dz/dx'])\nplt.grid(True)"
  },
  {
    "objectID": "posts/tensorflow/2022-10-10-Tensorflow-CustomLayersModels.html#coding-custom-layer",
    "href": "posts/tensorflow/2022-10-10-Tensorflow-CustomLayersModels.html#coding-custom-layer",
    "title": "TensorFlow Low-Level cheat sheet",
    "section": "Coding custom layer",
    "text": "Coding custom layer\nA layer class is inherited from Kera’s Layer class. Hence, MyLayer(Layer):\n\ndef __init__(self, units=32):\n\nInitializes the class, accepts parameters and sets up internal variables.\nsuper().__init__() returns a temporary object of the superclass (proxy object) the Layer class. This allows us to acces methods of the base class. - make sure to pass class name and self in super() - or **kwargs\nThen build and call functions can be added to create the layer and when it is called. N.B. build can often be moved to __init__.\n\ndef build(self, input_shape):\n\nruns when instance is created\ncreates state of layers (weights)\n\ndef call(self, inputs):\n\ncall does the computation\n\n\nThe values for the parameters can be explicitly set using functions like tf.random_normal_initializer() and sepcifying a shape.\nThe key to call is defining how the parts in build and __init__ are put together to create the computation of the layer.\nAlso note that in the call the format is the same as for the functional API, where the previous layer is added at the end of the next layer.\nAll the variables (weights and biases) can also be accesed with layer_class.variables.\n\n# Create a custom layer\nfrom tensorflow.keras.layers import Layer\nimport tensorflow as tf\n\nclass SimpleDense(Layer):\n    \n    def __init__(self,units=32):\n        super(SimpleDense, self).__init__()\n        self.units = units\n        \n    def build(self, input_shape):\n        w_init = tf.random_normal_initializer()\n        self.w = tf.Variable(\n                initial_value=w_init(shape=(input_shape[-1], self.units), dtype='float32'),\n                trainable=True,name='kernel',\n                                )\n        b_init = tf.zeros_initializer()\n        self.b = tf.Variable(\n                initial_value=b_init(shape=(self.units), dtype='float32'),\n                trainable=True, name='bias',\n                                )\n    def call(self, inputs):\n        return tf.matmul(inputs, self.w)+self.b\n    \ndense_layer = SimpleDense(units=32)\n\nx = tf.ones((1,1))\n\nprint(dense_layer(x) )\nprint()\nprint(f\"Weights = {dense_layer.weights[0].numpy()} \\n\\nand biases {dense_layer.weights[1].numpy()}\")\nprint()\nprint([ var.numpy() for var in dense_layer.variables])\n\ntf.Tensor(\n[[-0.07436698 -0.02011585  0.05582675  0.05404044  0.02519816  0.02855827\n   0.00046192  0.01360966  0.04663334  0.04556176  0.04592257  0.04647708\n  -0.03950281 -0.00014847 -0.03248019  0.08354251  0.07218082 -0.01156685\n   0.04577427 -0.06801199 -0.02725383 -0.02071865  0.08600459 -0.00035707\n  -0.03410981  0.00493511 -0.05133317 -0.12937713 -0.13792662 -0.01709494\n  -0.05110807 -0.01718794]], shape=(1, 32), dtype=float32)\n\nWeights = [[-0.07436698 -0.02011585  0.05582675  0.05404044  0.02519816  0.02855827\n   0.00046192  0.01360966  0.04663334  0.04556176  0.04592257  0.04647708\n  -0.03950281 -0.00014847 -0.03248019  0.08354251  0.07218082 -0.01156685\n   0.04577427 -0.06801199 -0.02725383 -0.02071865  0.08600459 -0.00035707\n  -0.03410981  0.00493511 -0.05133317 -0.12937713 -0.13792662 -0.01709494\n  -0.05110807 -0.01718794]] \n\nand biases [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n 0. 0. 0. 0. 0. 0. 0. 0.]\n\n[array([[-0.07436698, -0.02011585,  0.05582675,  0.05404044,  0.02519816,\n         0.02855827,  0.00046192,  0.01360966,  0.04663334,  0.04556176,\n         0.04592257,  0.04647708, -0.03950281, -0.00014847, -0.03248019,\n         0.08354251,  0.07218082, -0.01156685,  0.04577427, -0.06801199,\n        -0.02725383, -0.02071865,  0.08600459, -0.00035707, -0.03410981,\n         0.00493511, -0.05133317, -0.12937713, -0.13792662, -0.01709494,\n        -0.05110807, -0.01718794]], dtype=float32), array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n      dtype=float32)]\n\n\nOr equivalently…\n\n# Create a custom layer\nfrom tensorflow.keras.layers import Layer\nimport tensorflow as tf\n\nclass MyLayer(Layer):\n    \n    def __init__(self,units,input_dim):\n        super(MyLayer, self).__init__()\n        self.w = self.add_weight(shape=(input_dim,units),\n                                initializer='random_normal',\n                                trainable=True)\n        self.b = self.add_weight(shape=(units,),\n                                 initializer='zeros',\n                                trainable=True)\n    def call(self, inputs):\n        return tf.matmul(inputs, self.w)+self.b\n    \ndense_layer = MyLayer(3,5)\n\nx = tf.ones((1,5))\n\nprint(dense_layer(x) )\nprint()\nprint(f\"Weights = {dense_layer.weights[0].numpy()} \\n\\nand biases {dense_layer.weights[1].numpy()}\")"
  },
  {
    "objectID": "posts/tensorflow/2022-10-10-Tensorflow-CustomLayersModels.html#create-a-dropout-layer-as-a-custom-layer",
    "href": "posts/tensorflow/2022-10-10-Tensorflow-CustomLayersModels.html#create-a-dropout-layer-as-a-custom-layer",
    "title": "TensorFlow Low-Level cheat sheet",
    "section": "Create a Dropout layer as a custom layer",
    "text": "Create a Dropout layer as a custom layer\nN.B. uses tf.nn primitive Neural Net (NN) Operations.\n\nclass MyDropout(Layer):\n\n    def __init__(self, rate):\n        super(MyDropout, self).__init__()\n        self.rate = rate\n        \n    def call(self, inputs):\n        # Define forward pass for dropout layer\n        return tf.nn.dropout(inputs, rate=self.rate)"
  },
  {
    "objectID": "posts/tensorflow/2022-10-10-Tensorflow-CustomLayersModels.html#add-activation-functions",
    "href": "posts/tensorflow/2022-10-10-Tensorflow-CustomLayersModels.html#add-activation-functions",
    "title": "TensorFlow Low-Level cheat sheet",
    "section": "Add activation functions",
    "text": "Add activation functions\nTo add activation functions in the layer\n\nadd activation=None to the __init__ inputs so it accepts an activation but defaults to None if doesn’t recieve one\nadd the activation to the self variable\n\nuse self.activation = tf.keras.activations.get(activation)\ni.e. so we can pass a string or a function\n\ncall the activation function within the call\n\ni.e. return  self.activation(tf.matmul(inputs, self.w)+self.b)"
  },
  {
    "objectID": "posts/tensorflow/2022-10-10-Tensorflow-CustomLayersModels.html#custom-layers-in-a-model",
    "href": "posts/tensorflow/2022-10-10-Tensorflow-CustomLayersModels.html#custom-layers-in-a-model",
    "title": "TensorFlow Low-Level cheat sheet",
    "section": "Custom layers in a model",
    "text": "Custom layers in a model\nCan create the model by passing the layer into keras.Sequential as a list. In the same way as done with other layer elements.\n\nxs = np.arange(-1,5,dtype=float)\nys = xs*2 -1\n\nmodel = tf.keras.Sequential([SimpleDense(units=1)])\n\nmodel.compile(optimizer='sgd',loss='mse')\nmodel.fit(xs,ys,epochs=500,verbose=0)\nprint(model.predict([10.]))\n\n[[18.981386]]"
  },
  {
    "objectID": "posts/tensorflow/2022-10-10-Tensorflow-CustomLayersModels.html#model-example-residual-networks",
    "href": "posts/tensorflow/2022-10-10-Tensorflow-CustomLayersModels.html#model-example-residual-networks",
    "title": "TensorFlow Low-Level cheat sheet",
    "section": "Model example Residual Networks",
    "text": "Model example Residual Networks\n\n\nfrom tensorflow.keras.layers import Conv2D, Dense, Layer\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\n\nclass CNNResidual(Layer):\n    def __init__(self, layers, filters, **kwargs):\n        super(**kwargs).__init__()\n        self.hidden = [Conv2D(filters, (3,3), activation='relu') for _ in range(layers)]\n    def call(self, inputs):\n        x = inputs\n        for layer in self.hidden:\n            x = layer(x)\n        return inputs + x\n    \nclass DNNResidual(Layer):\n    def __init__(self, layers, neurons, **kwargs):\n        super(**kwargs).__init__()\n        self.hidden = [Dense(neurons, activation='relu') for _ in range(layers)]\n    def call(self, inputs):\n        x = inputs\n        for layer in self.hidden:\n            x = layer(x)\n        return inputs + x\n\nclass MyResidual(Model):\n    def __init(self, **kwargs):\n        self.hidden1 = Dense(30, activation='relu')\n        self.block1 = CNNResidual(2, 32)\n        self.block2 = DNNResidual(2, 64)\n        self.out = Dense(1)\n    def call(self, inputs):\n        x = self.hidden1(inputs)\n        x = self.block1(x)\n        for _ in range(1,4):\n            x = self.block2(x)\n        return self.out(x)"
  },
  {
    "objectID": "posts/tensorflow/2022-10-10-Tensorflow-Images.html#load-from-directory",
    "href": "posts/tensorflow/2022-10-10-Tensorflow-Images.html#load-from-directory",
    "title": "TensorFlow Images cheat sheet",
    "section": "Load from directory",
    "text": "Load from directory\nIf files are in folders can use flow_from_directory the files would need to be separated by class and training/validation as follows for a classification\ne.g. files in folders like this:\n\n/tmp/cats-v-dogs/validation/cats\n/tmp/cats-v-dogs/validation/dogs\n/tmp/cats-v-dogs/training/cats\n`/tmp/cats-v-dogs/training/dogs\n\n\ntrain_datagen = ImageDataGenerator()\n\ntrain_generator = train_datagen.flow_from_directory(directory=TRAINING_DIR,\n                                                      batch_size=256,\n                                                      class_mode='binary',\n                                                      target_size=(150, 150))\n# Test your generators\ntrain_generator, validation_generator = train_val_generators(TRAINING_DIR, VALIDATION_DIR)\n\n# Put in the fit\nmodel.fit(train_generator,\n                    epochs=15,\n                    verbose=1,\n                    validation_data=validation_generator)"
  },
  {
    "objectID": "posts/tensorflow/2022-10-10-Tensorflow-Images.html#an-example-of-transfer-learning-on-images-with-inception",
    "href": "posts/tensorflow/2022-10-10-Tensorflow-Images.html#an-example-of-transfer-learning-on-images-with-inception",
    "title": "TensorFlow Images cheat sheet",
    "section": "An example of transfer learning on images with Inception",
    "text": "An example of transfer learning on images with Inception\n\nLoad the model pre-trained weights\nImport the model architecture\nGive the model the input shape for data\n\npre_trained_model = InceptionV3(input_shape = (150, 150, 3),                                   include_top = False,                                    weights = None)\nBetter to keep the same shape as the model uses and change your data to match it than to change input_shape to match your data.\ninclude_top=False removes top most layer of the model- the output layer\nweights=None just uses the model architecture- note the weights are loaded on a later line\n\nLoad the weights into the model\nFreeze all the layers\nPick out the front part of the model, as the layers to the end are more specialized\nAdd extra layers to the model that can be fitted to\n\nNote this uses the functional API\n\nMatch the image size of our images to that needed by the model\n\nOur model expects 150X150X3 but our data is 50X50. So we need to multiply our image size by 3. This is done by the UpSampling2D layer\nmodel = tf.keras.layers.UpSampling2D(size=(3,3))(model)\nor use resize if image is bigger tf.image.resize(image, (150, 150,))\n\n# 1- Download the inception v3 weights\n!wget --no-check-certificate \\\n    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n\n# 2- Import the inception model  \nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\n\n# Create an instance of the inception model from the local pre-trained weights\nlocal_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n\n# 3- create the model and load in the weights\npre_trained_model = InceptionV3(input_shape = (150, 150, 3),\n                                  include_top = False, \n                                  weights = None) \n\n# 4- load weights into the model\npre_trained_model.load_weights(local_weights_file)\n\n# 5- Make all the layers in the pre-trained model non-trainable\nfor layers in pre_trained_model.layers:\n    layers.trainable = False\n    \n# 6- Pick out part of the model\nlast_desired_layer = pre_trained_model.get_layer('mixed7')    \nlast_output = last_desired_layer.output\n\n# 7- Add extra layers to the model\n\n# Flatten the output layer to 1 dimension\nx = layers.Flatten()(last_output)  \n\n# Add a fully connected layer with 1024 hidden units and ReLU activation\nx = layers.Dense(1024,activation='relu')(x)\n# Add a dropout rate of 0.2\nx = layers.Dropout(0.2)(x)  \n# Add a final sigmoid layer for classification\nx = layers.Dense(1,activation='sigmoid')(x) \n\n# 8. Increase input image sizes to match that needed by model using \n# a layer before the existing model starts\n\nmodel = tf.keras.layers.UpSampling2D(size=(3,3))(model)\n\n# Create the complete model by using the Model class\nmodel = Model(inputs=pre_trained_model.input, outputs=x)\n\n\n\n# Compile the model\nmodel.compile(optimizer = RMSprop(learning_rate=0.0001), \n            loss = 'binary_crossentropy',\n            metrics = ['accuracy'])"
  },
  {
    "objectID": "posts/tensorflow/2022-10-10-Tensorflow-Images.html#r-cnn",
    "href": "posts/tensorflow/2022-10-10-Tensorflow-Images.html#r-cnn",
    "title": "TensorFlow Images cheat sheet",
    "section": "R-CNN",
    "text": "R-CNN\nR-CNN (R=region) a region based CNN to implement selective search with neural networks.\n\nTakes input images\nExtract regions using selective search method (~2k)\nExtract features using CNN from each region\n\nwarped to match AlexNet inputs\n\nClassify with support vector machine (SVM) instead of dense layers\nPlus regression to get bounding box of images\nVery slow & computationally expensive\n\n\nRoss Girshick, Jeff Donahue, Trevor Darrell, Jitendra Malik, “Rich feature hierarchies for accurate object detection and semantic segmentation” 2014"
  },
  {
    "objectID": "posts/tensorflow/2022-10-10-Tensorflow-Images.html#fast-r-cnn",
    "href": "posts/tensorflow/2022-10-10-Tensorflow-Images.html#fast-r-cnn",
    "title": "TensorFlow Images cheat sheet",
    "section": "Fast R-CNN",
    "text": "Fast R-CNN\nThe aim was to improve issues above with RCNN.\n\nEntire image is fed into the ConvNet\n\nno selective search- computationally expensive\nthis Convnet is trained on finding features\nproduces a feature map of the image\n\nEach feature map can then be fed into fully connected dense layer\n\nget feature vector of image\n\nFeature vector fed into layers to do regression and classification\n\n\nRoss Girshick, Fast R-CNN, 2015"
  },
  {
    "objectID": "posts/tensorflow/2022-10-10-Tensorflow-Images.html#faster-r-cnn",
    "href": "posts/tensorflow/2022-10-10-Tensorflow-Images.html#faster-r-cnn",
    "title": "TensorFlow Images cheat sheet",
    "section": "Faster R-CNN",
    "text": "Faster R-CNN\n\nentire image into Convnet\nsliding window to find areas of interest\nsomething called a Region Proposed Network is used with data to find and create anchor boxes on image\nThe cropped and passed to Dense layers for classification and regression.\n\n Deep Learning for Computer Vision with Python"
  },
  {
    "objectID": "posts/tensorflow/2022-10-10-Tensorflow-Images.html#object-detection-in-tensorflow",
    "href": "posts/tensorflow/2022-10-10-Tensorflow-Images.html#object-detection-in-tensorflow",
    "title": "TensorFlow Images cheat sheet",
    "section": "Object detection in TensorFlow",
    "text": "Object detection in TensorFlow\nhttps://www.tensorflow.org/hub\n\nTensorFlow Hub is a repository of trained machine learning models ready for fine-tuning and deployable anywhere. Reuse trained models like BERT and Faster R-CNN with just a few lines of code.\n\nCopy url from hub page https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1 page for faster rcnn. And copy url is “https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1”\n\nimport tensorflow as tf\nimport tensorflow-hub as hub\n\nmodule_handle = \"https://tfhub.dev/google/faster_rcnn/openimages_v4/inception_resnet_v2/1\"\n\ndetector = hub.load(module_handle).signatures['default']\n\n\nAn example can be found here https://www.tensorflow.org/hub/tutorials/object_detection"
  },
  {
    "objectID": "posts/tensorflow/2022-10-10-Tensorflow-Images.html#object-detection-api",
    "href": "posts/tensorflow/2022-10-10-Tensorflow-Images.html#object-detection-api",
    "title": "TensorFlow Images cheat sheet",
    "section": "Object Detection API",
    "text": "Object Detection API\nhttps://colab.research.google.com/github/tensorflow/hub/blob/master/examples/colab/tf2_object_detection.ipynb\nhttps://github.com/tensorflow/models/blob/master/research/object_detection/g3doc/tf2.md https://github.com/tensorflow/models/tree/master/research/object_detection https://www.tensorflow.org/guide/checkpoint"
  },
  {
    "objectID": "posts/tensorflow/2022-10-10-Tensorflow-NLP.html",
    "href": "posts/tensorflow/2022-10-10-Tensorflow-NLP.html",
    "title": "TensorFlow NLP cheat sheet",
    "section": "",
    "text": "Basic Implementation\nThe standard language model starts with an embedding layer, this then needs to be flattened to a vector, then we can add a dense layer before an output layer.\nThe Embedding layer creates a vector-space for the text data. So for example, the words beautiful and ugly may be in opposite directions. And words such as cat and kitten may be close together in vector space.\nGlobalAveragePooling1Dcan be replaced by Flatten()\n\nmodel = tf.keras.Sequential([ \n    tf.keras.layers.Embedding(num_words,embedding_dim,input_length=maxlen),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(24,activation='relu'),\n    tf.keras.layers.Dense(5,'softmax')\n])\n\nThe model above does not take account for the order of words,\nIf we want to do this we can insert an additional layer after the embedding layer. For example, by using the LSTM model as below\n\nmodel_lstm= tf.keras.Sequential([\n    tf.keras.layers.Embedding(VOCAB_SIZE,EMBEDDING_DIM,input_length=MAXLEN),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n    tf.keras.layers.Dense(24,activation='relu'),\n    tf.keras.layers.Dense(NUM_CLASSES,activation='softmax')   \n])\n\nWe can even insert a conolution layer after the embedding instead\ntf.keras.layers.Conv1D(128,5,activation='relu')\nFor two consecutive layers of RNNs use return_sequences=True\n\ntf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm1_dim, return_sequences=True)),\ntf.keras.layers.Bidirectional(tf.keras.layers.LSTM(lstm2_dim)),\n\n\n\nText data Tokenizer\n\nCreate a Tokenizer instance\nFit tokenizer to text data with tokenizer.fit_on_texts(text_data)\nConvert text to sequences with sequences = tokenizer.texts_to_sequences(text_data)\n\nFor example, the following words have the indices: apple->1, brain->2, cat->3, that->4, is->5\nAnd a sequence of text within the data can be converted to a sequence: “that cat apple is brain” -> (4, 3, 1, 5, 2)\n\nGet the word index word_index = tokenizer.word_index\nGet the text back from the sequences text = tokenizer.sequences_to_texts(sequences)\n\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntokenizer = Tokenizer(oov_token=\"<OOV>\",num_words=10_000)\ntokenizer.fit_on_texts(text_data)\n\nsequences = label_tokenizer.texts_to_sequences(text_data)"
  }
]